{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1OA8n1V3Jt_"
   },
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Install Pacakges</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T15:34:28.381903Z",
     "start_time": "2025-02-01T15:34:17.422692Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-15T16:40:33.266367Z",
     "iopub.status.busy": "2025-03-15T16:40:33.265978Z",
     "iopub.status.idle": "2025-03-15T16:40:36.407299Z",
     "shell.execute_reply": "2025-03-15T16:40:36.406165Z",
     "shell.execute_reply.started": "2025-03-15T16:40:33.266317Z"
    },
    "id": "bRRfcrFG3JuA",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install datasets lxml TinyImageNet matplotlib seaborn torch torchvision scipy scikit-learn safe_pfl_utils breaching tabulate --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8oO1OslR3JuA"
   },
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Import Libraries</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T15:34:56.429560Z",
     "start_time": "2025-02-01T15:34:30.137748Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-15T16:40:36.408863Z",
     "iopub.status.busy": "2025-03-15T16:40:36.408631Z",
     "iopub.status.idle": "2025-03-15T16:40:36.417253Z",
     "shell.execute_reply": "2025-03-15T16:40:36.416605Z",
     "shell.execute_reply.started": "2025-03-15T16:40:36.408840Z"
    },
    "id": "IWgcTDs4vXBk",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import copy as py_copy\n",
    "import gc\n",
    "import logging\n",
    "import logging.config\n",
    "import os\n",
    "import os.path\n",
    "import random\n",
    "import sys\n",
    "import tarfile\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from heapq import nlargest\n",
    "from itertools import combinations\n",
    "from functools import partial\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "from typing import Callable, Optional\n",
    "from torch.nn.utils import parameters_to_vector as Params2Vec\n",
    "import torch.nn.utils.prune as prune\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from IPython.display import clear_output\n",
    "from PIL import Image\n",
    "from safe_pfl_utils.config import Config\n",
    "from safe_pfl_utils.constants import (\n",
    "    data_distribution_constants,\n",
    "    datasets_constants,\n",
    "    distances_constants,\n",
    "    models_constants,\n",
    ")\n",
    "from scipy.stats import wasserstein_distance\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tabulate import tabulate\n",
    "from tinyimagenet import TinyImageNet\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm as tq\n",
    "from torch.utils.model_zoo import tqdm\n",
    "from torchvision.datasets import (\n",
    "    CIFAR10,\n",
    "    CIFAR100,\n",
    "    MNIST,\n",
    "    STL10,\n",
    "    SVHN,\n",
    "    DatasetFolder,\n",
    "    FashionMNIST,\n",
    "    ImageFolder,\n",
    ")\n",
    "from torchvision.datasets.utils import check_integrity, download_file_from_google_drive\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "from torchvision.transforms import Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-XrkWV93JuB"
   },
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Configs</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T16:40:36.419117Z",
     "iopub.status.busy": "2025-03-15T16:40:36.418898Z",
     "iopub.status.idle": "2025-03-15T16:40:36.437493Z",
     "shell.execute_reply": "2025-03-15T16:40:36.436741Z",
     "shell.execute_reply.started": "2025-03-15T16:40:36.419097Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#! N20 old runs\n",
    "# DESIRED_DISTRIBUTION = [\n",
    "#     [2948, 0, 5293, 0, 0, 0, 0, 0, 0, 0],\n",
    "#     [1000, 0, 2330, 0, 0, 0, 0, 0, 0, 0],\n",
    "#     [1000, 0, 5292, 0, 0, 0, 0, 0, 0, 0],\n",
    "#     [0, 0, 0, 4249, 3729, 0, 0, 0, 0, 0],\n",
    "#     [0, 0, 0, 0, 3729, 0, 2465, 0, 0, 0],\n",
    "#     [0, 0, 0, 3720, 0, 0, 2145, 0, 0, 0],\n",
    "#     [0, 0, 0, 0, 0, 3865, 2864, 0, 0, 0],\n",
    "#     [0, 0, 0, 0, 0, 0, 0, 1865, 2863, 0],\n",
    "#     [0, 0, 0, 0, 0, 0, 0, 0, 5045, 3248],\n",
    "#     [0, 0, 0, 0, 0, 0, 0, 3465, 0, 1329],\n",
    "# ]\n",
    "\n",
    "#! N20 new data distribution (for test)\n",
    "# DESIRED_DISTRIBUTION = [\n",
    "#     [3448,2880,0,0,0,0,0,0,0,0],\n",
    "#     [0,1750,4046,0,0,0,0,0,0,0],\n",
    "#     [2000,0,3946,0,0,0,0,0,0,0],   \n",
    "#     [0,0,0,4249,4984,0,0,0,0,0],\n",
    "#     [0,0,0,0,3000,2750,0,0,0,0],\n",
    "#     [0,0,0,3720,0,0,3377,0,0,0],\n",
    "#     [0,0,0,0,0,5165,4096,0,0,0], \n",
    "#     [0,0,0,0,0,0,0,2540,3263,0],\n",
    "#     [0,0,0,0,0,0,0,0,5445,3892],\n",
    "#     [0,0,0,0,0,0,0,4140,0,1994], \n",
    "# ]\n",
    "\n",
    "#! FMNIST & CNN\n",
    "# DESIRED_DISTRIBUTION = [\n",
    "#     [2948, 2330, 5292, 0, 0, 0, 0, 0, 0, 0],  # Row 0: Classes 0, 1, 2\n",
    "#     [1000, 1200, 1400, 0, 0, 0, 0, 0, 0, 0],  # Row 1: Classes 0, 1, 2\n",
    "#     [1500, 1100, 1300, 0, 0, 0, 0, 0, 0, 0],  # Row 2: Classes 0, 1, 2\n",
    "#     [0, 0, 0, 4249, 3729, 1350, 0, 0, 0, 0],  # Row 3: Classes 3, 4, 5\n",
    "#     [0, 0, 0, 0, 3729, 1450, 2465, 0, 0, 0],  # Row 4: Classes 4, 5, 6\n",
    "#     [0, 0, 0, 3720, 0, 1250, 2145, 0, 0, 0],  # Row 5: Classes 3, 5, 6\n",
    "#     [0, 0, 0, 0, 400, 3865, 2864, 0, 0, 0],   # Row 6: Classes 4, 5, 6\n",
    "#     [0, 0, 0, 0, 0, 0, 0, 1865, 2863, 1329],  # Row 7: Classes 7, 8, 9\n",
    "#     [0, 0, 0, 0, 0, 0, 0, 1350, 5045, 3248],  # Row 8: Classes 7, 8, 9\n",
    "#     [0, 0, 0, 0, 0, 0, 0, 3465, 800, 1350],   # Row 9: Classes 7, 8, 9\n",
    "# ]\n",
    "\n",
    "# #! SVHN & ResNet\n",
    "DESIRED_DISTRIBUTION = [\n",
    "    [1600, 900, 1100, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [1200, 1550, 1050, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [1100, 1000, 1800, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 1500, 1400, 1350, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 1350, 1450, 1200, 0, 0, 0],\n",
    "    [0, 0, 0, 1350, 400, 0, 1300, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 400, 1350, 1250, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 1400, 1300, 1000],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 1350, 1150, 1650],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 700, 900, 1350],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T15:34:57.502389Z",
     "start_time": "2025-02-01T15:34:57.492389Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-15T16:45:34.249866Z",
     "iopub.status.busy": "2025-03-15T16:45:34.249663Z",
     "iopub.status.idle": "2025-03-15T16:45:34.257294Z",
     "shell.execute_reply": "2025-03-15T16:45:34.256405Z",
     "shell.execute_reply.started": "2025-03-15T16:45:34.249847Z"
    },
    "id": "qVX67JHf3JuB",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CNN-FMNIST configurations\n",
    "\"\"\"\n",
    "# configurations = Config(\n",
    "#     MODEL_TYPE=models_constants.MODEL_CNN,\n",
    "#     DATASET_TYPE=datasets_constants.DATA_SET_FMNIST,\n",
    "#     DATA_DISTRIBUTION_KIND=data_distribution_constants.DATA_DISTRIBUTION_FIX,\n",
    "#     DISTANCE_METRIC=distances_constants.DISTANCE_COSINE,\n",
    "#     DESIRED_DISTRIBUTION=DESIRED_DISTRIBUTION,\n",
    "#     CLUSTERING_PERIOD=6,  # 1, 10\n",
    "#     FEDERATED_LEARNING_ROUNDS=80,\n",
    "#     SAVE_BEFORE_AGGREGATION_MODELS=False,\n",
    "#     SENSITIVITY_PERCENTAGE=100,  #! DO NOT CHANGE THIS VALUE WILL BE CALCULATE AUTOMATICALLY\n",
    "#     NUMBER_OF_EPOCHS=1,\n",
    "#     TRAIN_BATCH_SIZE=128,\n",
    "#     TEST_BATCH_SIZE=128,\n",
    "# )\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "    ResNet18-SVHN configurations\n",
    "\"\"\"\n",
    "configurations = Config(\n",
    "    MODEL_TYPE=models_constants.MODEL_RESNET_18,\n",
    "    DATASET_TYPE=datasets_constants.DATA_SET_SVHN,\n",
    "    DATA_DISTRIBUTION_KIND=data_distribution_constants.DATA_DISTRIBUTION_FIX, #!\n",
    "    DISTANCE_METRIC=distances_constants.DISTANCE_COSINE,\n",
    "    DESIRED_DISTRIBUTION=DESIRED_DISTRIBUTION,\n",
    "    CLUSTERING_PERIOD=6,  # 1, 10\n",
    "    FEDERATED_LEARNING_ROUNDS=80,\n",
    "    SAVE_BEFORE_AGGREGATION_MODELS=False,\n",
    "    SENSITIVITY_PERCENTAGE=100,  #! DO NOT CHANGE THIS VALUE WILL BE CALCULATE AUTOMATICALLY\n",
    "    NUMBER_OF_EPOCHS=1,\n",
    "    TRAIN_BATCH_SIZE=128,\n",
    "    TEST_BATCH_SIZE=128,\n",
    ")\n",
    "\n",
    "\"\"\" \n",
    "    ResNet18-CIFAR10 configurations\n",
    "\"\"\"\n",
    "# configurations = Config(\n",
    "#     MODEL_TYPE=models_constants.MODEL_RESNET_18,\n",
    "#     DATASET_TYPE=datasets_constants.DATA_SET_CIFAR_10,\n",
    "#     DATA_DISTRIBUTION_KIND=data_distribution_constants.DATA_DISTRIBUTION_N_20,\n",
    "#     DISTANCE_METRIC=distances_constants.DISTANCE_COORDINATE,\n",
    "#     DESIRED_DISTRIBUTION=DESIRED_DISTRIBUTION,\n",
    "#     CLUSTERING_PERIOD=6,  # 1, 10\n",
    "#     FEDERATED_LEARNING_ROUNDS=80,\n",
    "#     SAVE_BEFORE_AGGREGATION_MODELS=False,\n",
    "#     SENSITIVITY_PERCENTAGE=100,  #! DO NOT CHANGE THIS VALUE WILL BE CALCULATE AUTOMATICALLY\n",
    "#     NUMBER_OF_EPOCHS=1,\n",
    "#     TRAIN_BATCH_SIZE=128,\n",
    "#     TEST_BATCH_SIZE=128,\n",
    "# )\n",
    "\n",
    "\"\"\" \n",
    "    MobileNetV2 SVHN configurations\n",
    "\"\"\"\n",
    "# configurations = Config(\n",
    "#     MODEL_TYPE=models_constants.MODEL_MOBILENET,\n",
    "#     DATASET_TYPE=datasets_constants.DATA_SET_SVHN,\n",
    "#     DATA_DISTRIBUTION_KIND=data_distribution_constants.DATA_DISTRIBUTION_N_20,\n",
    "#     DISTANCE_METRIC=distances_constants.DISTANCE_COORDINATE,\n",
    "#     DESIRED_DISTRIBUTION=DESIRED_DISTRIBUTION,\n",
    "#     CLUSTERING_PERIOD=6,  # 1, 10\n",
    "#     FEDERATED_LEARNING_ROUNDS=80,\n",
    "#     SAVE_BEFORE_AGGREGATION_MODELS=False,\n",
    "#     SENSITIVITY_PERCENTAGE=100,  #! DO NOT CHANGE THIS VALUE WILL BE CALCULATE AUTOMATICALLY\n",
    "#     TRAIN_BATCH_SIZE=128,\n",
    "#     TEST_BATCH_SIZE=128,\n",
    "# )\n",
    "\n",
    "\"\"\"\n",
    "    ALexNet-STL10 configurations\n",
    "\"\"\"\n",
    "# configurations = Config(\n",
    "#     MODEL_TYPE=models_constants.MODEL_AELXNET,\n",
    "#     DATASET_TYPE=datasets_constants.DATA_SET_STL_10,\n",
    "#     DATA_DISTRIBUTION_KIND=data_distribution_constants.DATA_DISTRIBUTION_N_20,\n",
    "#     DISTANCE_METRIC=distances_constants.DISTANCE_COORDINATE,\n",
    "#     DESIRED_DISTRIBUTION=DESIRED_DISTRIBUTION,\n",
    "#     CLUSTERING_PERIOD=6,  # 1, 10\n",
    "#     FEDERATED_LEARNING_ROUNDS=80,\n",
    "#     SAVE_BEFORE_AGGREGATION_MODELS=False,\n",
    "#     SENSITIVITY_PERCENTAGE=100,  #! DO NOT CHANGE THIS VALUE WILL BE CALCULATE AUTOMATICALLY\n",
    "#     TRAIN_BATCH_SIZE=128,\n",
    "#     TEST_BATCH_SIZE=128,\n",
    "# )\n",
    "\n",
    "\"\"\"\n",
    "    ResNet50-CIFAR100 configurations\n",
    "\"\"\"\n",
    "# configurations = Config(\n",
    "#     MODEL_TYPE=models_constants.MODEL_RESNET_50,\n",
    "#     DATASET_TYPE=datasets_constants.DATA_SET_CIFAR_100,\n",
    "#     DATA_DISTRIBUTION_KIND=data_distribution_constants.DATA_DISTRIBUTION_N_20,\n",
    "#     DISTANCE_METRIC=distances_constants.DISTANCE_EUCLIDEAN,\n",
    "#     DESIRED_DISTRIBUTION=DESIRED_DISTRIBUTION,\n",
    "#     CLUSTERING_PERIOD=6,\n",
    "#     FEDERATED_LEARNING_ROUNDS=80, #! just run 24 FL round is enough for coordinate distance\n",
    "#     SAVE_BEFORE_AGGREGATION_MODELS=False,\n",
    "#     SENSITIVITY_PERCENTAGE=100, #! DO NOT CHANGE THIS VALUE WILL BE CALCULATE AUTOMATICALLY\n",
    "#     NUMBER_OF_EPOCHS=10,\n",
    "#     TRAIN_BATCH_SIZE=256,\n",
    "#     TEST_BATCH_SIZE=256\n",
    "# )\n",
    "\n",
    "\"\"\"\n",
    "    vgg16-CIFAR100 configurations\n",
    "\"\"\"\n",
    "# configurations = Config(\n",
    "#     MODEL_TYPE=models_constants.MODEL_RESNET_50, #! set the model bellow\n",
    "#     DATASET_TYPE=datasets_constants.DATA_SET_CIFAR_100,\n",
    "#     DATA_DISTRIBUTION_KIND=data_distribution_constants.DATA_DISTRIBUTION_N_20,\n",
    "#     DISTANCE_METRIC=distances_constants.DISTANCE_EUCLIDEAN,\n",
    "#     DESIRED_DISTRIBUTION=DESIRED_DISTRIBUTION,\n",
    "#     CLUSTERING_PERIOD=6,\n",
    "#     FEDERATED_LEARNING_ROUNDS=80, #! just run 24 FL round is enough for coordinate distance\n",
    "#     SAVE_BEFORE_AGGREGATION_MODELS=True,\n",
    "#     SENSITIVITY_PERCENTAGE=100, #! DO NOT CHANGE THIS VALUE WILL BE CALCULATE AUTOMATICALLY\n",
    "#     NUMBER_OF_EPOCHS=10,\n",
    "#     TRAIN_BATCH_SIZE=128,\n",
    "#     TEST_BATCH_SIZE=128\n",
    "# )\n",
    "\n",
    "SAFE_PFL_CONFIG = configurations.get_config()\n",
    "\n",
    "# SAFE_PFL_CONFIG.update(\n",
    "#     {\n",
    "#         \"MODEL_TYPE\": \"vgg16\",\n",
    "#         \"DATASET_TYPE\": \"tinyimagenet\",\n",
    "#         \"NUMBER_OF_CLASSES\": 200,\n",
    "#     }\n",
    "# )\n",
    "\n",
    "if SAFE_PFL_CONFIG[\"MODEL_TYPE\"] == models_constants.MODEL_CNN:\n",
    "    SAFE_PFL_CONFIG.update({\"STOP_AVG_ACCURACY\": 1.0}) #! FILL IT\n",
    "elif SAFE_PFL_CONFIG[\"MODEL_TYPE\"] == models_constants.MODEL_RESNET_18:\n",
    "    SAFE_PFL_CONFIG.update({\"STOP_AVG_ACCURACY\": 1.0}) #! FILL IT\n",
    "elif SAFE_PFL_CONFIG[\"MODEL_TYPE\"] == models_constants.MODEL_RESNET_50:\n",
    "    SAFE_PFL_CONFIG.update({\"STOP_AVG_ACCURACY\": 1.0}) #! FILL IT\n",
    "elif SAFE_PFL_CONFIG[\"MODEL_TYPE\"] == models_constants.MODEL_MOBILENET:\n",
    "    SAFE_PFL_CONFIG.update({\"STOP_AVG_ACCURACY\": 1.0}) #! FILL IT\n",
    "elif SAFE_PFL_CONFIG[\"MODEL_TYPE\"] == models_constants.MODEL_AELXNET:\n",
    "    SAFE_PFL_CONFIG.update({\"STOP_AVG_ACCURACY\": 1.0}) #! FILL IT\n",
    "elif SAFE_PFL_CONFIG[\"MODEL_TYPE\"] == \"vgg16\":\n",
    "    SAFE_PFL_CONFIG.update({\"STOP_AVG_ACCURACY\": 1.0}) #! FILL IT\n",
    "\n",
    "SAFE_PFL_CONFIG.update(\n",
    "    {\n",
    "        \"DYNAMIC_SENSITIVITY_PERCENTAGE\": True,\n",
    "        \"DISTANCE_METRIC_ON_PARAMETERS\": False,\n",
    "\n",
    "        \"PRE_COMPUTED_OPTIMAL_CLUSTERING\": False,\n",
    "\n",
    "        \"FED_AVG\": False,\n",
    "        \n",
    "        \"REMOVE_COMMON_IDS\": True,\n",
    "        \"CLUSTER_AT_FIRST\": False,\n",
    "        \n",
    "        \"SAVE_GLOBAL_MODELS\": True,\n",
    "        \n",
    "        \"ATTACK_CASE_RESNET\": False,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T15:34:57.760382Z",
     "start_time": "2025-02-01T15:34:57.568082Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-03-15T16:40:36.454862Z",
     "iopub.status.busy": "2025-03-15T16:40:36.454592Z",
     "iopub.status.idle": "2025-03-15T16:40:36.473669Z",
     "shell.execute_reply": "2025-03-15T16:40:36.472745Z",
     "shell.execute_reply.started": "2025-03-15T16:40:36.454827Z"
    },
    "id": "uu3Fu3A3qnPR",
    "outputId": "45cda9ce-b5d6-4d22-8419-861beac9014a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "os.environ[\"PL_GLOBAL_SEED\"] = str(seed)\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\", font_scale=1.5, rc={\"axes.unicode_minus\": False})\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# to produce reproducible results (like random.seed())\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T16:40:36.474910Z",
     "iopub.status.busy": "2025-03-15T16:40:36.474656Z",
     "iopub.status.idle": "2025-03-15T16:40:36.493657Z",
     "shell.execute_reply": "2025-03-15T16:40:36.492805Z",
     "shell.execute_reply.started": "2025-03-15T16:40:36.474881Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Log:\n",
    "    def __init__(self):\n",
    "        log_path = datetime.now().strftime(\n",
    "            f'Model={SAFE_PFL_CONFIG[\"MODEL_TYPE\"]}-Dataset={SAFE_PFL_CONFIG[\"DATASET_TYPE\"]}-N={SAFE_PFL_CONFIG[\"PARTITION\"]}-P={SAFE_PFL_CONFIG[\"SENSITIVITY_PERCENTAGE\"]}_on={SAFE_PFL_CONFIG[\"DISTANCE_METRIC\"]}_with_pre_computed_clustering={SAFE_PFL_CONFIG[\"PRE_COMPUTED_OPTIMAL_CLUSTERING\"]}_at=%Y-%m-%d_%H'\n",
    "        )\n",
    "        log_file = \"logs/\" + log_path + \".log\"\n",
    "\n",
    "        os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "        if os.path.exists(log_file):\n",
    "            try:\n",
    "                os.remove(log_file)\n",
    "                print(f\"Old log file '{log_file}' deleted.\")\n",
    "            except PermissionError as _:\n",
    "                print(\n",
    "                    \"Log file deletion can cause data lost, if you are sure please restart you session\"\n",
    "                )\n",
    "\n",
    "        self.log_instance = logging.getLogger(\"SAFE_PFL_LOGGER\")\n",
    "        self.log_instance.setLevel(logging.DEBUG)\n",
    "        self.log_instance.propagate = False\n",
    "\n",
    "        formatter = logging.Formatter(\n",
    "            fmt=\"%(asctime)s, line: %(lineno)d %(levelname)8s | %(message)s\",\n",
    "            datefmt=\"%Y/%m/%d %H:%M:%S\",\n",
    "        )\n",
    "\n",
    "        # Create a file handler\n",
    "        file_handler = logging.FileHandler(log_file, mode=\"a\")\n",
    "        file_handler.setFormatter(formatter)\n",
    "        self.log_instance.addHandler(file_handler)\n",
    "\n",
    "        # Create a stream handler (for console output)\n",
    "        screen_handler = logging.StreamHandler(stream=sys.stdout)\n",
    "        screen_handler.setFormatter(formatter)\n",
    "        self.log_instance.addHandler(screen_handler)\n",
    "\n",
    "        self.log_instance.info(\"Logger object created successfully...\")\n",
    "        self.log_instance.warning(f\"The {log_file} will be truncated at each run\")\n",
    "\n",
    "    def info(self, info: str):\n",
    "        self.log_instance.info(info)\n",
    "        self.flush()\n",
    "\n",
    "    def warn(self, warn: str):\n",
    "        self.log_instance.warning(warn)\n",
    "        self.flush()\n",
    "\n",
    "    def debug(self, debug: str):\n",
    "        self.log_instance.debug(debug)\n",
    "        self.flush()\n",
    "\n",
    "    def critical(self, critical: str):\n",
    "        self.log_instance.critical(critical)\n",
    "        self.flush()\n",
    "\n",
    "    def error(self, error: str):\n",
    "        self.log_instance.error(error)\n",
    "        self.flush()\n",
    "\n",
    "    def flush(self):\n",
    "        for handler in self.log_instance.handlers:\n",
    "            if hasattr(handler, \"flush\"):\n",
    "                handler.flush()\n",
    "\n",
    "    def close(self):\n",
    "        self.log_instance.handlers.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T16:40:36.494823Z",
     "iopub.status.busy": "2025-03-15T16:40:36.494541Z",
     "iopub.status.idle": "2025-03-15T16:40:36.516657Z",
     "shell.execute_reply": "2025-03-15T16:40:36.515824Z",
     "shell.execute_reply.started": "2025-03-15T16:40:36.494792Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "log = Log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T16:40:36.519402Z",
     "iopub.status.busy": "2025-03-15T16:40:36.519186Z",
     "iopub.status.idle": "2025-03-15T16:40:36.535327Z",
     "shell.execute_reply": "2025-03-15T16:40:36.534627Z",
     "shell.execute_reply.started": "2025-03-15T16:40:36.519383Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "table_data = [[key, value] for key, value in SAFE_PFL_CONFIG.items()]\n",
    "log.info(tabulate(table_data, headers=[\"Config Key\", \"Value\"], tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Garbage Collection</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T16:40:36.536999Z",
     "iopub.status.busy": "2025-03-15T16:40:36.536791Z",
     "iopub.status.idle": "2025-03-15T16:40:36.835246Z",
     "shell.execute_reply": "2025-03-15T16:40:36.834645Z",
     "shell.execute_reply.started": "2025-03-15T16:40:36.536973Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "\n",
    "def print_gpu_memory():\n",
    "    log.info(f\"Allocated memory: {torch.cuda.memory_allocated() / 1024 ** 2:.2f} MB\")\n",
    "    log.info(f\"Cached memory: {torch.cuda.memory_reserved() / 1024 ** 2:.2f} MB\")\n",
    "\n",
    "\n",
    "log.info(\"before memory cleaning\")\n",
    "print_gpu_memory()\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# cuda.select_device(0)\n",
    "# cuda.close()\n",
    "\n",
    "log.info(\"after memory cleaning\")\n",
    "print_gpu_memory()\n",
    "\n",
    "# ----------- manually clear memory in case of any error\n",
    "#!sudo fuser -v /dev/nvidia* or nvidia-smi\n",
    "# remove all python process ids from gpu\n",
    "#!sudo kill -9 PID.\n",
    "\n",
    "# * Make directories\n",
    "MODEL_SAVING_PATH = (\n",
    "    os.path.join(\n",
    "        \"./models\", SAFE_PFL_CONFIG[\"MODEL_TYPE\"], SAFE_PFL_CONFIG[\"DATASET_TYPE\"]\n",
    "    )\n",
    "    + \"/\"\n",
    ")\n",
    "if not os.path.exists(MODEL_SAVING_PATH):\n",
    "    os.makedirs(MODEL_SAVING_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSl3rZx23JuC"
   },
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Model Network</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T15:34:57.836152Z",
     "start_time": "2025-02-01T15:34:57.827935Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-15T16:40:36.836336Z",
     "iopub.status.busy": "2025-03-15T16:40:36.836062Z",
     "iopub.status.idle": "2025-03-15T16:40:36.847954Z",
     "shell.execute_reply": "2025-03-15T16:40:36.847275Z",
     "shell.execute_reply.started": "2025-03-15T16:40:36.836302Z"
    },
    "id": "evEmrviBwIoH",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, _model_type: str, _number_of_classes: int):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self._model_type = _model_type\n",
    "        self._number_of_classes = _number_of_classes\n",
    "\n",
    "        if self._model_type == \"resnet18\":\n",
    "            if SAFE_PFL_CONFIG[\"ATTACK_CASE_RESNET\"]:\n",
    "                \n",
    "                from breaching.cases.models.resnets import ResNet, resnet_depths_to_config, BasicBlock, Bottleneck, get_layer_functions\n",
    "                \n",
    "                depth = 18\n",
    "                width = 1\n",
    "                channels = 3  # Standard for RGB images\n",
    "                block, layers = resnet_depths_to_config(depth)\n",
    "                \n",
    "                self.resnet = ResNet(\n",
    "                    block,\n",
    "                    layers,\n",
    "                    channels,\n",
    "                    self._number_of_classes,\n",
    "                    stem=\"CIFAR\",\n",
    "                    convolution_type=\"Standard\",\n",
    "                    nonlin=\"ReLU\",\n",
    "                    norm=\"BatchNorm2d\",\n",
    "                    downsample=\"B\",\n",
    "                    width_per_group=(16 if len(layers) < 4 else 64) * width,\n",
    "                    zero_init_residual=False,\n",
    "                )\n",
    "            else:\n",
    "                self.resnet = models.resnet18(pretrained=False)\n",
    "                self.resnet.fc = nn.Sequential(nn.Linear(512, self._number_of_classes))\n",
    "        elif self._model_type == \"resnet50\":\n",
    "            self.resnet = models.resnet50(pretrained=False)\n",
    "            self.resnet.fc = nn.Linear(\n",
    "                self.resnet.fc.in_features, self._number_of_classes\n",
    "            )\n",
    "        elif self._model_type == \"cnn\":\n",
    "            self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "            self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "            self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "            self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "            self.fc2 = nn.Linear(128, self._number_of_classes)\n",
    "        elif self._model_type == \"mobilenet\":\n",
    "            self.mobilenet = models.mobilenet_v2(pretrained=False)\n",
    "            num_features = self.mobilenet.classifier[-1].in_features\n",
    "            self.mobilenet.classifier[-1] = nn.Linear(num_features, self._number_of_classes)\n",
    "        elif self._model_type == \"alexnet\":\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            )\n",
    "\n",
    "            self._to_linear = 128 * (128 // 8) * (128 // 8)\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(self._to_linear, 512),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(),\n",
    "                nn.Linear(512, self._number_of_classes),\n",
    "            )\n",
    "        elif self._model_type == \"vgg16\":\n",
    "            self.vgg16 = models.vgg16(pretrained=False)\n",
    "            self.vgg16.avgpool = torch.nn.AdaptiveAvgPool2d(1)\n",
    "            self.vgg16.classifier = torch.nn.Sequential(\n",
    "                torch.nn.Linear(512, 256),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(0.5),\n",
    "                torch.nn.Linear(256, 128),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(0.5),\n",
    "                torch.nn.Linear(128, self._number_of_classes),\n",
    "            )\n",
    "        else:\n",
    "            log.error(f'unsupported model type: {self._model_type}')\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = None\n",
    "        if self._model_type in [\"resnet18\", \"resnet50\"]:\n",
    "            out = self.resnet(x)\n",
    "        elif self._model_type == \"cnn\":\n",
    "            x = F.relu(self.conv1(x))  # Output: 32x28x28\n",
    "            x = self.pool(x)  # Output: 32x14x14\n",
    "            x = F.relu(self.conv2(x))  # Output: 64x14x14\n",
    "            x = self.pool(x)  # Output: 64x7x7\n",
    "            # Flatten the output for fully connected layers\n",
    "            x = x.view(x.size(0), -1)  # Flatten to (batch_size, 64*7*7)\n",
    "            # Fully connected layers\n",
    "            x = F.relu(self.fc1(x))  # Output: 128\n",
    "            x = self.fc2(x)  # Output: num_classes\n",
    "            return x\n",
    "        elif self._model_type == \"mobilenet\":\n",
    "            out = self.mobilenet(x)\n",
    "\n",
    "        elif self._model_type == \"alexnet\":\n",
    "            x = self.features(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = self.classifier(x)\n",
    "            out = x\n",
    "        elif self._model_type == \"vgg16\":\n",
    "            out = self.vgg16(x)\n",
    "        else:\n",
    "            log.error(f'unsupported model type: {self._model_type}')\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSKd2tLw3JuD"
   },
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Loading & Saving</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T15:34:58.011740Z",
     "start_time": "2025-02-01T15:34:58.005177Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-15T16:40:36.849018Z",
     "iopub.status.busy": "2025-03-15T16:40:36.848807Z",
     "iopub.status.idle": "2025-03-15T16:40:36.863221Z",
     "shell.execute_reply": "2025-03-15T16:40:36.862528Z",
     "shell.execute_reply.started": "2025-03-15T16:40:36.848998Z"
    },
    "id": "LazN3rY5xDiZ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_torch_model(node_id):\n",
    "    model_path = f\"models/node_{node_id}.pth\"\n",
    "    model = torch.load(model_path)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_torch_model_before_agg(node_id):\n",
    "    model_path = f\"models/before_aggregation/node_{node_id}.pth\"\n",
    "    model = torch.load(model_path)\n",
    "    return model\n",
    "\n",
    "\n",
    "def save_torch_model_before_agg(model, client_id: str):\n",
    "    model_path = f\"models/before_aggregation/node_{client_id}.pth\"\n",
    "    torch.save(model, model_path)\n",
    "\n",
    "\n",
    "def save_torch_model(model, node_id):\n",
    "    model_path = f\"models/node_{node_id}.pth\"\n",
    "    torch.save(model, model_path)\n",
    "\n",
    "\n",
    "def save_model_param(model, node_id, round_number):\n",
    "    model_path = f\"models/node_{node_id}_round_{round_number}.pth\"\n",
    "    torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sq920RVv3JuD"
   },
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Non-IID Distribution</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T15:34:58.106624Z",
     "start_time": "2025-02-01T15:34:58.044482Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-15T16:40:36.864493Z",
     "iopub.status.busy": "2025-03-15T16:40:36.864206Z",
     "iopub.status.idle": "2025-03-15T16:40:36.931125Z",
     "shell.execute_reply": "2025-03-15T16:40:36.930317Z",
     "shell.execute_reply.started": "2025-03-15T16:40:36.864464Z"
    },
    "id": "eGjDwC9x3JuD",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "IMG_EXTENSIONS = (\n",
    "    \".jpg\",\n",
    "    \".jpeg\",\n",
    "    \".png\",\n",
    "    \".ppm\",\n",
    "    \".bmp\",\n",
    "    \".pgm\",\n",
    "    \".tif\",\n",
    "    \".tiff\",\n",
    "    \".webp\",\n",
    ")\n",
    "\n",
    "\n",
    "def mkdirs(dirpath):\n",
    "    try:\n",
    "        os.makedirs(dirpath)\n",
    "    except Exception as _:\n",
    "        pass\n",
    "\n",
    "\n",
    "def pil_loader(path):\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, \"rb\") as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert(\"RGB\")\n",
    "\n",
    "\n",
    "class CustomTensorDataset(data.TensorDataset):\n",
    "    def __getitem__(self, index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors) + (index,)\n",
    "\n",
    "\n",
    "class MNIST_truncated(data.Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        dataidxs=None,\n",
    "        train=True,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "        download=False,\n",
    "    ):\n",
    "\n",
    "        self.root = root\n",
    "        self.dataidxs = dataidxs\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.download = download\n",
    "\n",
    "        self.data, self.target = self.__build_truncated_dataset__()\n",
    "\n",
    "    def __build_truncated_dataset__(self):\n",
    "\n",
    "        mnist_dataobj = MNIST(\n",
    "            self.root, self.train, self.transform, self.target_transform, self.download\n",
    "        )\n",
    "\n",
    "        data = mnist_dataobj.data\n",
    "        target = mnist_dataobj.targets\n",
    "\n",
    "        if self.dataidxs is not None:\n",
    "            data = data[self.dataidxs]\n",
    "            target = target[self.dataidxs]\n",
    "\n",
    "        return data, target\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.target[index]\n",
    "\n",
    "        img = Image.fromarray(img.numpy(), mode=\"L\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "class FashionMNIST_truncated(data.Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        dataidxs=None,\n",
    "        train=True,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "        download=False,\n",
    "    ):\n",
    "\n",
    "        self.root = root\n",
    "        self.dataidxs = dataidxs\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.download = download\n",
    "\n",
    "        self.data, self.target = self.__build_truncated_dataset__()\n",
    "\n",
    "    def __build_truncated_dataset__(self):\n",
    "\n",
    "        mnist_dataobj = FashionMNIST(\n",
    "            self.root, self.train, self.transform, self.target_transform, self.download\n",
    "        )\n",
    "\n",
    "        data = mnist_dataobj.data\n",
    "        target = mnist_dataobj.targets\n",
    "\n",
    "        if self.dataidxs is not None:\n",
    "            data = data[self.dataidxs]\n",
    "            target = target[self.dataidxs]\n",
    "\n",
    "        return data, target\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.target[index]\n",
    "\n",
    "        img = Image.fromarray(img.numpy(), mode=\"L\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "class SVHN_custom(data.Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        dataidxs=None,\n",
    "        train=True,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "        download=False,\n",
    "    ):\n",
    "\n",
    "        self.root = root\n",
    "        self.dataidxs = dataidxs\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.download = download\n",
    "\n",
    "        self.data, self.target = self.__build_truncated_dataset__()\n",
    "\n",
    "    def __build_truncated_dataset__(self):\n",
    "        if self.train is True:\n",
    "\n",
    "            svhn_dataobj = SVHN(\n",
    "                self.root, \"train\", self.transform, self.target_transform, self.download\n",
    "            )\n",
    "            data = svhn_dataobj.data\n",
    "            target = svhn_dataobj.labels\n",
    "        else:\n",
    "            svhn_dataobj = SVHN(\n",
    "                self.root, \"test\", self.transform, self.target_transform, self.download\n",
    "            )\n",
    "            data = svhn_dataobj.data\n",
    "            target = svhn_dataobj.labels\n",
    "\n",
    "        if self.dataidxs is not None:\n",
    "            data = data[self.dataidxs]\n",
    "            target = target[self.dataidxs]\n",
    "        return data, target\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.target[index]\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(np.transpose(img, (1, 2, 0)))\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "# torchvision CelebA\n",
    "class CelebA_custom(VisionDataset):\n",
    "    \"\"\"`Large-scale CelebFaces Attributes (CelebA) Dataset <http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html>`_ Dataset.\n",
    "\n",
    "    Args:\n",
    "        root (string): Root directory where images are downloaded to.\n",
    "        split (string): One of {'train', 'valid', 'test', 'all'}.\n",
    "            Accordingly dataset is selected.\n",
    "        target_type (string or list, optional): Type of target to use, ``attr``, ``identity``, ``bbox``,\n",
    "            or ``landmarks``. Can also be a list to output a tuple with all specified target types.\n",
    "            The targets represent:\n",
    "                ``attr`` (np.array shape=(40,) dtype=int): binary (0, 1) labels for attributes\n",
    "                ``identity`` (int): label for each person (data points with the same identity are the same person)\n",
    "                ``bbox`` (np.array shape=(4,) dtype=int): bounding box (x, y, width, height)\n",
    "                ``landmarks`` (np.array shape=(10,) dtype=int): landmark points (lefteye_x, lefteye_y, righteye_x,\n",
    "                    righteye_y, nose_x, nose_y, leftmouth_x, leftmouth_y, rightmouth_x, rightmouth_y)\n",
    "            Defaults to ``attr``. If empty, ``None`` will be returned as target.\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.ToTensor``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        download (bool, optional): If true, downloads the dataset from the internet and\n",
    "            puts it in root directory. If dataset is already downloaded, it is not\n",
    "            downloaded again.\n",
    "    \"\"\"\n",
    "\n",
    "    base_folder = \"celeba\"\n",
    "    # There currently does not appear to be a easy way to extract 7z in python (without introducing additional\n",
    "    # dependencies). The \"in-the-wild\" (not aligned+cropped) images are only in 7z, so they are not available\n",
    "    # right now.\n",
    "    file_list = [\n",
    "        # File ID                         MD5 Hash                            Filename\n",
    "        (\n",
    "            \"0B7EVK8r0v71pZjFTYXZWM3FlRnM\",\n",
    "            \"00d2c5bc6d35e252742224ab0c1e8fcb\",\n",
    "            \"img_align_celeba.zip\",\n",
    "        ),\n",
    "        # (\"0B7EVK8r0v71pbWNEUjJKdDQ3dGc\", \"b6cd7e93bc7a96c2dc33f819aa3ac651\", \"img_align_celeba_png.7z\"),\n",
    "        # (\"0B7EVK8r0v71peklHb0pGdDl6R28\", \"b6cd7e93bc7a96c2dc33f819aa3ac651\", \"img_celeba.7z\"),\n",
    "        (\n",
    "            \"0B7EVK8r0v71pblRyaVFSWGxPY0U\",\n",
    "            \"75e246fa4810816ffd6ee81facbd244c\",\n",
    "            \"list_attr_celeba.txt\",\n",
    "        ),\n",
    "        (\n",
    "            \"1_ee_0u7vcNLOfNLegJRHmolfH5ICW-XS\",\n",
    "            \"32bd1bd63d3c78cd57e08160ec5ed1e2\",\n",
    "            \"identity_CelebA.txt\",\n",
    "        ),\n",
    "        (\n",
    "            \"0B7EVK8r0v71pbThiMVRxWXZ4dU0\",\n",
    "            \"00566efa6fedff7a56946cd1c10f1c16\",\n",
    "            \"list_bbox_celeba.txt\",\n",
    "        ),\n",
    "        (\n",
    "            \"0B7EVK8r0v71pd0FJY3Blby1HUTQ\",\n",
    "            \"cc24ecafdb5b50baae59b03474781f8c\",\n",
    "            \"list_landmarks_align_celeba.txt\",\n",
    "        ),\n",
    "        # (\"0B7EVK8r0v71pTzJIdlJWdHczRlU\", \"063ee6ddb681f96bc9ca28c6febb9d1a\", \"list_landmarks_celeba.txt\"),\n",
    "        (\n",
    "            \"0B7EVK8r0v71pY0NSMzRuSXJEVkk\",\n",
    "            \"d32c9cbf5e040fd4025c592c306e6668\",\n",
    "            \"list_eval_partition.txt\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        dataidxs=None,\n",
    "        split=\"train\",\n",
    "        target_type=\"attr\",\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "        download=False,\n",
    "    ):\n",
    "        import pandas\n",
    "\n",
    "        super(CelebA_custom, self).__init__(\n",
    "            root, transform=transform, target_transform=target_transform\n",
    "        )\n",
    "        self.split = split\n",
    "        if isinstance(target_type, list):\n",
    "            self.target_type = target_type\n",
    "        else:\n",
    "            self.target_type = [target_type]\n",
    "\n",
    "        if not self.target_type and self.target_transform is not None:\n",
    "            raise RuntimeError(\"target_transform is specified but target_type is empty\")\n",
    "\n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        if not self._check_integrity():\n",
    "            raise RuntimeError(\n",
    "                \"Dataset not found or corrupted.\"\n",
    "                + \" You can use download=True to download it\"\n",
    "            )\n",
    "\n",
    "        split_map = {\n",
    "            \"train\": 0,\n",
    "            \"valid\": 1,\n",
    "            \"test\": 2,\n",
    "            \"all\": None,\n",
    "        }\n",
    "        split = split_map[split.lower()]\n",
    "\n",
    "        fn = partial(os.path.join, self.root, self.base_folder)\n",
    "        splits = pandas.read_csv(\n",
    "            fn(\"list_eval_partition.txt\"),\n",
    "            delim_whitespace=True,\n",
    "            header=None,\n",
    "            index_col=0,\n",
    "        )\n",
    "        identity = pandas.read_csv(\n",
    "            fn(\"identity_CelebA.txt\"), delim_whitespace=True, header=None, index_col=0\n",
    "        )\n",
    "        bbox = pandas.read_csv(\n",
    "            fn(\"list_bbox_celeba.txt\"), delim_whitespace=True, header=1, index_col=0\n",
    "        )\n",
    "        landmarks_align = pandas.read_csv(\n",
    "            fn(\"list_landmarks_align_celeba.txt\"), delim_whitespace=True, header=1\n",
    "        )\n",
    "        attr = pandas.read_csv(\n",
    "            fn(\"list_attr_celeba.txt\"), delim_whitespace=True, header=1\n",
    "        )\n",
    "\n",
    "        mask = slice(None) if split is None else (splits[1] == split)\n",
    "\n",
    "        self.filename = splits[mask].index.values\n",
    "        self.identity = torch.as_tensor(identity[mask].values)\n",
    "        self.bbox = torch.as_tensor(bbox[mask].values)\n",
    "        self.landmarks_align = torch.as_tensor(landmarks_align[mask].values)\n",
    "        self.attr = torch.as_tensor(attr[mask].values)\n",
    "        self.attr = (self.attr + 1) // 2  # map from {-1, 1} to {0, 1}\n",
    "        self.attr_names = list(attr.columns)\n",
    "        self.gender_index = self.attr_names.index(\"Male\")\n",
    "        self.dataidxs = dataidxs\n",
    "        if self.dataidxs is None:\n",
    "            self.target = self.attr[\n",
    "                :, self.gender_index : self.gender_index + 1\n",
    "            ].reshape(-1)\n",
    "        else:\n",
    "            self.target = self.attr[\n",
    "                self.dataidxs, self.gender_index : self.gender_index + 1\n",
    "            ].reshape(-1)\n",
    "\n",
    "    def _check_integrity(self):\n",
    "        for _, md5, filename in self.file_list:\n",
    "            fpath = os.path.join(self.root, self.base_folder, filename)\n",
    "            _, ext = os.path.splitext(filename)\n",
    "            # Allow original archive to be deleted (zip and 7z)\n",
    "            # Only need the extracted images\n",
    "            if ext not in [\".zip\", \".7z\"] and not check_integrity(fpath, md5):\n",
    "                return False\n",
    "\n",
    "        # Should check a hash of the images\n",
    "        return os.path.isdir(\n",
    "            os.path.join(self.root, self.base_folder, \"img_align_celeba\")\n",
    "        )\n",
    "\n",
    "    def download(self):\n",
    "        import zipfile\n",
    "\n",
    "        if self._check_integrity():\n",
    "            print(\"Files already downloaded and verified\")\n",
    "            return\n",
    "\n",
    "        for file_id, md5, filename in self.file_list:\n",
    "            download_file_from_google_drive(\n",
    "                file_id, os.path.join(self.root, self.base_folder), filename, md5\n",
    "            )\n",
    "\n",
    "        with zipfile.ZipFile(\n",
    "            os.path.join(self.root, self.base_folder, \"img_align_celeba.zip\"), \"r\"\n",
    "        ) as f:\n",
    "            f.extractall(os.path.join(self.root, self.base_folder))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.dataidxs is None:\n",
    "            X = PIL.Image.open(\n",
    "                os.path.join(\n",
    "                    self.root,\n",
    "                    self.base_folder,\n",
    "                    \"img_align_celeba\",\n",
    "                    self.filename[index],\n",
    "                )\n",
    "            )\n",
    "\n",
    "            target = []\n",
    "            for t in self.target_type:\n",
    "                if t == \"attr\":\n",
    "                    target.append(self.attr[index, self.gender_index])\n",
    "                elif t == \"identity\":\n",
    "                    target.append(self.identity[index, 0])\n",
    "                elif t == \"bbox\":\n",
    "                    target.append(self.bbox[index, :])\n",
    "                elif t == \"landmarks\":\n",
    "                    target.append(self.landmarks_align[index, :])\n",
    "                else:\n",
    "                    # TODO: refactor with utils.verify_str_arg\n",
    "                    raise ValueError('Target type \"{}\" is not recognized.'.format(t))\n",
    "        else:\n",
    "            X = PIL.Image.open(\n",
    "                os.path.join(\n",
    "                    self.root,\n",
    "                    self.base_folder,\n",
    "                    \"img_align_celeba\",\n",
    "                    self.filename[self.dataidxs[index]],\n",
    "                )\n",
    "            )\n",
    "\n",
    "            target = []\n",
    "            for t in self.target_type:\n",
    "                if t == \"attr\":\n",
    "                    target.append(self.attr[self.dataidxs[index], self.gender_index])\n",
    "                elif t == \"identity\":\n",
    "                    target.append(self.identity[self.dataidxs[index], 0])\n",
    "                elif t == \"bbox\":\n",
    "                    target.append(self.bbox[self.dataidxs[index], :])\n",
    "                elif t == \"landmarks\":\n",
    "                    target.append(self.landmarks_align[self.dataidxs[index], :])\n",
    "                else:\n",
    "                    # TODO: refactor with utils.verify_str_arg\n",
    "                    raise ValueError('Target type \"{}\" is not recognized.'.format(t))\n",
    "\n",
    "        if self.transform is not None:\n",
    "            X = self.transform(X)\n",
    "        # print(\"target[0]:\", target[0])\n",
    "        if target:\n",
    "            target = tuple(target) if len(target) > 1 else target[0]\n",
    "\n",
    "            if self.target_transform is not None:\n",
    "                target = self.target_transform(target)\n",
    "        else:\n",
    "            target = None\n",
    "        # print(\"celeba target:\", target)\n",
    "        return X, target\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.dataidxs is None:\n",
    "            return len(self.attr)\n",
    "        else:\n",
    "            return len(self.dataidxs)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        lines = [\"Target type: {target_type}\", \"Split: {split}\"]\n",
    "        return \"\\n\".join(lines).format(**self.__dict__)\n",
    "\n",
    "\n",
    "class STL10_truncated(data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        dataidxs=None,\n",
    "        split=\"train\",\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "        download=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Custom STL10 dataset with support for data indexing.\n",
    "        Args:\n",
    "            root (str): Dataset root directory.\n",
    "            dataidxs (list, optional): Indices for data partitioning. Defaults to None.\n",
    "            split (str, optional): Dataset split ('train', 'test', 'unlabeled'). Defaults to 'train'.\n",
    "            transform (callable, optional): Transformations for the input data. Defaults to None.\n",
    "            target_transform (callable, optional): Transformations for the target labels. Defaults to None.\n",
    "            download (bool, optional): Whether to download the dataset. Defaults to False.\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.dataidxs = dataidxs\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.download = download\n",
    "        self.data, self.target = self.__build_truncated_dataset__()\n",
    "\n",
    "    def __build_truncated_dataset__(self):\n",
    "        stl10_dataobj = STL10(\n",
    "            self.root,\n",
    "            split=self.split,\n",
    "            transform=self.transform,\n",
    "            target_transform=self.target_transform,\n",
    "            download=self.download,\n",
    "        )\n",
    "        data = stl10_dataobj.data\n",
    "        target = np.array(stl10_dataobj.labels)\n",
    "\n",
    "        if self.dataidxs is not None:\n",
    "            data = data[self.dataidxs]\n",
    "            target = target[self.dataidxs]\n",
    "\n",
    "        return data, target\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is the class index.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.target[index]\n",
    "\n",
    "        # Ensure the image has the correct shape and dtype for PIL\n",
    "        img = np.transpose(img, (1, 2, 0))  # Convert from (C, H, W) to (H, W, C)\n",
    "        img = img.astype(np.uint8)  # Ensure dtype is uint8 for PIL compatibility\n",
    "        img = Image.fromarray(img)  # Convert to PIL Image\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "class CIFAR10_truncated(data.Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        dataidxs=None,\n",
    "        train=True,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "        download=False,\n",
    "    ):\n",
    "\n",
    "        self.root = root\n",
    "        self.dataidxs = dataidxs\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.download = download\n",
    "\n",
    "        self.data, self.target = self.__build_truncated_dataset__()\n",
    "\n",
    "    def __build_truncated_dataset__(self):\n",
    "\n",
    "        cifar_dataobj = CIFAR10(\n",
    "            self.root, self.train, self.transform, self.target_transform, self.download\n",
    "        )\n",
    "\n",
    "        data = cifar_dataobj.data\n",
    "        target = np.array(cifar_dataobj.targets)\n",
    "\n",
    "        if self.dataidxs is not None:\n",
    "            if isinstance(self.dataidxs, (list, np.ndarray, tuple)):\n",
    "                self.dataidxs = np.array(self.dataidxs, dtype=np.int64)\n",
    "                data = data[self.dataidxs]\n",
    "                target = target[self.dataidxs]\n",
    "            else:\n",
    "                raise TypeError(\"dataidxs must be a list, numpy array, or None.\")\n",
    "\n",
    "        return data, target\n",
    "\n",
    "    def truncate_channel(self, index):\n",
    "        for i in range(index.shape[0]):\n",
    "            gs_index = index[i]\n",
    "            self.data[gs_index, :, :, 1] = 0.0\n",
    "            self.data[gs_index, :, :, 2] = 0.0\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.target[index]\n",
    "\n",
    "        # print(\"cifar10 img:\", img)\n",
    "        # print(\"cifar10 target:\", target)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "def gen_bar_updater() -> Callable[[int, int, int], None]:\n",
    "    pbar = tqdm(total=None)\n",
    "\n",
    "    def bar_update(count, block_size, total_size):\n",
    "        if pbar.total is None and total_size:\n",
    "            pbar.total = total_size\n",
    "        progress_bytes = count * block_size\n",
    "        pbar.update(progress_bytes - pbar.n)\n",
    "\n",
    "    return bar_update\n",
    "\n",
    "\n",
    "def download_url(\n",
    "    url: str, root: str, filename: Optional[str] = None, md5: Optional[str] = None\n",
    ") -> None:\n",
    "    \"\"\"Download a file from a url and place it in root.\n",
    "    Args:\n",
    "        url (str): URL to download file from\n",
    "        root (str): Directory to place downloaded file in\n",
    "        filename (str, optional): Name to save the file under. If None, use the basename of the URL\n",
    "        md5 (str, optional): MD5 checksum of the download. If None, do not check\n",
    "    \"\"\"\n",
    "    import urllib\n",
    "\n",
    "    root = os.path.expanduser(root)\n",
    "    if not filename:\n",
    "        filename = os.path.basename(url)\n",
    "    fpath = os.path.join(root, filename)\n",
    "\n",
    "    os.makedirs(root, exist_ok=True)\n",
    "\n",
    "    # check if file is already present locally\n",
    "    if check_integrity(fpath, md5):\n",
    "        print(\"Using downloaded and verified file: \" + fpath)\n",
    "    else:  # download the file\n",
    "        try:\n",
    "            print(\"Downloading \" + url + \" to \" + fpath)\n",
    "            urllib.request.urlretrieve(url, fpath, reporthook=gen_bar_updater())\n",
    "        except (urllib.error.URLError, IOError) as e:  # type: ignore[attr-defined]\n",
    "            if url[:5] == \"https\":\n",
    "                url = url.replace(\"https:\", \"http:\")\n",
    "                print(\n",
    "                    \"Failed download. Trying https -> http instead.\"\n",
    "                    \" Downloading \" + url + \" to \" + fpath\n",
    "                )\n",
    "                urllib.request.urlretrieve(url, fpath, reporthook=gen_bar_updater())\n",
    "            else:\n",
    "                raise e\n",
    "        # check integrity of downloaded file\n",
    "        if not check_integrity(fpath, md5):\n",
    "            raise RuntimeError(\"File not found or corrupted.\")\n",
    "\n",
    "\n",
    "def _is_tarxz(filename: str) -> bool:\n",
    "    return filename.endswith(\".tar.xz\")\n",
    "\n",
    "\n",
    "def _is_tar(filename: str) -> bool:\n",
    "    return filename.endswith(\".tar\")\n",
    "\n",
    "\n",
    "def _is_targz(filename: str) -> bool:\n",
    "    return filename.endswith(\".tar.gz\")\n",
    "\n",
    "\n",
    "def _is_tgz(filename: str) -> bool:\n",
    "    return filename.endswith(\".tgz\")\n",
    "\n",
    "\n",
    "def _is_gzip(filename: str) -> bool:\n",
    "    return filename.endswith(\".gz\") and not filename.endswith(\".tar.gz\")\n",
    "\n",
    "\n",
    "def _is_zip(filename: str) -> bool:\n",
    "    return filename.endswith(\".zip\")\n",
    "\n",
    "\n",
    "def extract_archive(\n",
    "    from_path: str, to_path: Optional[str] = None, remove_finished: bool = False\n",
    ") -> None:\n",
    "    if to_path is None:\n",
    "        to_path = os.path.dirname(from_path)\n",
    "\n",
    "    if _is_tar(from_path):\n",
    "        with tarfile.open(from_path, \"r\") as tar:\n",
    "\n",
    "            def is_within_directory(directory, target):\n",
    "\n",
    "                abs_directory = os.path.abspath(directory)\n",
    "                abs_target = os.path.abspath(target)\n",
    "\n",
    "                prefix = os.path.commonprefix([abs_directory, abs_target])\n",
    "\n",
    "                return prefix == abs_directory\n",
    "\n",
    "            def safe_extract(tar, path=\".\", members=None, *, numeric_owner=False):\n",
    "\n",
    "                for member in tar.getmembers():\n",
    "                    member_path = os.path.join(path, member.name)\n",
    "                    if not is_within_directory(path, member_path):\n",
    "                        raise Exception(\"Attempted Path Traversal in Tar File\")\n",
    "\n",
    "                tar.extractall(path, members, numeric_owner=numeric_owner)\n",
    "\n",
    "            safe_extract(tar, path=to_path)\n",
    "    elif _is_targz(from_path) or _is_tgz(from_path):\n",
    "        with tarfile.open(from_path, \"r:gz\") as tar:\n",
    "\n",
    "            def is_within_directory(directory, target):\n",
    "\n",
    "                abs_directory = os.path.abspath(directory)\n",
    "                abs_target = os.path.abspath(target)\n",
    "\n",
    "                prefix = os.path.commonprefix([abs_directory, abs_target])\n",
    "\n",
    "                return prefix == abs_directory\n",
    "\n",
    "            def safe_extract(tar, path=\".\", members=None, *, numeric_owner=False):\n",
    "\n",
    "                for member in tar.getmembers():\n",
    "                    member_path = os.path.join(path, member.name)\n",
    "                    if not is_within_directory(path, member_path):\n",
    "                        raise Exception(\"Attempted Path Traversal in Tar File\")\n",
    "\n",
    "                tar.extractall(path, members, numeric_owner=numeric_owner)\n",
    "\n",
    "            safe_extract(tar, path=to_path)\n",
    "    elif _is_tarxz(from_path):\n",
    "        with tarfile.open(from_path, \"r:xz\") as tar:\n",
    "\n",
    "            def is_within_directory(directory, target):\n",
    "\n",
    "                abs_directory = os.path.abspath(directory)\n",
    "                abs_target = os.path.abspath(target)\n",
    "\n",
    "                prefix = os.path.commonprefix([abs_directory, abs_target])\n",
    "\n",
    "                return prefix == abs_directory\n",
    "\n",
    "            def safe_extract(tar, path=\".\", members=None, *, numeric_owner=False):\n",
    "\n",
    "                for member in tar.getmembers():\n",
    "                    member_path = os.path.join(path, member.name)\n",
    "                    if not is_within_directory(path, member_path):\n",
    "                        raise Exception(\"Attempted Path Traversal in Tar File\")\n",
    "\n",
    "                tar.extractall(path, members, numeric_owner=numeric_owner)\n",
    "\n",
    "            safe_extract(tar, path=to_path)\n",
    "    elif _is_gzip(from_path):\n",
    "        to_path = os.path.join(\n",
    "            to_path, os.path.splitext(os.path.basename(from_path))[0]\n",
    "        )\n",
    "        with open(to_path, \"wb\") as out_f, gzip.GzipFile(from_path) as zip_f:\n",
    "            out_f.write(zip_f.read())\n",
    "    elif _is_zip(from_path):\n",
    "        with zipfile.ZipFile(from_path, \"r\") as z:\n",
    "            z.extractall(to_path)\n",
    "    else:\n",
    "        raise ValueError(\"Extraction of {} not supported\".format(from_path))\n",
    "\n",
    "    if remove_finished:\n",
    "        os.remove(from_path)\n",
    "\n",
    "\n",
    "def download_and_extract_archive(\n",
    "    url: str,\n",
    "    download_root: str,\n",
    "    extract_root: Optional[str] = None,\n",
    "    filename: Optional[str] = None,\n",
    "    md5: Optional[str] = None,\n",
    "    remove_finished: bool = False,\n",
    ") -> None:\n",
    "    download_root = os.path.expanduser(download_root)\n",
    "    if extract_root is None:\n",
    "        extract_root = download_root\n",
    "    if not filename:\n",
    "        filename = os.path.basename(url)\n",
    "\n",
    "    download_url(url, download_root, filename, md5)\n",
    "\n",
    "    archive = os.path.join(download_root, filename)\n",
    "    print(\"Extracting {} to {}\".format(archive, extract_root))\n",
    "    extract_archive(archive, extract_root, remove_finished)\n",
    "\n",
    "\n",
    "class FEMNIST(MNIST):\n",
    "    \"\"\"\n",
    "    This dataset is derived from the Leaf repository\n",
    "    (https://github.com/TalwalkarLab/leaf) pre-processing of the Extended MNIST\n",
    "    dataset, grouping examples by writer. Details about Leaf were published in\n",
    "    \"LEAF: A Benchmark for Federated Settings\" https://arxiv.org/abs/1812.01097.\n",
    "    \"\"\"\n",
    "\n",
    "    resources = [\n",
    "        (\n",
    "            \"https://raw.githubusercontent.com/tao-shen/FEMNIST_pytorch/master/femnist.tar.gz\",\n",
    "            \"59c65cec646fc57fe92d27d83afdf0ed\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        dataidxs=None,\n",
    "        train=True,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "        download=False,\n",
    "    ):\n",
    "        super(MNIST, self).__init__(\n",
    "            root, transform=transform, target_transform=target_transform\n",
    "        )\n",
    "        self.train = train\n",
    "        self.dataidxs = dataidxs\n",
    "\n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        if not self._check_exists():\n",
    "            raise RuntimeError(\n",
    "                \"Dataset not found.\" + \" You can use download=True to download it\"\n",
    "            )\n",
    "        if self.train:\n",
    "            data_file = self.training_file\n",
    "        else:\n",
    "            data_file = self.test_file\n",
    "\n",
    "        self.data, self.targets, self.users_index = torch.load(\n",
    "            os.path.join(self.processed_folder, data_file)\n",
    "        )\n",
    "\n",
    "        if self.dataidxs is not None:\n",
    "            self.data = self.data[self.dataidxs]\n",
    "            self.targets = self.targets[self.dataidxs]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.data[index], int(self.targets[index])\n",
    "        img = Image.fromarray(img.numpy(), mode=\"F\")\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        return img, target\n",
    "\n",
    "    def download(self):\n",
    "        \"\"\"Download the FEMNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
    "        import shutil\n",
    "\n",
    "        if self._check_exists():\n",
    "            return\n",
    "\n",
    "        mkdirs(self.raw_folder)\n",
    "        mkdirs(self.processed_folder)\n",
    "\n",
    "        # download files\n",
    "        for url, md5 in self.resources:\n",
    "            filename = url.rpartition(\"/\")[2]\n",
    "            download_and_extract_archive(\n",
    "                url, download_root=self.raw_folder, filename=filename, md5=md5\n",
    "            )\n",
    "\n",
    "        # process and save as torch files\n",
    "        print(\"Processing...\")\n",
    "        shutil.move(\n",
    "            os.path.join(self.raw_folder, self.training_file), self.processed_folder\n",
    "        )\n",
    "        shutil.move(\n",
    "            os.path.join(self.raw_folder, self.test_file), self.processed_folder\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _check_exists(self) -> bool:\n",
    "        return all(\n",
    "            check_integrity(\n",
    "                os.path.join(\n",
    "                    self.raw_folder,\n",
    "                    os.path.splitext(os.path.basename(url))[0]\n",
    "                    + os.path.splitext(os.path.basename(url))[1],\n",
    "                )\n",
    "            )\n",
    "            for url, _ in self.resources\n",
    "        )\n",
    "\n",
    "\n",
    "class Generated(MNIST):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        dataidxs=None,\n",
    "        train=True,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "        download=False,\n",
    "    ):\n",
    "        super(MNIST, self).__init__(\n",
    "            root, transform=transform, target_transform=target_transform\n",
    "        )\n",
    "        self.train = train\n",
    "        self.dataidxs = dataidxs\n",
    "\n",
    "        if self.train:\n",
    "            self.data = np.load(\"data/generated/X_train.npy\")\n",
    "            self.targets = np.load(\"data/generated/y_train.npy\")\n",
    "        else:\n",
    "            self.data = np.load(\"data/generated/X_test.npy\")\n",
    "            self.targets = np.load(\"data/generated/y_test.npy\")\n",
    "\n",
    "        if self.dataidxs is not None:\n",
    "            self.data = self.data[self.dataidxs]\n",
    "            self.targets = self.targets[self.dataidxs]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data, target = self.data[index], self.targets[index]\n",
    "        return data, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "class genData(MNIST):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data, target = self.data[index], self.targets[index]\n",
    "        return data, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "class CIFAR100_truncated(data.Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        dataidxs=None,\n",
    "        train=True,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "        download=False,\n",
    "    ):\n",
    "\n",
    "        self.root = root\n",
    "        self.dataidxs = dataidxs\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.download = download\n",
    "\n",
    "        self.data, self.target = self.__build_truncated_dataset__()\n",
    "\n",
    "    def __build_truncated_dataset__(self):\n",
    "\n",
    "        cifar_dataobj = CIFAR100(\n",
    "            self.root, self.train, self.transform, self.target_transform, self.download\n",
    "        )\n",
    "\n",
    "        if torchvision.__version__ == \"0.2.1\":\n",
    "            if self.train:\n",
    "                data, target = cifar_dataobj.train_data, np.array(\n",
    "                    cifar_dataobj.train_labels\n",
    "                )\n",
    "            else:\n",
    "                data, target = cifar_dataobj.test_data, np.array(\n",
    "                    cifar_dataobj.test_labels\n",
    "                )\n",
    "        else:\n",
    "            data = cifar_dataobj.data\n",
    "            target = np.array(cifar_dataobj.targets)\n",
    "\n",
    "        if self.dataidxs is not None:\n",
    "            data = data[self.dataidxs]\n",
    "            target = target[self.dataidxs]\n",
    "\n",
    "        return data, target\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.target[index]\n",
    "        img = Image.fromarray(img)\n",
    "        # print(\"cifar10 img:\", img)\n",
    "        # print(\"cifar10 target:\", target)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "class ImageFolder_custom(DatasetFolder):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        dataidxs=None,\n",
    "        train=True,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "        download=None,\n",
    "    ):\n",
    "        self.root = root\n",
    "        self.dataidxs = dataidxs\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        imagefolder_obj = ImageFolder(self.root, self.transform, self.target_transform)\n",
    "        self.loader = imagefolder_obj.loader\n",
    "        if self.dataidxs is not None:\n",
    "            self.samples = np.array(imagefolder_obj.samples)[self.dataidxs]\n",
    "        else:\n",
    "            self.samples = np.array(imagefolder_obj.samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.samples[index][0]\n",
    "        target = self.samples[index][1]\n",
    "        target = int(target)\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return sample, target\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.dataidxs is None:\n",
    "            return len(self.samples)\n",
    "        else:\n",
    "            return len(self.dataidxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T15:34:58.180294Z",
     "start_time": "2025-02-01T15:34:58.164557Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-15T16:40:36.932100Z",
     "iopub.status.busy": "2025-03-15T16:40:36.931864Z",
     "iopub.status.idle": "2025-03-15T16:40:36.952718Z",
     "shell.execute_reply": "2025-03-15T16:40:36.951933Z",
     "shell.execute_reply.started": "2025-03-15T16:40:36.932075Z"
    },
    "id": "27nyJr8n3JuE",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def mkdirs(dirpath):\n",
    "    try:\n",
    "        os.makedirs(dirpath)\n",
    "    except Exception as _:\n",
    "        pass\n",
    "\n",
    "\n",
    "def load_mnist_data(datadir):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    mnist_train_ds = MNIST_truncated(\n",
    "        datadir, train=True, download=True, transform=transform\n",
    "    )\n",
    "    mnist_test_ds = MNIST_truncated(\n",
    "        datadir, train=False, download=True, transform=transform\n",
    "    )\n",
    "    X_train, y_train = mnist_train_ds.data, mnist_train_ds.target\n",
    "    X_test, y_test = mnist_test_ds.data, mnist_test_ds.target\n",
    "    X_train = X_train.data.numpy()\n",
    "    y_train = y_train.data.numpy()\n",
    "    X_test = X_test.data.numpy()\n",
    "    y_test = y_test.data.numpy()\n",
    "    return (X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "def load_fmnist_data(datadir):\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    "    )\n",
    "    mnist_train_ds = FashionMNIST_truncated(\n",
    "        datadir, train=True, download=True, transform=transform\n",
    "    )\n",
    "    mnist_test_ds = FashionMNIST_truncated(\n",
    "        datadir, train=False, download=True, transform=transform\n",
    "    )\n",
    "    X_train, y_train = mnist_train_ds.data, mnist_train_ds.target\n",
    "    X_test, y_test = mnist_test_ds.data, mnist_test_ds.target\n",
    "    X_train = X_train.data.numpy()\n",
    "    y_train = y_train.data.numpy()\n",
    "    X_test = X_test.data.numpy()\n",
    "    y_test = y_test.data.numpy()\n",
    "    return (X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "def load_svhn_data(datadir):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(\n",
    "                (\n",
    "                    SAFE_PFL_CONFIG[\"TRANSFORM_INPUT_SIZE\"],\n",
    "                    SAFE_PFL_CONFIG[\"TRANSFORM_INPUT_SIZE\"],\n",
    "                )\n",
    "            ),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "        ]\n",
    "    )\n",
    "    svhn_train_ds = SVHN_custom(datadir, train=True, download=True, transform=transform)\n",
    "    svhn_test_ds = SVHN_custom(datadir, train=False, download=True, transform=transform)\n",
    "    X_train, y_train = svhn_train_ds.data, svhn_train_ds.target\n",
    "    X_test, y_test = svhn_test_ds.data, svhn_test_ds.target\n",
    "    # X_train = X_train.data.numpy()\n",
    "    # y_train = y_train.data.numpy()\n",
    "    # X_test = X_test.data.numpy()\n",
    "    # y_test = y_test.data.numpy()\n",
    "    return (X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "def load_cifar10_data(datadir):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ]\n",
    "    )\n",
    "    cifar10_train_ds = CIFAR10_truncated(\n",
    "        datadir, train=True, download=True, transform=transform\n",
    "    )\n",
    "    cifar10_test_ds = CIFAR10_truncated(\n",
    "        datadir, train=False, download=True, transform=transform\n",
    "    )\n",
    "    X_train, y_train = cifar10_train_ds.data, cifar10_train_ds.target\n",
    "    X_test, y_test = cifar10_test_ds.data, cifar10_test_ds.target\n",
    "\n",
    "    return (X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "def load_celeba_data(datadir):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    celeba_train_ds = CelebA_custom(\n",
    "        datadir, split=\"train\", target_type=\"attr\", download=True, transform=transform\n",
    "    )\n",
    "    celeba_test_ds = CelebA_custom(\n",
    "        datadir, split=\"test\", target_type=\"attr\", download=True, transform=transform\n",
    "    )\n",
    "    gender_index = celeba_train_ds.attr_names.index(\"Male\")\n",
    "    y_train = celeba_train_ds.attr[:, gender_index : gender_index + 1].reshape(-1)\n",
    "    y_test = celeba_test_ds.attr[:, gender_index : gender_index + 1].reshape(-1)\n",
    "    # y_train = y_train.numpy()\n",
    "    # y_test = y_test.numpy()\n",
    "    return (None, y_train, None, y_test)\n",
    "\n",
    "\n",
    "def load_femnist_data(datadir):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    mnist_train_ds = FEMNIST(datadir, train=True, transform=transform, download=True)\n",
    "    mnist_test_ds = FEMNIST(datadir, train=False, transform=transform, download=True)\n",
    "    X_train, y_train, u_train = (\n",
    "        mnist_train_ds.data,\n",
    "        mnist_train_ds.targets,\n",
    "        mnist_train_ds.users_index,\n",
    "    )\n",
    "    X_test, y_test, u_test = (\n",
    "        mnist_test_ds.data,\n",
    "        mnist_test_ds.targets,\n",
    "        mnist_test_ds.users_index,\n",
    "    )\n",
    "    X_train = X_train.data.numpy()\n",
    "    y_train = y_train.data.numpy()\n",
    "    u_train = np.array(u_train)\n",
    "    X_test = X_test.data.numpy()\n",
    "    y_test = y_test.data.numpy()\n",
    "    u_test = np.array(u_test)\n",
    "    return (X_train, y_train, u_train, X_test, y_test, u_test)\n",
    "\n",
    "\n",
    "def load_cifar100_data(datadir):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    cifar100_train_ds = CIFAR100_truncated(\n",
    "        datadir, train=True, download=True, transform=transform\n",
    "    )\n",
    "    cifar100_test_ds = CIFAR100_truncated(\n",
    "        datadir, train=False, download=True, transform=transform\n",
    "    )\n",
    "    X_train, y_train = cifar100_train_ds.data, cifar100_train_ds.target\n",
    "    X_test, y_test = cifar100_test_ds.data, cifar100_test_ds.target\n",
    "    # y_train = y_train.numpy()\n",
    "    # y_test = y_test.numpy()\n",
    "    return (X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "def load_tinyimagenet_data(datadir):\n",
    "    split = \"val\"\n",
    "    TinyImageNet(datadir, split=split)\n",
    "    transform_train = transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomCrop(64, padding=4),  # Random cropping with padding\n",
    "            transforms.RandomHorizontalFlip(),  # Horizontal flip\n",
    "            transforms.RandomRotation(15),  # Random rotation\n",
    "            transforms.ColorJitter(\n",
    "                brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1\n",
    "            ),  # Color jitter\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.4802, 0.4481, 0.3975], std=[0.2302, 0.2265, 0.2262]\n",
    "            ),  # Normalization\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    transform_test = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.4802, 0.4481, 0.3975], std=[0.2302, 0.2265, 0.2262]\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    # transform = transforms.Compose([transforms.ToTensor()])\n",
    "    xray_train_ds = ImageFolder_custom(\n",
    "        datadir + \"tiny-imagenet-200/train/\", transform=transform_train\n",
    "    )\n",
    "    xray_test_ds = ImageFolder_custom(\n",
    "        datadir + \"tiny-imagenet-200/val/\", transform=transform_test\n",
    "    )\n",
    "    X_train, y_train = np.array([s[0] for s in xray_train_ds.samples]), np.array(\n",
    "        [int(s[1]) for s in xray_train_ds.samples]\n",
    "    )\n",
    "    X_test, y_test = np.array([s[0] for s in xray_test_ds.samples]), np.array(\n",
    "        [int(s[1]) for s in xray_test_ds.samples]\n",
    "    )\n",
    "    return (X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "def load_stl10_data(datadir):\n",
    "    transform_train = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(\n",
    "                (\n",
    "                    SAFE_PFL_CONFIG[\"TRANSFORM_INPUT_SIZE\"],\n",
    "                    SAFE_PFL_CONFIG[\"TRANSFORM_INPUT_SIZE\"],\n",
    "                )\n",
    "            ),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "        ]\n",
    "    )\n",
    "    transform_test = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(\n",
    "                (\n",
    "                    SAFE_PFL_CONFIG[\"TRANSFORM_INPUT_SIZE\"],\n",
    "                    SAFE_PFL_CONFIG[\"TRANSFORM_INPUT_SIZE\"],\n",
    "                )\n",
    "            ),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    stl10_train_ds = STL10_truncated(\n",
    "        datadir, split=\"train\", transform=transform_train, download=True\n",
    "    )\n",
    "    stl10_test_ds = STL10_truncated(\n",
    "        datadir, split=\"test\", transform=transform_test, download=True\n",
    "    )\n",
    "\n",
    "    X_train, y_train = stl10_train_ds.data, stl10_train_ds.target\n",
    "    X_test, y_test = stl10_test_ds.data, stl10_test_ds.target\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def record_net_data_stats(y_train, net_dataidx_map, logdir):\n",
    "    net_cls_counts = {}\n",
    "    for net_i, dataidx in net_dataidx_map.items():\n",
    "        unq, unq_cnt = np.unique(y_train[dataidx], return_counts=True)\n",
    "        tmp = {unq[i]: unq_cnt[i] for i in range(len(unq))}\n",
    "        net_cls_counts[net_i] = tmp\n",
    "    log.info(\"Data statistics: %s\" % str(net_cls_counts))\n",
    "    return net_cls_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T15:34:58.236480Z",
     "start_time": "2025-02-01T15:34:58.221212Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-15T16:40:36.953894Z",
     "iopub.status.busy": "2025-03-15T16:40:36.953620Z",
     "iopub.status.idle": "2025-03-15T16:40:36.974723Z",
     "shell.execute_reply": "2025-03-15T16:40:36.974031Z",
     "shell.execute_reply.started": "2025-03-15T16:40:36.953864Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def partition_data(dataset, datadir, logdir, partition, n_parties, beta=0.1):\n",
    "    test_dataidx_map = {}\n",
    "\n",
    "    # Load dataset\n",
    "    if dataset == \"mnist\":\n",
    "        X_train, y_train, X_test, y_test = load_mnist_data(datadir)\n",
    "    elif dataset == \"fmnist\":\n",
    "        X_train, y_train, X_test, y_test = load_fmnist_data(datadir)\n",
    "    elif dataset == \"cifar10\":\n",
    "        X_train, y_train, X_test, y_test = load_cifar10_data(datadir)\n",
    "    elif dataset == \"svhn\":\n",
    "        X_train, y_train, X_test, y_test = load_svhn_data(datadir)\n",
    "    elif dataset == \"celeba\":\n",
    "        X_train, y_train, X_test, y_test = load_celeba_data(datadir)\n",
    "    elif dataset == \"femnist\":\n",
    "        X_train, y_train, u_train, X_test, y_test, u_test = load_femnist_data(datadir)\n",
    "    elif dataset == \"cifar100\":\n",
    "        X_train, y_train, X_test, y_test = load_cifar100_data(datadir)\n",
    "    elif dataset == \"tinyimagenet\":\n",
    "        X_train, y_train, X_test, y_test = load_tinyimagenet_data(datadir)\n",
    "    elif dataset == \"STL10\":\n",
    "        X_train, y_train, X_test, y_test = load_stl10_data(datadir)\n",
    "    elif dataset == \"generated\":\n",
    "        # Code for generated dataset (omitted for brevity)\n",
    "        pass\n",
    "    # Add other datasets if needed\n",
    "\n",
    "    n_train = y_train.shape[0]\n",
    "    n_test = y_test.shape[0]\n",
    "    \n",
    "    # Partition the data\n",
    "    if partition == \"homo\":\n",
    "        # Homogeneous data partition\n",
    "        idxs = np.random.permutation(n_train)\n",
    "        batch_idxs = np.array_split(idxs, n_parties)\n",
    "        net_dataidx_map = {i: batch_idxs[i] for i in range(n_parties)}\n",
    "\n",
    "    elif partition == \"noniid-labeldir\":\n",
    "        min_size = 0\n",
    "        min_require_size = 10  # Minimum number required for each party\n",
    "        if dataset == \"cifar100\":\n",
    "            K = 100  # Number of classes\n",
    "        else:\n",
    "            k = 10\n",
    "            K = 10\n",
    "\n",
    "        N = y_train.shape[0]\n",
    "        net_dataidx_map = {}\n",
    "        test_dataidx_map = {}  # Make sure to initialize this\n",
    "\n",
    "        while min_size < min_require_size:\n",
    "            idx_batch = [[] for _ in range(n_parties)]\n",
    "            for k in range(K):\n",
    "                idx_k = np.where(y_train == k)[0]\n",
    "                np.random.shuffle(idx_k)\n",
    "                proportions = np.random.dirichlet(np.repeat(beta, n_parties))\n",
    "                proportions = np.array(\n",
    "                    [\n",
    "                        p * (len(idx_j) < N / n_parties)\n",
    "                        for p, idx_j in zip(proportions, idx_batch)\n",
    "                    ]\n",
    "                )\n",
    "                proportions = proportions / proportions.sum()  # Normalize\n",
    "                proportions = (np.cumsum(proportions) * len(idx_k)).astype(int)[:-1]\n",
    "                idx_batch = [\n",
    "                    idx_j + idx.tolist()\n",
    "                    for idx_j, idx in zip(idx_batch, np.split(idx_k, proportions))\n",
    "                ]\n",
    "\n",
    "            min_size = min([len(idx_j) for idx_j in idx_batch])\n",
    "\n",
    "        for j in range(n_parties):\n",
    "            np.random.shuffle(idx_batch[j])\n",
    "            net_dataidx_map[j] = idx_batch[j]\n",
    "\n",
    "            # Initialize test_dataidx_map for current party\n",
    "            test_dataidx_map[j] = []\n",
    "\n",
    "            # Gather test indices for current party based on labels in net_dataidx_map[j]\n",
    "            for k in range(K):\n",
    "                if k in y_train[net_dataidx_map[j]]:\n",
    "                    # Access test indices for class k\n",
    "                    idx_test_k = np.where(y_test == k)[0]\n",
    "                    np.random.shuffle(idx_test_k)\n",
    "\n",
    "                    # The number of sample for each party based on training set size\n",
    "                    n_samples = int(len(net_dataidx_map[j]) * len(idx_test_k) / N)\n",
    "                    test_dataidx_map[j].extend(idx_test_k[:n_samples])\n",
    "\n",
    "            test_dataidx_map[j] = np.array(test_dataidx_map[j])\n",
    "\n",
    "        # Cleanup to avoid empty concatenation error\n",
    "        for j in range(n_parties):\n",
    "            if len(test_dataidx_map[j]) == 0:\n",
    "                test_dataidx_map[j] = np.array(\n",
    "                    []\n",
    "                )  # Set to an empty array to avoid errors later\n",
    "\n",
    "    elif partition == \"noniid-fix\":\n",
    "        # Custom fixed distribution logic\n",
    "        desired_distribution = SAFE_PFL_CONFIG[\"DESIRED_DISTRIBUTION\"]\n",
    "\n",
    "        # Number of clients and classes\n",
    "        num_clients = len(desired_distribution)\n",
    "        num_classes = len(desired_distribution[0])\n",
    "\n",
    "        assert num_clients == SAFE_PFL_CONFIG[\"NUMBER_OF_CLIENTS\"]\n",
    "        assert num_classes == SAFE_PFL_CONFIG[\"NUMBER_OF_CLASSES\"]\n",
    "\n",
    "        ##Initialize the data indices for each client\n",
    "        net_dataidx_map = {i: [] for i in range(num_clients)}\n",
    "        # Iterate over each class and assign samples to clients based on the desired distribution\n",
    "        for class_idx in range(num_classes):\n",
    "            # Get the indices of all samples belonging to the current class\n",
    "            class_indices = np.where(y_train == class_idx)[0]\n",
    "\n",
    "            # Shuffle the indices to ensure randomness\n",
    "            np.random.shuffle(class_indices)\n",
    "\n",
    "            # Assign samples to clients based on the desired distribution\n",
    "            start_idx = 0\n",
    "            for client_idx in range(num_clients):\n",
    "                num_samples = desired_distribution[client_idx][class_idx]\n",
    "                if num_samples > 0:\n",
    "                    end_idx = start_idx + num_samples\n",
    "                    net_dataidx_map[client_idx].extend(class_indices[start_idx:end_idx])\n",
    "                    start_idx = end_idx\n",
    "\n",
    "        # Initialize test_dataidx_map for each client\n",
    "        for j in range(num_clients):\n",
    "            test_dataidx_map[j] = []\n",
    "\n",
    "            # Gather test indices for current party based on labels in net_dataidx_map[j]\n",
    "            for k in range(num_classes):\n",
    "                if k in y_train[net_dataidx_map[j]]:\n",
    "                    # Access test indices for class k\n",
    "                    idx_test_k = np.where(y_test == k)[0]\n",
    "                    np.random.shuffle(idx_test_k)\n",
    "\n",
    "                    # The number of samples for each party based on training set size\n",
    "                    n_samples = max(1, int(len(net_dataidx_map[j]) * len(idx_test_k) / n_train))\n",
    "                    # n_samples = min(n_samples, len(idx_test_k))  # Ensure we don't exceed available samples\n",
    "                    test_dataidx_map[j].extend(idx_test_k[:n_samples])\n",
    "\n",
    "            test_dataidx_map[j] = np.array(test_dataidx_map[j])\n",
    "\n",
    "        # Cleanup to avoid empty concatenation error\n",
    "        for j in range(num_clients):\n",
    "            if len(test_dataidx_map[j]) == 0:\n",
    "                test_dataidx_map[j] = np.array(\n",
    "                    []\n",
    "                )  # Set to an empty array to avoid errors later\n",
    "\n",
    "    elif partition.startswith(\"noniid-#label\") and partition[13:].isdigit():\n",
    "        # Existing logic for noniid-#label partitioning\n",
    "        num = int(partition[13:])\n",
    "        if dataset in (\"celeba\", \"covtype\", \"a9a\", \"rcv1\", \"SUSY\"):\n",
    "            num = 1\n",
    "            K = 2\n",
    "        else:\n",
    "            if dataset == \"cifar100\":\n",
    "                K = 100\n",
    "            elif dataset == \"tinyimagenet\":\n",
    "                K = 200\n",
    "            else:\n",
    "                K = 10\n",
    "        if num == K:\n",
    "            # IID partition\n",
    "            net_dataidx_map = {\n",
    "                i: np.ndarray(0, dtype=np.int64) for i in range(n_parties)\n",
    "            }\n",
    "            for i in range(K):\n",
    "                idx_k = np.where(y_train == i)[0]\n",
    "                np.random.shuffle(idx_k)\n",
    "                split = np.array_split(idx_k, n_parties)\n",
    "                for j in range(n_parties):\n",
    "                    net_dataidx_map[j] = np.append(net_dataidx_map[j], split[j])\n",
    "        else:\n",
    "            times = [0 for _ in range(K)]\n",
    "            contain = []\n",
    "            for i in range(n_parties):\n",
    "                current = [i % K]\n",
    "                times[i % K] += 1\n",
    "                j = 1\n",
    "                while j < num:\n",
    "                    ind = random.randint(0, K - 1)\n",
    "                    if ind not in current:\n",
    "                        j += 1\n",
    "                        current.append(ind)\n",
    "                        times[ind] += 1\n",
    "                contain.append(current)\n",
    "            net_dataidx_map = {\n",
    "                i: np.ndarray(0, dtype=np.int64) for i in range(n_parties)\n",
    "            }\n",
    "            test_dataidx_map = {\n",
    "                i: np.ndarray(0, dtype=np.int64) for i in range(n_parties)\n",
    "            }\n",
    "            for i in range(K):\n",
    "                if times[i] > 0:\n",
    "                    idx_k = np.where(y_train == i)[0]\n",
    "                    idx_t = np.where(y_test == i)[0]\n",
    "                    np.random.shuffle(idx_k)\n",
    "                    np.random.shuffle(idx_t)\n",
    "                    split = np.array_split(idx_k, times[i])\n",
    "                    splitt = np.array_split(idx_t, times[i])\n",
    "                    ids = 0\n",
    "                    for j in range(n_parties):\n",
    "                        if i in contain[j]:\n",
    "                            net_dataidx_map[j] = np.append(\n",
    "                                net_dataidx_map[j], split[ids]\n",
    "                            )\n",
    "                            test_dataidx_map[j] = np.append(\n",
    "                                test_dataidx_map[j], splitt[ids]\n",
    "                            )\n",
    "                            ids += 1\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown partition method: {partition}\")\n",
    "\n",
    "    # Record the data statistics\n",
    "    traindata_cls_counts = record_net_data_stats(y_train, net_dataidx_map, logdir)\n",
    "\n",
    "    return (\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        net_dataidx_map,\n",
    "        test_dataidx_map,\n",
    "        traindata_cls_counts,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T15:34:58.296280Z",
     "start_time": "2025-02-01T15:34:58.284280Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-15T16:40:36.975764Z",
     "iopub.status.busy": "2025-03-15T16:40:36.975529Z",
     "iopub.status.idle": "2025-03-15T16:40:36.995552Z",
     "shell.execute_reply": "2025-03-15T16:40:36.994788Z",
     "shell.execute_reply.started": "2025-03-15T16:40:36.975734Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0.0, std=1.0, net_id=None, total=0):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        self.net_id = net_id\n",
    "        self.num = int(sqrt(total))\n",
    "        if self.num * self.num < total:\n",
    "            self.num = self.num + 1\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        if self.net_id is None:\n",
    "            return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "        else:\n",
    "            tmp = torch.randn(tensor.size())\n",
    "            filt = torch.zeros(tensor.size())\n",
    "            size = int(28 / self.num)\n",
    "            row = int(self.net_id / size)\n",
    "            col = self.net_id % size\n",
    "            for i in range(size):\n",
    "                for j in range(size):\n",
    "                    filt[:, row * size + i, col * size + j] = 1\n",
    "            tmp = tmp * filt\n",
    "            return tensor + tmp * self.std + self.mean\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \"(mean={0}, std={1})\".format(\n",
    "            self.mean, self.std\n",
    "        )\n",
    "\n",
    "\n",
    "def get_dataloader(\n",
    "    dataset,\n",
    "    datadir,\n",
    "    train_bs,\n",
    "    test_bs,\n",
    "    dataidxs=None,\n",
    "    testidxs=None,\n",
    "    noise_level=0,\n",
    "    net_id=None,\n",
    "    total=0,\n",
    "):\n",
    "    if dataset in (\n",
    "        \"mnist\",\n",
    "        \"femnist\",\n",
    "        \"fmnist\",\n",
    "        \"cifar10\",\n",
    "        \"svhn\",\n",
    "        \"generated\",\n",
    "        \"covtype\",\n",
    "        \"a9a\",\n",
    "        \"rcv1\",\n",
    "        \"SUSY\",\n",
    "        \"cifar100\",\n",
    "        \"tinyimagenet\",\n",
    "        \"STL10\",\n",
    "    ):\n",
    "        if dataset == \"mnist\":\n",
    "            dl_obj = MNIST_truncated\n",
    "            transform_train = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    AddGaussianNoise(0.0, noise_level, net_id, total),\n",
    "                ]\n",
    "            )\n",
    "            transform_test = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    AddGaussianNoise(0.0, noise_level, net_id, total),\n",
    "                ]\n",
    "            )\n",
    "        elif dataset == \"femnist\":\n",
    "            dl_obj = FEMNIST\n",
    "            transform_train = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    AddGaussianNoise(0.0, noise_level, net_id, total),\n",
    "                ]\n",
    "            )\n",
    "            transform_test = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    AddGaussianNoise(0.0, noise_level, net_id, total),\n",
    "                ]\n",
    "            )\n",
    "        elif dataset == \"fmnist\":\n",
    "            dl_obj = FashionMNIST_truncated\n",
    "            transform_train = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5,), (0.5,)),\n",
    "                ]\n",
    "            )\n",
    "            transform_test = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5,), (0.5,)),\n",
    "                ]\n",
    "            )\n",
    "        elif dataset == \"svhn\":\n",
    "            dl_obj = SVHN_custom\n",
    "            transform_train = transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize(\n",
    "                        (\n",
    "                            SAFE_PFL_CONFIG[\"TRANSFORM_INPUT_SIZE\"],\n",
    "                            SAFE_PFL_CONFIG[\"TRANSFORM_INPUT_SIZE\"],\n",
    "                        )\n",
    "                    ),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "                ]\n",
    "            )\n",
    "            transform_test = transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize(\n",
    "                        (\n",
    "                            SAFE_PFL_CONFIG[\"TRANSFORM_INPUT_SIZE\"],\n",
    "                            SAFE_PFL_CONFIG[\"TRANSFORM_INPUT_SIZE\"],\n",
    "                        )\n",
    "                    ),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        elif dataset == \"cifar10\":\n",
    "            dl_obj = CIFAR10_truncated\n",
    "            log.warn(\"test me please! CIFAR10_truncated\")\n",
    "            transform_train = transforms.Compose(\n",
    "                [\n",
    "                    # transforms.Resize((224,224)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Lambda(\n",
    "                        lambda x: F.pad(\n",
    "                            Variable(x.unsqueeze(0), requires_grad=False),\n",
    "                            (4, 4, 4, 4),\n",
    "                            mode=\"reflect\",\n",
    "                        ).data.squeeze()\n",
    "                    ),\n",
    "                    transforms.ToPILImage(),\n",
    "                    transforms.RandomCrop(32),\n",
    "                    transforms.ToTensor(),\n",
    "                    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                ]\n",
    "            )\n",
    "            transform_test = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                ]\n",
    "            )\n",
    "        elif dataset == \"cifar100\":\n",
    "            print(\"in 100\")\n",
    "            dl_obj = CIFAR100_truncated\n",
    "            normalize = transforms.Normalize(\n",
    "                mean=[0.5070751592371323, 0.48654887331495095, 0.4409178433670343],\n",
    "                std=[0.2673342858792401, 0.2564384629170883, 0.27615047132568404],\n",
    "            )\n",
    "\n",
    "            transform_train = transforms.Compose(\n",
    "                [\n",
    "                    # transforms.ToPILImage(),\n",
    "                    transforms.RandomCrop(32, padding=4),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.RandomRotation(15),\n",
    "                    transforms.ToTensor(),\n",
    "                    normalize,\n",
    "                ]\n",
    "            )\n",
    "            # data prep for test set\n",
    "            transform_test = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "        elif dataset == \"tinyimagenet\":\n",
    "            dl_obj = ImageFolder_custom\n",
    "            transform_train = transforms.Compose(\n",
    "                [\n",
    "                    transforms.RandomCrop(\n",
    "                        64, padding=4\n",
    "                    ),  # Random cropping with padding\n",
    "                    transforms.RandomHorizontalFlip(),  # Horizontal flip\n",
    "                    transforms.RandomRotation(15),  # Random rotation\n",
    "                    transforms.ColorJitter(\n",
    "                        brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1\n",
    "                    ),  # Color jitter\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(\n",
    "                        mean=[0.4802, 0.4481, 0.3975], std=[0.2302, 0.2265, 0.2262]\n",
    "                    ),  # Normalization\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            transform_test = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(\n",
    "                        mean=[0.4802, 0.4481, 0.3975], std=[0.2302, 0.2265, 0.2262]\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "        elif dataset == \"STL10\":\n",
    "            dl_obj = STL10_truncated\n",
    "            transform_train = transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize(\n",
    "                        (\n",
    "                            SAFE_PFL_CONFIG[\"TRANSFORM_INPUT_SIZE\"],\n",
    "                            SAFE_PFL_CONFIG[\"TRANSFORM_INPUT_SIZE\"],\n",
    "                        )\n",
    "                    ),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                ]\n",
    "            )\n",
    "            transform_test = transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize(\n",
    "                        (\n",
    "                            SAFE_PFL_CONFIG[\"TRANSFORM_INPUT_SIZE\"],\n",
    "                            SAFE_PFL_CONFIG[\"TRANSFORM_INPUT_SIZE\"],\n",
    "                        )\n",
    "                    ),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            dl_obj = Generated\n",
    "            transform_train = None\n",
    "            transform_test = None\n",
    "        if dataset == \"tinyimagenet\":\n",
    "            train_ds = dl_obj(\n",
    "                datadir + \"tiny-imagenet-200/train/\",\n",
    "                dataidxs=dataidxs,\n",
    "                transform=transform_train,\n",
    "            )\n",
    "            test_ds = dl_obj(\n",
    "                datadir + \"tiny-imagenet-200/val/\",\n",
    "                dataidxs=testidxs,\n",
    "                transform=transform_test,\n",
    "            )\n",
    "        elif dataset == \"STL10\":\n",
    "            train_ds = dl_obj(\n",
    "                datadir,\n",
    "                dataidxs=dataidxs,\n",
    "                split=\"train\",\n",
    "                transform=transform_train,\n",
    "                download=True,\n",
    "            )\n",
    "            test_ds = dl_obj(\n",
    "                datadir,\n",
    "                dataidxs=testidxs,\n",
    "                split=\"test\",\n",
    "                transform=transform_test,\n",
    "                download=True,\n",
    "            )\n",
    "        else:\n",
    "            print(\"dir\", datadir)\n",
    "            train_ds = dl_obj(\n",
    "                datadir,\n",
    "                dataidxs=dataidxs,\n",
    "                train=True,\n",
    "                transform=transform_train,\n",
    "                download=True,\n",
    "            )\n",
    "            test_ds = dl_obj(\n",
    "                datadir,\n",
    "                dataidxs=testidxs,\n",
    "                train=False,\n",
    "                transform=transform_test,\n",
    "                download=True,\n",
    "            )\n",
    "        train_dl = data.DataLoader(\n",
    "            dataset=train_ds, batch_size=train_bs, shuffle=True, drop_last=False\n",
    "        )\n",
    "        test_dl = data.DataLoader(\n",
    "            dataset=test_ds, batch_size=test_bs, shuffle=False, drop_last=False\n",
    "        )\n",
    "    return train_dl, test_dl, train_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T15:34:58.337199Z",
     "start_time": "2025-02-01T15:34:58.330777Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-15T16:40:36.996658Z",
     "iopub.status.busy": "2025-03-15T16:40:36.996389Z",
     "iopub.status.idle": "2025-03-15T16:40:37.011635Z",
     "shell.execute_reply": "2025-03-15T16:40:37.010936Z",
     "shell.execute_reply.started": "2025-03-15T16:40:36.996630Z"
    },
    "id": "eizGyXaA3JuE",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_loaders():\n",
    "    (\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        net_dataidx_map,\n",
    "        test_dataidx_map,\n",
    "        traindata_cls_counts,\n",
    "    ) = partition_data(\n",
    "        dataset=SAFE_PFL_CONFIG[\"DATASET_TYPE\"],\n",
    "        datadir=\"./data/\",\n",
    "        logdir=\"./logs/\",\n",
    "        partition=SAFE_PFL_CONFIG[\"PARTITION\"],\n",
    "        n_parties=10,\n",
    "    )\n",
    "    train_loaders = []\n",
    "    test_loaders = []\n",
    "    for client_id in range(SAFE_PFL_CONFIG[\"NUMBER_OF_CLIENTS\"]):\n",
    "        dataidxs = net_dataidx_map[client_id]\n",
    "        testidxs = test_dataidx_map[client_id]\n",
    "\n",
    "        train_dl_local, test_dl_local, train_ds_local, test_ds_local = get_dataloader(\n",
    "            dataset=SAFE_PFL_CONFIG[\"DATASET_TYPE\"],\n",
    "            datadir=\"./data/\",\n",
    "            train_bs=SAFE_PFL_CONFIG[\"TRAIN_BATCH_SIZE\"],\n",
    "            test_bs=SAFE_PFL_CONFIG[\"TEST_BATCH_SIZE\"],\n",
    "            dataidxs=dataidxs,\n",
    "            testidxs=testidxs,\n",
    "        )\n",
    "        train_loaders.append(train_dl_local)\n",
    "        test_loaders.append(test_dl_local)\n",
    "\n",
    "    return train_loaders, test_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T15:34:58.352471Z",
     "start_time": "2025-02-01T15:34:58.348457Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-15T16:40:37.012472Z",
     "iopub.status.busy": "2025-03-15T16:40:37.012251Z",
     "iopub.status.idle": "2025-03-15T16:40:37.027729Z",
     "shell.execute_reply": "2025-03-15T16:40:37.027029Z",
     "shell.execute_reply.started": "2025-03-15T16:40:37.012454Z"
    },
    "id": "-IvzdpYcxGZx",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_and_prepare_data():\n",
    "    train_loaders, test_loaders = get_loaders()\n",
    "    return train_loaders, test_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T15:34:59.236021Z",
     "start_time": "2025-02-01T15:34:58.368023Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-03-15T16:40:37.028657Z",
     "iopub.status.busy": "2025-03-15T16:40:37.028478Z",
     "iopub.status.idle": "2025-03-15T16:42:36.660122Z",
     "shell.execute_reply": "2025-03-15T16:42:36.659099Z",
     "shell.execute_reply.started": "2025-03-15T16:40:37.028639Z"
    },
    "id": "ORJsNkg1xMY4",
    "outputId": "9769a96f-c6b6-46c1-ebf6-2eff0f483bf2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_loaders, test_loaders = load_and_prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CrFoxva3JuE"
   },
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Data Visualization & Silhouette</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T15:35:04.628185Z",
     "start_time": "2025-02-01T15:34:59.262895Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-03-15T16:42:36.661470Z",
     "iopub.status.busy": "2025-03-15T16:42:36.661117Z",
     "iopub.status.idle": "2025-03-15T16:42:44.379664Z",
     "shell.execute_reply": "2025-03-15T16:42:44.378815Z",
     "shell.execute_reply.started": "2025-03-15T16:42:36.661431Z"
    },
    "id": "TigBlX7z3JuE",
    "outputId": "31693f42-3347-4e01-e61c-3ab36ebfbdfb",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_label_distribution(dataloader, loader_name: str):\n",
    "    label_counts = np.zeros(SAFE_PFL_CONFIG[\"NUMBER_OF_CLASSES\"])\n",
    "    for _, labels in dataloader:\n",
    "        for label in labels.numpy():\n",
    "            label_counts[label] += 1\n",
    "\n",
    "    log.info(f\"client {loader_name} label distribution is: {label_counts}\")\n",
    "    return label_counts\n",
    "\n",
    "\n",
    "\n",
    "def plot_stacked_label_distribution(distributions):\n",
    "    \"\"\"\n",
    "    Plots a stacked bar chart for label distributions across clients.\n",
    "    \"\"\"\n",
    "    num_clients = len(distributions)\n",
    "    num_classes = len(distributions[0])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    bar_width = 0.8\n",
    "    x_positions = np.arange(num_clients)\n",
    "\n",
    "    distributions = np.array(distributions)\n",
    "    bottoms = np.zeros(num_clients)\n",
    "\n",
    "    for class_id in range(num_classes):\n",
    "        class_counts = distributions[:, class_id]\n",
    "        ax.bar(x_positions, class_counts, bar_width, label=f'Class {class_id}', bottom=bottoms)\n",
    "        bottoms += class_counts\n",
    "\n",
    "    ax.set_xlabel('Clients', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Number of Samples', fontsize=14, fontweight='bold')\n",
    "    ax.set_title('Stacked Label Distribution Across Clients', fontsize=16, fontweight='bold')\n",
    "\n",
    "    ax.set_xticks(x_positions)\n",
    "    ax.set_xticklabels([f'Client {i + 1}' for i in range(num_clients)], fontsize=12, fontweight='bold')\n",
    "\n",
    "    ax.yaxis.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    ax.legend(title='Classes', fontsize=12, title_fontsize=14, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def compute_similarity_matrix(distributions):\n",
    "    similarity_matrix = cosine_similarity(distributions)\n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "def cluster_clients(similarity_matrix):\n",
    "    clustering = AffinityPropagation(affinity='precomputed', random_state=42)\n",
    "    clustering.fit(similarity_matrix)\n",
    "    return clustering.labels_\n",
    "\n",
    "\n",
    "def group_clients_by_cluster(labels):\n",
    "    clusters = {}\n",
    "    for client_id, cluster_id in enumerate(labels):\n",
    "        if cluster_id not in clusters:\n",
    "            clusters[cluster_id] = []\n",
    "        clusters[cluster_id].append(client_id)\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def compute_silhouette_score(similarity_matrix, cluster_labels):\n",
    "    distance_matrix = 2 - (similarity_matrix + 1)\n",
    "    score = silhouette_score(distance_matrix, cluster_labels, metric='precomputed')\n",
    "    return score\n",
    "\n",
    "\n",
    "log.info(\"clients train loader label distribution\")\n",
    "train_label_distributions = [calculate_label_distribution(loader, \"train\") for loader in train_loaders]\n",
    "plot_stacked_label_distribution(train_label_distributions)\n",
    "\n",
    "log.info(\"clients test loader label distribution\")\n",
    "test_label_distributions = [calculate_label_distribution(loader, \"test\") for loader in test_loaders]\n",
    "plot_stacked_label_distribution(test_label_distributions)\n",
    "\n",
    "train_similarity_matrix = compute_similarity_matrix(train_label_distributions)\n",
    "test_similarity_matrix = compute_similarity_matrix(test_label_distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T15:35:04.708501Z",
     "start_time": "2025-02-01T15:35:04.701360Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-15T16:42:44.380786Z",
     "iopub.status.busy": "2025-03-15T16:42:44.380554Z",
     "iopub.status.idle": "2025-03-15T16:42:44.417500Z",
     "shell.execute_reply": "2025-03-15T16:42:44.416902Z",
     "shell.execute_reply.started": "2025-03-15T16:42:44.380765Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "OPTIMAL_TRAIN_CLUSTERING = cluster_clients(train_similarity_matrix)\n",
    "log.info(\"Clients train loader clustering label based on their dataset\")\n",
    "log.info(OPTIMAL_TRAIN_CLUSTERING)\n",
    "train_clusters = group_clients_by_cluster(OPTIMAL_TRAIN_CLUSTERING)\n",
    "log.info(\"Clients train loader clustering based on their dataset\")\n",
    "log.info(train_clusters)\n",
    "\n",
    "OPTIMAL_TEST_CLUSTERING = cluster_clients(test_similarity_matrix)\n",
    "log.info(\"Clients test loader clustering label based on their dataset\")\n",
    "log.info(OPTIMAL_TEST_CLUSTERING)\n",
    "test_clusters = group_clients_by_cluster(OPTIMAL_TEST_CLUSTERING)\n",
    "log.info(\"Clients test loader clustering based on their dataset\")\n",
    "log.info(test_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T16:42:44.418463Z",
     "iopub.status.busy": "2025-03-15T16:42:44.418253Z",
     "iopub.status.idle": "2025-03-15T16:42:44.422351Z",
     "shell.execute_reply": "2025-03-15T16:42:44.421537Z",
     "shell.execute_reply.started": "2025-03-15T16:42:44.418436Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def extract_features(data_loader, model, device):\n",
    "#     model.eval()\n",
    "#     features = []\n",
    "#     with torch.no_grad():\n",
    "#         for images, _ in tqdm(data_loader):\n",
    "#             images = images.to(device)\n",
    "#             embeddings = model(images)\n",
    "#             features.append(embeddings.cpu().numpy())\n",
    "#     return np.concatenate(features, axis=0)\n",
    "\n",
    "# def compute_wasserstein_matrix(data_loaders, model, device):\n",
    "#     num_clients = len(data_loaders)\n",
    "#     distance_matrix = np.zeros((num_clients, num_clients))\n",
    "#     feature_lists = []\n",
    "#     for data_loader in data_loaders:\n",
    "#         feature_lists.append(extract_features(data_loader, model, device))\n",
    "#     for i in range(num_clients):\n",
    "#         for j in range(num_clients):\n",
    "#             distance_matrix[i, j] = wasserstein_distance(feature_lists[i].flatten(), feature_lists[j].flatten())\n",
    "#     return distance_matrix\n",
    "\n",
    "\n",
    "# # --- Fine-Tuning ResNet18 ---\n",
    "# resnet18 = models.resnet18(pretrained=True)\n",
    "# num_classes = 10\n",
    "# resnet18.fc = nn.Linear(resnet18.fc.in_features, num_classes)\n",
    "# resnet18.to(DEVICE)\n",
    "\n",
    "# # Combine data loaders for fine-tuning\n",
    "# combined_dataset = torch.utils.data.ConcatDataset([loader.dataset for loader in train_loaders])\n",
    "# combined_loader = torch.utils.data.DataLoader(combined_dataset, batch_size=128, shuffle=True)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(resnet18.parameters(), lr=0.001)\n",
    "\n",
    "# num_epochs = 2  # Adjust as needed\n",
    "# resnet18.train()\n",
    "# for epoch in range(num_epochs):\n",
    "#     for images, labels in tqdm(combined_loader):\n",
    "#         images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = resnet18(images)\n",
    "#         loss = criterion(outputs, labels.long())\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "# # --- Feature Extraction with Fine-Tuned Model ---\n",
    "# resnet18.eval()\n",
    "# fine_tuned_feature_extractor = torch.nn.Sequential(*(list(resnet18.children())[:-1])).to(DEVICE)\n",
    "\n",
    "# distance_matrix = compute_wasserstein_matrix(train_loaders, fine_tuned_feature_extractor, DEVICE)\n",
    "\n",
    "# print(\"Wasserstein Distance Matrix (Fine-Tuned):\")\n",
    "# print(distance_matrix)\n",
    "\n",
    "# # --- Clustering with Affinity Propagation ---\n",
    "# affinity_propagation = AffinityPropagation(affinity='precomputed', random_state=42)\n",
    "# affinity_propagation.fit(-distance_matrix)\n",
    "# cluster_labels = affinity_propagation.labels_\n",
    "\n",
    "# print(\"Cluster Labels (Fine-Tuned):\")\n",
    "# print(cluster_labels)\n",
    "\n",
    "# # --- Visualization ---\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.heatmap(distance_matrix, annot=True, cmap=\"viridis\")\n",
    "# plt.title(\"Wasserstein Distance Matrix Heatmap (Fine-Tuned)\")\n",
    "# plt.xlabel(\"Client Index\")\n",
    "# plt.ylabel(\"Client Index\")\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.scatterplot(x=range(len(cluster_labels)), y=[0]*len(cluster_labels), hue=cluster_labels, s=200)\n",
    "# plt.yticks([])\n",
    "# plt.title(\"Cluster Assignments (Fine-Tuned)\")\n",
    "# plt.xlabel(\"Client Index\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T15:35:04.727604Z",
     "start_time": "2025-02-01T15:35:04.724505Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-15T16:42:44.423242Z",
     "iopub.status.busy": "2025-03-15T16:42:44.423061Z",
     "iopub.status.idle": "2025-03-15T16:42:44.438453Z",
     "shell.execute_reply": "2025-03-15T16:42:44.437620Z",
     "shell.execute_reply.started": "2025-03-15T16:42:44.423226Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# silhouette_cosine = compute_silhouette_score(similarity_matrix, [0, 1, 0, 2, 2, 3, 3, 3, 2, 1])\n",
    "# print(f\"Silhouette score for data clustering is: {silhouette_cosine}\")\n",
    "\n",
    "# silhouette_cosine = compute_silhouette_score(similarity_matrix, [2, 0, 1, 1, 1, 1, 2, 2, 1, 0,])\n",
    "# print(f\"Silhouette score for cosine is: {silhouette_cosine}\")\n",
    "\n",
    "# silhouette_cosine_less_sig_pruned = compute_silhouette_score(similarity_matrix, [0, 3, 0, 1, 1, 3, 2, 2, 3, 3,])\n",
    "# print(f\"Silhouette score for cosine (optimal) common less sig pruned is: {silhouette_cosine_less_sig_pruned}\")\n",
    "\n",
    "# silhouette_coordinate = compute_silhouette_score(similarity_matrix, [0, 3, 0, 1, 1, 3, 2, 2, 0, 3,])\n",
    "# print(f\"Silhouette score for coordinate is: {silhouette_coordinate}\")\n",
    "\n",
    "# silhouette_euclidean = compute_silhouette_score(similarity_matrix, [3, 0, 3, 1, 0, 3, 3, 3, 2, 0,])\n",
    "# print(f\"Silhouette score for euclidean is: {silhouette_euclidean}\")\n",
    "\n",
    "# silhouette_wasserstein = compute_silhouette_score(similarity_matrix, [2, 0, 2, 2, 2, 0, 2, 2, 1, 0,])\n",
    "# print(f\"Silhouette score for wasserstein is: {silhouette_wasserstein}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFgZGqbk3JuE"
   },
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Utils</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T15:35:04.785720Z",
     "start_time": "2025-02-01T15:35:04.777426Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-15T16:42:44.443109Z",
     "iopub.status.busy": "2025-03-15T16:42:44.442878Z",
     "iopub.status.idle": "2025-03-15T16:42:44.467691Z",
     "shell.execute_reply": "2025-03-15T16:42:44.466914Z",
     "shell.execute_reply.started": "2025-03-15T16:42:44.443090Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def vectorise_model(model):\n",
    "    return Params2Vec(model.parameters())\n",
    "\n",
    "def display_train_stats(cfl_stats, communication_rounds, output_clarence_status=False):\n",
    "    if output_clarence_status:\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    acc_mean = np.mean(cfl_stats.acc_clients, axis=1)\n",
    "    acc_std = np.std(cfl_stats.acc_clients, axis=1)\n",
    "\n",
    "    log.info(f\"the global accuracy is: {acc_mean} +- {acc_std}\")\n",
    "\n",
    "    plt.fill_between(\n",
    "        cfl_stats.rounds, acc_mean - acc_std, acc_mean + acc_std, alpha=0.5, color=\"C0\"\n",
    "    )\n",
    "    plt.plot(cfl_stats.rounds, acc_mean, color=\"C0\")\n",
    "\n",
    "    if \"split\" in cfl_stats.__dict__:\n",
    "        for s in cfl_stats.split:\n",
    "            plt.axvline(x=s, linestyle=\"-\", color=\"k\", label=r\"Split\")\n",
    "\n",
    "    plt.text(\n",
    "        x=communication_rounds,\n",
    "        y=1,\n",
    "        ha=\"right\",\n",
    "        va=\"top\",\n",
    "        s=\"Clusters: {}\".format([x for x in cfl_stats.clusters[-1]]),\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"Communication Rounds\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "\n",
    "    plt.xlim(0, communication_rounds)\n",
    "    plt.ylim(0, 1)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "class ExperimentLogger:\n",
    "    def log(self, values):\n",
    "        for k, v in values.items():\n",
    "            if k not in self.__dict__:\n",
    "                self.__dict__[k] = [v]\n",
    "            else:\n",
    "                self.__dict__[k] += [v]\n",
    "\n",
    "def copy(target, source):\n",
    "    for name in target:\n",
    "        target[name].data = source[name].data.clone()\n",
    "\n",
    "\n",
    "def flatten(source):\n",
    "    return torch.cat([value.flatten() for value in source.values()])\n",
    "\n",
    "def pairwise_cosine_similarity(clients):\n",
    "    comparing_vectors = None\n",
    "    if SAFE_PFL_CONFIG[\"DISTANCE_METRIC_ON_PARAMETERS\"]:\n",
    "        log.info(\n",
    "            f'running cosine similarity on parameters since `SAFE_PFL_CONFIG[\"DISTANCE_METRIC_ON_PARAMETERS\"]` value is: {SAFE_PFL_CONFIG[\"DISTANCE_METRIC_ON_PARAMETERS\"]}'\n",
    "        )\n",
    "        comparing_vectors = [\n",
    "            vectorise_model(client.model).detach().cpu().numpy() for client in clients\n",
    "        ]\n",
    "    else:\n",
    "        log.info(\n",
    "            f'running cosine similarity on gradients since `SAFE_PFL_CONFIG[\"DISTANCE_METRIC_ON_PARAMETERS\"]` value is: {SAFE_PFL_CONFIG[\"DISTANCE_METRIC_ON_PARAMETERS\"]}'\n",
    "        )\n",
    "        comparing_vectors = [\n",
    "            np.array(list(client.gradients.values())) for client in clients\n",
    "        ]\n",
    "        log.info(\n",
    "            f\"the length of gradients for each model is {len(comparing_vectors[0])}\"\n",
    "        )\n",
    "\n",
    "    n = len(clients)\n",
    "    similarities = np.zeros((n, n))\n",
    "\n",
    "    for i in range(n):\n",
    "        vi = comparing_vectors[i]\n",
    "        norm_i = np.linalg.norm(vi)\n",
    "\n",
    "        for j in range(n):\n",
    "            vj = comparing_vectors[j]\n",
    "            norm_j = np.linalg.norm(vj)\n",
    "            if norm_i == 0 or norm_j == 0:\n",
    "                similarities[i][j] = 0.0\n",
    "            else:\n",
    "                similarities[i][j] = np.dot(vi, vj) / (norm_i * norm_j)\n",
    "\n",
    "    np.fill_diagonal(similarities, 1)\n",
    "    return similarities\n",
    "\n",
    "\n",
    "def pairwise_coordinate_similarity(clients):\n",
    "    _top_gradients_count = int(\n",
    "        np.ceil(\n",
    "            SAFE_PFL_CONFIG[\"SENSITIVITY_PERCENTAGE\"] * len(clients[0].gradients) / 100\n",
    "        )\n",
    "    )\n",
    "\n",
    "    _top_sensitive_gradients = []\n",
    "    for client in clients:\n",
    "        grads = client.gradients.items()\n",
    "        top_keys = [\n",
    "            k for k, _ in nlargest(_top_gradients_count, grads, key=lambda x: x[1])\n",
    "        ]\n",
    "\n",
    "        log.info(\n",
    "            f\"top sensitive computed with {len(top_keys)} entries. and all are {len(top_keys) == len(set(top_keys))}ly unique.\"\n",
    "        )\n",
    "        _top_sensitive_gradients.append(set(top_keys))\n",
    "\n",
    "    if SAFE_PFL_CONFIG[\"REMOVE_COMMON_IDS\"]:\n",
    "        all_ids = [id_ for ids in _top_sensitive_gradients for id_ in ids]\n",
    "        id_counts = Counter(all_ids)\n",
    "        common_ids = {id_ for id_, count in id_counts.items() if count == len(clients)}\n",
    "\n",
    "        _top_sensitive_gradients = [\n",
    "            ids - common_ids for ids in _top_sensitive_gradients\n",
    "        ]\n",
    "\n",
    "        for _top_g in _top_sensitive_gradients:\n",
    "            log.info(\n",
    "                f\"top sensitive computed (removed common ids) with {len(_top_g)} entries. and all are {len(_top_g) == len(set(_top_g))}ly unique.\"\n",
    "            )\n",
    "\n",
    "    n_clients = len(clients)\n",
    "    similarities = np.zeros((n_clients, n_clients), dtype=float)\n",
    "\n",
    "    for i, j in combinations(range(n_clients), 2):\n",
    "        set_i = _top_sensitive_gradients[i]\n",
    "        set_j = _top_sensitive_gradients[j]\n",
    "        intersection = len(set_i & set_j)\n",
    "        similarities[i, j] = similarities[j, i] = intersection\n",
    "\n",
    "    np.fill_diagonal(similarities, _top_gradients_count)\n",
    "    similarities = similarities /  _top_gradients_count\n",
    "    return similarities\n",
    "\n",
    "\n",
    "def pairwise_wasserstein_similarity(clients):\n",
    "    comparing_vectors = None\n",
    "    if SAFE_PFL_CONFIG[\"DISTANCE_METRIC_ON_PARAMETERS\"]:\n",
    "        log.info(\n",
    "            f'running wasserstein similarity on parameters since `SAFE_PFL_CONFIG[\"DISTANCE_METRIC_ON_PARAMETERS\"]` value is: {SAFE_PFL_CONFIG[\"DISTANCE_METRIC_ON_PARAMETERS\"]}'\n",
    "        )\n",
    "        comparing_vectors = [\n",
    "            vectorise_model(client.model).detach().cpu().numpy() for client in clients\n",
    "        ]\n",
    "    else:\n",
    "        log.info(\n",
    "            f'running wasserstein similarity on gradients since `SAFE_PFL_CONFIG[\"DISTANCE_METRIC_ON_PARAMETERS\"]` value is: {SAFE_PFL_CONFIG[\"DISTANCE_METRIC_ON_PARAMETERS\"]}'\n",
    "        )\n",
    "        comparing_vectors = [\n",
    "            np.array(list(client.gradients.values())) for client in clients\n",
    "        ]\n",
    "        log.info(\n",
    "            f\"the length of gradients for each model is {len(comparing_vectors[0])}\"\n",
    "        )\n",
    "\n",
    "    distances = np.zeros((len(clients), len(clients)))\n",
    "\n",
    "    for i in range(len(clients)):\n",
    "        for j in range(len(clients)):\n",
    "            distances[i, j] = wasserstein_distance(comparing_vectors[i], comparing_vectors[j])\n",
    "\n",
    "    similarity_matrix = -distances\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "def pairwise_euclidean_similarity(clients):\n",
    "\n",
    "    comparing_vectors = None\n",
    "    if SAFE_PFL_CONFIG[\"DISTANCE_METRIC_ON_PARAMETERS\"]:\n",
    "        log.info(\n",
    "            f'running euclidean similarity on parameters since `SAFE_PFL_CONFIG[\"DISTANCE_METRIC_ON_PARAMETERS\"]` value is: {SAFE_PFL_CONFIG[\"DISTANCE_METRIC_ON_PARAMETERS\"]}'\n",
    "        )\n",
    "        comparing_vectors = [\n",
    "            vectorise_model(client.model).detach().cpu().numpy() for client in clients\n",
    "        ]\n",
    "    else:\n",
    "        log.info(\n",
    "            f'running euclidean similarity on gradients since `SAFE_PFL_CONFIG[\"DISTANCE_METRIC_ON_PARAMETERS\"]` value is: {SAFE_PFL_CONFIG[\"DISTANCE_METRIC_ON_PARAMETERS\"]}'\n",
    "        )\n",
    "        comparing_vectors = [\n",
    "            np.array(list(client.gradients.values())) for client in clients\n",
    "        ]\n",
    "        log.info(\n",
    "            f\"the length of gradients for each model is {len(comparing_vectors[0])}\"\n",
    "        )\n",
    "\n",
    "    n = len(clients)\n",
    "    similarities = np.zeros((n, n))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            similarities[i][j] = np.linalg.norm(comparing_vectors[i] - comparing_vectors[j])\n",
    "    \n",
    "    similarity_matrix = -similarities\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "def eval_op(model, loader):\n",
    "    model.eval()\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device=DEVICE, non_blocking=True)\n",
    "    correct, total = 0, 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels.long())\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    loss = running_loss / total\n",
    "    accuracy = correct / total\n",
    "\n",
    "    return loss, accuracy\n",
    "\n",
    "\n",
    "def train_op(model, loader, optimizer, epochs=1):\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device=DEVICE, non_blocking=True)\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in loader:\n",
    "\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels.long())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        if epoch > 1:\n",
    "            log.info(f\"[{epoch + 1}] loss: {running_loss / len(loader):.3f}\")\n",
    "\n",
    "    return model, running_loss / len(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Federated Learning Components</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T16:42:44.486453Z",
     "iopub.status.busy": "2025-03-15T16:42:44.486155Z",
     "iopub.status.idle": "2025-03-15T16:42:44.491512Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FederatedTrainingDevice(object):\n",
    "    def __init__(self, model_fn):\n",
    "        self.model = model_fn(\n",
    "            SAFE_PFL_CONFIG[\"MODEL_TYPE\"], SAFE_PFL_CONFIG[\"NUMBER_OF_CLASSES\"]\n",
    "        ).to(DEVICE)\n",
    "\n",
    "    def evaluate(self):\n",
    "        _loss, _accuracy = eval_op(self.model, self.eval_loader)\n",
    "\n",
    "        if _loss < 1.0 and _accuracy > 0.6:\n",
    "            log.info(\n",
    "                f\"testing done for client no {self.id} with accuracy of {_accuracy} and loss of {_loss} [GOOD]\"\n",
    "            )\n",
    "        elif _loss < 2.0 and _accuracy > 0.4:\n",
    "            log.warn(\n",
    "                f\"testing done for client no {self.id} with accuracy of {_accuracy} and loss of {_loss} [MODERATE]\"\n",
    "            )\n",
    "        else:\n",
    "            log.warn(\n",
    "                f\"testing done for client no {self.id} with accuracy of {_accuracy} and loss of {_loss} [POOR]\"\n",
    "            )\n",
    "\n",
    "        return _accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T16:42:44.502654Z",
     "iopub.status.busy": "2025-03-15T16:42:44.502408Z",
     "iopub.status.idle": "2025-03-15T16:42:44.511194Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GradientExtractor:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.gradient_history = {}\n",
    "        self.gradients = {}\n",
    "        self.hooks = []\n",
    "        self.history_weight = 0.8\n",
    "        self._register_hooks()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        \"\"\"Register hooks for conv and fc layers\"\"\"\n",
    "        def hook_fn(name):\n",
    "            def get_gradients(grad):\n",
    "                self.gradients[name] = grad.detach()\n",
    "            return get_gradients\n",
    "\n",
    "        for name, module in self.model.named_modules():\n",
    "            if hasattr(module, 'weight') and module.weight is not None:\n",
    "                hook = module.weight.register_hook(hook_fn(f\"{name}_weight\"))\n",
    "                self.hooks.append(hook)\n",
    "            if hasattr(module, 'bias') and module.bias is not None:\n",
    "                hook = module.bias.register_hook(hook_fn(f\"{name}_bias\"))\n",
    "                self.hooks.append(hook)\n",
    "\n",
    "    def extract_gradients(self, dataloader, criterion, num_batches=None):\n",
    "        \"\"\"Extract gradients with stability mechanisms\"\"\"\n",
    "        self.model.train()\n",
    "        batch_gradients = {}\n",
    "        \n",
    "        for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
    "            if num_batches and batch_idx >= num_batches:\n",
    "                break\n",
    "                \n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            self.model.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Store gradients with historical averaging\n",
    "            for name, grad in self.gradients.items():\n",
    "                if name not in batch_gradients:\n",
    "                    batch_gradients[name] = []\n",
    "                if name not in self.gradient_history:\n",
    "                    self.gradient_history[name] = grad.cpu().numpy()\n",
    "                else:\n",
    "                    # Apply exponential moving average\n",
    "                    current_grad = grad.cpu().numpy()\n",
    "                    self.gradient_history[name] = (\n",
    "                        self.history_weight * self.gradient_history[name] +\n",
    "                        (1 - self.history_weight) * current_grad\n",
    "                    )\n",
    "                batch_gradients[name].append(self.gradient_history[name])\n",
    "        \n",
    "        avg_gradients = {}\n",
    "        for name, grads in batch_gradients.items():\n",
    "            avg_gradients[name] = np.mean(grads, axis=0)\n",
    "            \n",
    "        return avg_gradients\n",
    "\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        \"\"\"Remove all hooks\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T16:42:44.514211Z",
     "iopub.status.busy": "2025-03-15T16:42:44.514019Z",
     "iopub.status.idle": "2025-03-15T16:42:44.529169Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ClusterManager:\n",
    "    def __init__(self, stability_threshold=0.3):\n",
    "        self.previous_clusters = None\n",
    "        self.stability_threshold = stability_threshold\n",
    "        self.cluster_history = []\n",
    "        self.momentum = 0.7\n",
    "\n",
    "    def calculate_cluster_similarity(self, prev_clusters, new_clusters):\n",
    "        \"\"\"Calculate similarity between cluster assignments\"\"\"\n",
    "        if prev_clusters is None:\n",
    "            return 0\n",
    "        \n",
    "        common_clients = set(prev_clusters.keys()) & set(new_clusters.keys())\n",
    "        if not common_clients:\n",
    "            return 0\n",
    "            \n",
    "        similarity = sum(1 for client in common_clients \n",
    "                        if prev_clusters[client] == new_clusters[client])\n",
    "        return similarity / len(common_clients)\n",
    "\n",
    "    def should_update_clusters(self, new_clusters):\n",
    "        \"\"\"Determine if clusters should be updated based on stability\"\"\"\n",
    "        if not self.cluster_history:\n",
    "            self.cluster_history.append(new_clusters)\n",
    "            return True\n",
    "            \n",
    "        similarity = self.calculate_cluster_similarity(\n",
    "            self.cluster_history[-1], new_clusters\n",
    "        )\n",
    "        \n",
    "        if similarity >= self.stability_threshold:\n",
    "            self.cluster_history.append(new_clusters)\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def get_stable_clusters(self, new_clusters):\n",
    "        \"\"\"Get stable cluster assignments using momentum and history\"\"\"\n",
    "        if not self.previous_clusters:\n",
    "            self.previous_clusters = new_clusters\n",
    "            return new_clusters\n",
    "            \n",
    "        if not self.should_update_clusters(new_clusters):\n",
    "            return self.previous_clusters\n",
    "            \n",
    "        # Apply momentum to cluster assignments\n",
    "        stable_clusters = {}\n",
    "        for client_id in new_clusters:\n",
    "            if client_id in self.previous_clusters:\n",
    "                stable_clusters[client_id] = (\n",
    "                    self.momentum * self.previous_clusters[client_id] +\n",
    "                    (1 - self.momentum) * new_clusters[client_id]\n",
    "                )\n",
    "            else:\n",
    "                stable_clusters[client_id] = new_clusters[client_id]\n",
    "                \n",
    "        self.previous_clusters = stable_clusters\n",
    "        return stable_clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T16:42:44.532080Z",
     "iopub.status.busy": "2025-03-15T16:42:44.531842Z",
     "iopub.status.idle": "2025-03-15T16:42:44.544446Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_layer_gradients(model, dataloader):\n",
    "    \"\"\"Extract and process gradients with stability mechanisms\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    extractor = GradientExtractor(model)\n",
    "    # cluster_manager = ClusterManager()\n",
    "    \n",
    "    # Extract stabilized gradients\n",
    "    layer_gradients = extractor.extract_gradients(dataloader, criterion)\n",
    "    \n",
    "    # Process gradients with stability consideration\n",
    "    processed_gradients = {}\n",
    "    id_counter = 0\n",
    "    batch_size = dataloader.batch_size\n",
    "    \n",
    "    for layer_name, gradient in layer_gradients.items():\n",
    "        flat_grad = gradient.reshape(-1)\n",
    "        flat_grad = flat_grad / batch_size\n",
    "        \n",
    "        for grad_value in flat_grad:\n",
    "            processed_gradients[id_counter] = float(grad_value)\n",
    "            id_counter += 1\n",
    "    \n",
    "    # Clean up hooks\n",
    "    extractor.remove_hooks()\n",
    "    \n",
    "    # Get stable clusters\n",
    "    # stable_clusters = cluster_manager.get_stable_clusters(processed_gradients)\n",
    "    # print(stable_clusters)\n",
    "\n",
    "    return processed_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T15:35:04.796267Z",
     "start_time": "2025-02-01T15:35:04.789231Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-15T16:42:44.545449Z",
     "iopub.status.busy": "2025-03-15T16:42:44.545188Z",
     "iopub.status.idle": "2025-03-15T16:42:44.561387Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Client(FederatedTrainingDevice):\n",
    "    def __init__(\n",
    "        self, model_fn, optimizer_fn, id_num, train_data_loader, evaluation_data_loader\n",
    "    ):\n",
    "        super().__init__(model_fn)\n",
    "        self.optimizer = optimizer_fn(self.model.parameters())\n",
    "\n",
    "        self.train_loader = train_data_loader\n",
    "        self.eval_loader = evaluation_data_loader\n",
    "\n",
    "        self.gradients = {}\n",
    "\n",
    "        self.id = id_num\n",
    "\n",
    "        log.info(f\"client no: {self.id} initialized\")\n",
    "\n",
    "    def synchronize_with_server(self, server):\n",
    "        self.model.load_state_dict(server.model.state_dict())\n",
    "\n",
    "    def compute_weight_update(\n",
    "        self,\n",
    "        be_ready_for_clustering,\n",
    "        epochs=SAFE_PFL_CONFIG[\"ROUND_EPOCHS\"],\n",
    "        loader=None,\n",
    "    ):\n",
    "        _updated_model, train_stats = train_op(\n",
    "            self.model,\n",
    "            self.train_loader if not loader else loader,\n",
    "            self.optimizer,\n",
    "            epochs,\n",
    "        )\n",
    "\n",
    "        self.model.load_state_dict(_updated_model.state_dict())\n",
    "        del _updated_model\n",
    "\n",
    "        log.info(f\"training done for client no {self.id} with loss of {train_stats}\")\n",
    "\n",
    "        if be_ready_for_clustering:\n",
    "            criterion = torch.nn.CrossEntropyLoss().to(device=DEVICE, non_blocking=True)\n",
    "\n",
    "            _model = py_copy.deepcopy(self.model)\n",
    "            _model.eval()\n",
    "            \n",
    "            accumulated_grads = []\n",
    "            for param in _model.parameters():\n",
    "                if param.requires_grad:\n",
    "                    accumulated_grads.append(torch.zeros_like(param, device=DEVICE))\n",
    "                else:\n",
    "                    accumulated_grads.append(None)\n",
    "\n",
    "            for inputs, labels in self.train_loader:\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = _model(inputs)\n",
    "                loss = criterion(outputs, labels.long())\n",
    "                \n",
    "                grads = torch.autograd.grad(loss, _model.parameters(), allow_unused=True)\n",
    "                \n",
    "                for i, grad in enumerate(grads):\n",
    "                    if grad is not None:\n",
    "                        accumulated_grads[i] += grad.detach().abs()\n",
    "\n",
    "            all_grads = []\n",
    "            for grad in accumulated_grads:\n",
    "                if grad is not None:\n",
    "                    all_grads.append(grad.view(-1).cpu())\n",
    "            \n",
    "            if all_grads:\n",
    "                combined_grads = torch.cat(all_grads).numpy()\n",
    "                self.gradients = {i: val for i, val in enumerate(combined_grads)}\n",
    "                log.info(f\"Gradients computed with {len(self.gradients)} entries.\")\n",
    "            else:\n",
    "                log.warn(\"No gradients were computed.\")\n",
    "                self.gradients = {}\n",
    "            \n",
    "            del _model\n",
    "        # if be_ready_for_clustering:\n",
    "        #     try:\n",
    "        #         self.gradients = extract_layer_gradients(\n",
    "        #             self.model, \n",
    "        #             self.train_loader\n",
    "        #         )\n",
    "        #         log.info(f\"Gradients computed with {len(self.gradients)} entries.\")\n",
    "        #     except Exception as e:\n",
    "        #         log.error(f\"Error extracting gradients: {str(e)}\")\n",
    "        #         self.gradients = {}\n",
    "        return train_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T15:35:04.813075Z",
     "start_time": "2025-02-01T15:35:04.807060Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-15T16:42:44.562465Z",
     "iopub.status.busy": "2025-03-15T16:42:44.562144Z",
     "iopub.status.idle": "2025-03-15T16:42:44.576876Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Server(FederatedTrainingDevice):\n",
    "    def __init__(self, model_fn):\n",
    "        super().__init__(model_fn)\n",
    "        self.model_cache = []\n",
    "\n",
    "    def compute_pairwise_similarities(self, clients):\n",
    "        _distance_metric = SAFE_PFL_CONFIG[\"DISTANCE_METRIC\"]\n",
    "        log.info(f\"Start compute pairwise similarities with metric: {_distance_metric}\")\n",
    "\n",
    "        if _distance_metric == distances_constants.DISTANCE_COSINE:\n",
    "            return pairwise_cosine_similarity(clients)\n",
    "        elif _distance_metric == distances_constants.DISTANCE_COORDINATE:\n",
    "            return pairwise_coordinate_similarity(clients)\n",
    "        elif _distance_metric == distances_constants.DISTANCE_WASSERSTEIN:\n",
    "            return pairwise_wasserstein_similarity(clients)\n",
    "        elif _distance_metric == distances_constants.DISTANCE_EUCLIDEAN:\n",
    "            return pairwise_euclidean_similarity(clients)\n",
    "        else:\n",
    "            raise ValueError(f\"unsupported distance metric {_distance_metric}\")\n",
    "\n",
    "    def cluster_clients(self, similarities):\n",
    "\n",
    "        log.info(\"similarity matrix is that feeds the clustering\")\n",
    "        similarity_df = pd.DataFrame(similarities)\n",
    "        log.info(\"\\n\" + similarity_df.to_string())\n",
    "\n",
    "        clustering = AffinityPropagation(\n",
    "            affinity=\"precomputed\",\n",
    "            random_state=42,\n",
    "        ).fit(similarities)\n",
    "\n",
    "        log.info(f\"Cluster labels: {clustering.labels_}\")\n",
    "\n",
    "        del similarities\n",
    "\n",
    "        return clustering\n",
    "\n",
    "    def aggregate(self, models):\n",
    "        log.info(f\"models to be aggregated count: {len(models)}\")\n",
    "\n",
    "        device = next(models[0].parameters()).device\n",
    "        for model in models:\n",
    "            model.to(device)\n",
    "        avg_model = py_copy.deepcopy(models[0])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for param_name, param in avg_model.named_parameters():\n",
    "                param.data.zero_()\n",
    "                for model in models:\n",
    "                    param.data.add_(model.state_dict()[param_name].data / len(models))\n",
    "        \n",
    "        return avg_model\n",
    "\n",
    "\n",
    "    def aggregate_clusterwise(self, client_clusters):\n",
    "                \n",
    "        for cluster_idx, cluster in enumerate(client_clusters):\n",
    "            if len(cluster) == 1:\n",
    "                continue\n",
    "\n",
    "            idcs = [client.id for client in cluster]\n",
    "            log.info(f\"Aggregating clients: {idcs}\")\n",
    "\n",
    "            cluster_models = [client.model for client in cluster]\n",
    "\n",
    "            avg_model = self.aggregate(cluster_models)\n",
    "            \n",
    "            if SAFE_PFL_CONFIG[\"SAVE_GLOBAL_MODELS\"]:\n",
    "                _path = f'models/after_aggregation/{SAFE_PFL_CONFIG[\"MODEL_TYPE\"]}'\n",
    "                os.makedirs(_path, exist_ok=True)\n",
    "\n",
    "                global_id = f\"cluster{cluster_idx}_aggregated\"\n",
    "                model_path = f'{_path}/global_{global_id}.pth'\n",
    "                torch.save(avg_model.state_dict(), model_path)\n",
    "                log.info(f\"Saved aggregated model for cluster {cluster_idx} to {model_path}\")\n",
    "\n",
    "            for client in cluster:\n",
    "                client.model.load_state_dict(avg_model.state_dict()) \n",
    "                # client.optimizer = torch.optim.Adam(client.model.parameters(), lr=0.001,  amsgrad=True)\n",
    "                # client.optimizer = torch.optim.SGD(client.model.parameters(),lr=0.001, momentum=0.9, weight_decay=1e-4)\n",
    "                # client.optimizer = torch.optim.SGD(client.model.parameters(),lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Calculating Optimal Sensitivity Percentage (A.K.A `P`)</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T16:42:44.577692Z",
     "iopub.status.busy": "2025-03-15T16:42:44.577478Z",
     "iopub.status.idle": "2025-03-15T16:42:44.593094Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(base_weights, model_weights):\n",
    "    \"\"\"Calculate the cosine similairty between two vectors\"\"\"\n",
    "    return torch.nan_to_num(\n",
    "        torch.clip(\n",
    "            torch.dot(base_weights, model_weights)\n",
    "            / (torch.linalg.norm(base_weights) * torch.linalg.norm(model_weights)),\n",
    "            -1,\n",
    "            1,\n",
    "        ),\n",
    "        0,\n",
    "    )\n",
    "\n",
    "def global_prune_without_masks(model, amount):\n",
    "    \"\"\"Global Unstructured Pruning of model.\"\"\"\n",
    "    parameters_to_prune = []\n",
    "    for mod in model.modules():\n",
    "        if hasattr(mod, \"weight\"):\n",
    "            if isinstance(mod.weight, torch.nn.Parameter):\n",
    "                parameters_to_prune.append((mod, \"weight\"))\n",
    "        if hasattr(mod, \"bias\"):\n",
    "            if isinstance(mod.bias, torch.nn.Parameter):\n",
    "                parameters_to_prune.append((mod, \"bias\"))\n",
    "    parameters_to_prune = tuple(parameters_to_prune)\n",
    "    prune.global_unstructured(\n",
    "        parameters_to_prune,\n",
    "        pruning_method=prune.L1Unstructured,\n",
    "        amount=amount,\n",
    "    )\n",
    "    for mod in model.modules():\n",
    "        if hasattr(mod, \"weight_orig\"):\n",
    "            if isinstance(mod.weight_orig, torch.nn.Parameter):\n",
    "                prune.remove(mod, \"weight\")\n",
    "        if hasattr(mod, \"bias_orig\"):\n",
    "            if isinstance(mod.bias_orig, torch.nn.Parameter):\n",
    "                prune.remove(mod, \"bias\")\n",
    "\n",
    "\n",
    "def calculate_optimal_sensitivity_percentage(example_client_model):\n",
    "    prune_rate = torch.linspace(0, 1, 101)\n",
    "    cosine_sim = []\n",
    "    base_vec = vectorise_model(example_client_model)\n",
    "    prune_net = Net(\n",
    "        SAFE_PFL_CONFIG[\"MODEL_TYPE\"], SAFE_PFL_CONFIG[\"NUMBER_OF_CLASSES\"]\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    log.info(\"starting calculating optimal sensitivity percentage...\")\n",
    "\n",
    "    for p in prune_rate:\n",
    "        p = float(p)\n",
    "        prune_net.load_state_dict(example_client_model.state_dict())\n",
    "        global_prune_without_masks(prune_net, p)\n",
    "        prune_net_vec = vectorise_model(prune_net)\n",
    "        cosine_sim.append(cosine_similarity(base_vec, prune_net_vec).item())\n",
    "\n",
    "    c = torch.vstack((torch.Tensor(cosine_sim), prune_rate))\n",
    "    d = c.T\n",
    "    dists = []\n",
    "    for i in d:\n",
    "        dists.append(torch.dist(i, torch.Tensor([1, 1])))\n",
    "    min = torch.argmin(torch.Tensor(dists))\n",
    "\n",
    "    del dists\n",
    "\n",
    "    plt.plot(\n",
    "        prune_rate, cosine_sim, label=f'{SAFE_PFL_CONFIG[\"MODEL_TYPE\"]} Parateo Front'\n",
    "    )\n",
    "    plt.xlim(0, 1.05)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.scatter(1, 1, label=\"Utopia\", c=\"red\", marker=\"*\", s=150)\n",
    "    plt.scatter(prune_rate[min], cosine_sim[min], color=\"k\", marker=\"o\", label=\"Optima\")\n",
    "    plt.xlabel(xlabel=\"pruning rate\")\n",
    "    plt.ylabel(ylabel=\"cosine similarity\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    del cosine_sim\n",
    "    del base_vec\n",
    "    del prune_net\n",
    "\n",
    "    optimal_sensitivity_percentage = (1.0 - prune_rate[min]) * 100\n",
    "    del prune_rate\n",
    "\n",
    "    return optimal_sensitivity_percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Executing</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T15:35:05.183145Z",
     "start_time": "2025-02-01T15:35:04.826590Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-15T16:42:44.594195Z",
     "iopub.status.busy": "2025-03-15T16:42:44.593928Z",
     "iopub.status.idle": "2025-03-15T16:43:06.539343Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "client_list = [i for i in range(SAFE_PFL_CONFIG[\"NUMBER_OF_CLIENTS\"])]\n",
    "assert len(client_list) == SAFE_PFL_CONFIG[\"NUMBER_OF_CLIENTS\"]\n",
    "\n",
    "clients = [\n",
    "    Client(\n",
    "        Net,\n",
    "        # lambda x : torch.optim.Adam(x, lr=0.001,  amsgrad=True),\n",
    "        lambda x: torch.optim.SGD(\n",
    "            x, lr=0.001, momentum=0.9, weight_decay=1e-4 \n",
    "            # x, lr=0.001, momentum=0.9,\n",
    "        ),  #! we have to use SGD since our base papers also tested their methods via SGD\n",
    "        i,\n",
    "        train_loaders[i],\n",
    "        test_loaders[i],\n",
    "    )\n",
    "    for i in client_list\n",
    "]\n",
    "\n",
    "server = Server(Net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T15:35:05.826887Z",
     "start_time": "2025-02-01T15:35:05.201153Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-15T16:43:06.540069Z",
     "iopub.status.busy": "2025-03-15T16:43:06.539873Z",
     "iopub.status.idle": "2025-03-15T16:43:08.722644Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for client in [clients[0], clients[3]]:\n",
    "    x, y = next(iter(client.train_loader))\n",
    "\n",
    "    log.info(\"Client {}:\".format(client.id))\n",
    "    plt.figure(figsize=(15, 1))\n",
    "    for i in range(10):\n",
    "        plt.subplot(1, 10, i + 1)\n",
    "        plt.imshow(x[i, 0].numpy().T, cmap=\"Greys\")\n",
    "\n",
    "    del x\n",
    "    del y\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T15:35:26.201255Z",
     "start_time": "2025-02-01T15:35:05.860359Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-15T16:45:42.687485Z",
     "iopub.status.busy": "2025-03-15T16:45:42.687166Z",
     "iopub.status.idle": "2025-03-15T16:49:32.261532Z",
     "shell.execute_reply": "2025-03-15T16:49:32.258211Z",
     "shell.execute_reply.started": "2025-03-15T16:45:42.687458Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cfl_stats = ExperimentLogger()\n",
    "cluster_indices = [np.arange(len(clients)).astype(\"int\")]\n",
    "global_clients_clustered = []\n",
    "CLUSTERING_LABELS = None\n",
    "STOP_CLUSTERING: bool = False\n",
    "\n",
    "for c_round in range(1, SAFE_PFL_CONFIG[\"FEDERATED_LEARNING_ROUNDS\"] + 1):\n",
    "    if c_round == 1:\n",
    "        for client in clients:\n",
    "            client.synchronize_with_server(server)\n",
    "\n",
    "    \"\"\"\n",
    "        Checking clustering conditions\n",
    "    \"\"\"\n",
    "    TRIGGER_CLUSTERING = (\n",
    "        not SAFE_PFL_CONFIG[\"FED_AVG\"]\n",
    "        and not STOP_CLUSTERING\n",
    "        and not SAFE_PFL_CONFIG[\"PRE_COMPUTED_OPTIMAL_CLUSTERING\"]\n",
    "        and c_round % SAFE_PFL_CONFIG[\"CLUSTERING_PERIOD\"] == 0\n",
    "        or SAFE_PFL_CONFIG[\"CLUSTER_AT_FIRST\"]\n",
    "    )\n",
    "    SAFE_PFL_CONFIG[\"CLUSTER_AT_FIRST\"] = False\n",
    "    \"\"\"\n",
    "        Participating clients training loop\n",
    "    \"\"\"\n",
    "    for index, client in enumerate(clients):\n",
    "        client.compute_weight_update(\n",
    "            be_ready_for_clustering=TRIGGER_CLUSTERING,\n",
    "            epochs=SAFE_PFL_CONFIG[\"ROUND_EPOCHS\"],\n",
    "        )\n",
    "\n",
    "    \"\"\"\n",
    "        Calculating the optimal sensitivity value (P)\n",
    "    \"\"\"\n",
    "    if (\n",
    "        c_round == 1\n",
    "        and SAFE_PFL_CONFIG[\"DISTANCE_METRIC\"]\n",
    "        == distances_constants.DISTANCE_COORDINATE\n",
    "        and SAFE_PFL_CONFIG[\"DYNAMIC_SENSITIVITY_PERCENTAGE\"]\n",
    "    ):\n",
    "        SAFE_PFL_CONFIG.update(\n",
    "            {\n",
    "                \"SENSITIVITY_PERCENTAGE\": calculate_optimal_sensitivity_percentage(\n",
    "                    clients[0].model\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "        log.info(\n",
    "            f'done calculating optimal sensitivity percentage with value of {SAFE_PFL_CONFIG[\"SENSITIVITY_PERCENTAGE\"]}'\n",
    "        )\n",
    "\n",
    "    if TRIGGER_CLUSTERING:\n",
    "        full_similarities = server.compute_pairwise_similarities(clients=clients)\n",
    "        log.warn(f\"Global clustering triggered {c_round}\")\n",
    "\n",
    "        clustering = server.cluster_clients(full_similarities)\n",
    "\n",
    "        # cleaning the memory up\n",
    "        del full_similarities\n",
    "        for client in clients:\n",
    "            client.gradients = {}\n",
    "\n",
    "        cluster_indices = []\n",
    "        CLUSTERING_LABELS = clustering.labels_\n",
    "        for label in np.unique(clustering.labels_):\n",
    "            cluster_indices.append(np.where(clustering.labels_ == label)[0].tolist())\n",
    "\n",
    "        if SAFE_PFL_CONFIG[\"SAVE_BEFORE_AGGREGATION_MODELS\"]:\n",
    "            for client in clients:\n",
    "                torch.save(\n",
    "                    client.model.state_dict(),\n",
    "                    MODEL_SAVING_PATH + f\"client_{client.id}_model.pt\",\n",
    "                )\n",
    "\n",
    "    elif (\n",
    "        c_round % SAFE_PFL_CONFIG[\"CLUSTERING_PERIOD\"] == 0\n",
    "        and SAFE_PFL_CONFIG[\"PRE_COMPUTED_OPTIMAL_CLUSTERING\"]\n",
    "    ):\n",
    "        cluster_indices = []\n",
    "        for label in np.unique(OPTIMAL_TRAIN_CLUSTERING):\n",
    "            cluster_indices.append(\n",
    "                np.where(OPTIMAL_TRAIN_CLUSTERING == label)[0].tolist()\n",
    "            )\n",
    "\n",
    "        log.info(\n",
    "            f\"clustering based on optimal clustering {cluster_indices} @ round number {c_round}\"\n",
    "        )\n",
    "\n",
    "        if SAFE_PFL_CONFIG[\"SAVE_BEFORE_AGGREGATION_MODELS\"]:\n",
    "            for client in clients:\n",
    "                torch.save(\n",
    "                    client.model.state_dict(),\n",
    "                    MODEL_SAVING_PATH + f\"client_{client.id}_model.pt\",\n",
    "                )\n",
    "\n",
    "    client_clusters = []\n",
    "    for cluster in cluster_indices:\n",
    "        new_orientation = []\n",
    "        for index in cluster:\n",
    "            new_orientation.append(clients[index])\n",
    "        client_clusters.append(new_orientation)\n",
    "    global_clients_clustered = client_clusters\n",
    "\n",
    "    # acc_clients = [client.evaluate() for client in clients]\n",
    "\n",
    "    server.aggregate_clusterwise(global_clients_clustered)\n",
    "\n",
    "    acc_clients = [client.evaluate() for client in clients]\n",
    "\n",
    "    if not STOP_CLUSTERING:\n",
    "        acc_mean = np.mean(acc_clients)\n",
    "        log.info(\n",
    "            f'checking whether to stop clustering or not with STOP_AVG_ACCURACY value of {SAFE_PFL_CONFIG[\"STOP_AVG_ACCURACY\"]} and averaged accuracy of {acc_mean}'\n",
    "        )\n",
    "        if acc_mean >= SAFE_PFL_CONFIG[\"STOP_AVG_ACCURACY\"] and (\n",
    "            np.array_equal(CLUSTERING_LABELS, OPTIMAL_TRAIN_CLUSTERING)\n",
    "            or np.array_equal(CLUSTERING_LABELS, OPTIMAL_TEST_CLUSTERING)\n",
    "        ):\n",
    "            log.info(f\"clustering stop triggered at round {c_round}\")\n",
    "            STOP_CLUSTERING = True\n",
    "\n",
    "    cfl_stats.log(\n",
    "        {\n",
    "            \"acc_clients\": acc_clients,\n",
    "            \"rounds\": c_round,\n",
    "            \"clusters\": cluster_indices,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    display_train_stats(\n",
    "        cfl_stats,\n",
    "        SAFE_PFL_CONFIG[\"FEDERATED_LEARNING_ROUNDS\"],\n",
    "        output_clarence_status=False,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6072062,
     "sourceId": 9887519,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
