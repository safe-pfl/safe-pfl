{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1OA8n1V3Jt_"
   },
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Install Pacakges</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T01:55:29.849551Z",
     "iopub.status.busy": "2025-01-12T01:55:29.849175Z",
     "iopub.status.idle": "2025-01-12T01:55:33.137544Z",
     "shell.execute_reply": "2025-01-12T01:55:33.136580Z",
     "shell.execute_reply.started": "2025-01-12T01:55:29.849523Z"
    },
    "id": "bRRfcrFG3JuA",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install datasets lxml TinyImageNet --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8oO1OslR3JuA"
   },
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Import Libraries</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T01:55:33.139159Z",
     "iopub.status.busy": "2025-01-12T01:55:33.138851Z",
     "iopub.status.idle": "2025-01-12T01:55:33.146970Z",
     "shell.execute_reply": "2025-01-12T01:55:33.146040Z",
     "shell.execute_reply.started": "2025-01-12T01:55:33.139130Z"
    },
    "id": "IWgcTDs4vXBk",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import csv\n",
    "import gc\n",
    "import logging\n",
    "import os\n",
    "import os.path\n",
    "import random\n",
    "import tarfile\n",
    "import warnings\n",
    "from collections import OrderedDict\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from math import sqrt\n",
    "from typing import Callable, List, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from scipy.spatial.distance import cosine, euclidean, jensenshannon\n",
    "from scipy.stats import wasserstein_distance\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tinyimagenet import TinyImageNet\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.model_zoo import tqdm\n",
    "from torchvision.datasets import (CIFAR10, CIFAR100, MNIST, STL10, SVHN,\n",
    "                                  DatasetFolder, FashionMNIST, ImageFolder)\n",
    "from torchvision.datasets.utils import (check_integrity,\n",
    "                                        download_file_from_google_drive)\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "from torchvision.transforms import Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChmAsGJP3JuB"
   },
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Garbage Collection</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-12T01:55:33.148862Z",
     "iopub.status.busy": "2025-01-12T01:55:33.148626Z",
     "iopub.status.idle": "2025-01-12T01:55:33.537493Z",
     "shell.execute_reply": "2025-01-12T01:55:33.536621Z",
     "shell.execute_reply.started": "2025-01-12T01:55:33.148843Z"
    },
    "id": "JzVjIyuE3JuB",
    "outputId": "592440ec-262a-4a15-f7e2-60e31cfb616b",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before memory cleaning:\n",
      "\n",
      "Allocated memory: 23.67 MB\n",
      "Cached memory: 52.00 MB\n",
      "after memory cleaning:\n",
      "\n",
      "Allocated memory: 23.67 MB\n",
      "Cached memory: 28.00 MB\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "\n",
    "def print_gpu_memory():\n",
    "    print(f\"Allocated memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "    print(f\"Cached memory: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "\n",
    "\n",
    "print(\"before memory cleaning:\\n\")\n",
    "print_gpu_memory()\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# cuda.select_device(0)\n",
    "# cuda.close()\n",
    "\n",
    "print(\"after memory cleaning:\\n\")\n",
    "print_gpu_memory()\n",
    "\n",
    "# ----------- manually clear memory in case of any error\n",
    "#!sudo fuser -v /dev/nvidia* or nvidia-smi\n",
    "# remove all python process ids from gpu\n",
    "#!sudo kill -9 PID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0GQ2etG3JuB"
   },
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Make Directories</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-12T01:55:33.538884Z",
     "iopub.status.busy": "2025-01-12T01:55:33.538644Z",
     "iopub.status.idle": "2025-01-12T01:55:33.826296Z",
     "shell.execute_reply": "2025-01-12T01:55:33.825171Z",
     "shell.execute_reply.started": "2025-01-12T01:55:33.538864Z"
    },
    "id": "xCdedy7p3JuB",
    "outputId": "3433e7a0-910f-4264-ce78-226fe36591c1",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘models’: File exists\n",
      "mkdir: cannot create directory ‘models/before_aggregation’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir models\n",
    "!mkdir models/before_aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-XrkWV93JuB"
   },
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Configs</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T01:55:33.827746Z",
     "iopub.status.busy": "2025-01-12T01:55:33.827485Z",
     "iopub.status.idle": "2025-01-12T01:55:33.837577Z",
     "shell.execute_reply": "2025-01-12T01:55:33.836816Z",
     "shell.execute_reply.started": "2025-01-12T01:55:33.827719Z"
    },
    "id": "_X_jTe9C3JuB",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "os.environ[\"PL_GLOBAL_SEED\"] = str(seed)\n",
    "\n",
    "sns.set_theme(\n",
    "    style=\"darkgrid\", font_scale=1.5, rc={\"axes.unicode_minus\": False}\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# to produce reproducible results (like random.seed())\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "logging.basicConfig()\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T01:55:33.838700Z",
     "iopub.status.busy": "2025-01-12T01:55:33.838432Z",
     "iopub.status.idle": "2025-01-12T01:55:33.849684Z",
     "shell.execute_reply": "2025-01-12T01:55:33.848844Z",
     "shell.execute_reply.started": "2025-01-12T01:55:33.838668Z"
    },
    "id": "qVX67JHf3JuB",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "|MODEL_TYPE |  DATASET_TYPE    | NUMBER_OF_CLASSES| PARTITION      | ROUND_EPOCHS| SENSITIVITY_PERCENTAGE   | TRAIN/TEST_BATCH_SIZE | TRANSFORM_INPUT_SIZE  |\n",
    "|-----------|------------------|------------------|----------------|-------------|--------------------------|-----------------------|-----------------------|\n",
    "|cnn        |  fmnist          | 10               | noniid-#label2 | 1           | 10                       | 128                   | 128                   |\n",
    "|resnet18   |  cifar10         | 10               | noniid-#label2 | 1           | 10                       | 128                   | 128                   |\n",
    "|resnet50   |  cifar100        | 100              | noniid-#label20| 10          | 10                       | 128                   | 32                    |\n",
    "|mobilenet  |  svhn            | 10               | noniid-#label2 | 1           | 10                       | 64                    | 224                   |\n",
    "|alexnet    |  stl10           | 10               | noniid-#label10| 10          | 10                       | 128                   | 128                   |\n",
    "|-----------|------------------|------------------|----------------|-------------|--------------------------|-----------------------|-----------------------|\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" LEARNING HYPERPARAMETERS\"\"\"\n",
    "MODEL_TYPE = \"cnn\"\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "TEST_BATCH_SIZE = 128\n",
    "# by default set to 0.001 and for AlexNet set to 0.0001\n",
    "LEARNING_RATE = 0.0001\n",
    "WEIGHT_DECAY = 1e-4\n",
    "NUMBER_OF_CLIENTS = 10\n",
    "# set to 10 for AlexNet\n",
    "ROUND_EPOCHS = 1\n",
    "\"\"\" DATA DISTRIBUTION \"\"\"\n",
    "DATASET_TYPE = \"fmnist\"\n",
    "PARTITION = (\n",
    "    \"noniid-\" + \"#label2\"\n",
    ")  # the second part accepted format is: \"labeldir\" (Dirichlet), \"#label20\", noniid-fix\n",
    "DIRICHLET_BETA = 0.1\n",
    "NUMBER_OF_CLASSES = 10\n",
    "TRANSFORM_INPUT_SIZE = 128  # just works for svhn/stl10 dataset transformer\n",
    "DESIRED_DISTRIBUTION = [\n",
    "    [2948, 0, 5293, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 3466, 2330, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [2000, 0, 5292, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 4249, 3729, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 3729, 0, 2465, 0, 0, 0],\n",
    "    [0, 0, 0, 3720, 0, 0, 2145, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 3865, 2864, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 1865, 2863, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 5045, 3248],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 3465, 0, 1329],\n",
    "]\n",
    "\"\"\" \n",
    "SAFE-PFL\n",
    "DISTANCE_METRIC values are:\n",
    "- coordinate\n",
    "- cosine\n",
    "- euclidean\n",
    "- jensen-shannon\n",
    "- wasserstein\n",
    "\"\"\"\n",
    "SAVE_BEFORE_AGGREGATION_MODELS = True\n",
    "DO_CLUSTER = True\n",
    "CLUSTERING_PERIOD = 1  # Set to 1 to run simple Federated Learning with out clustering\n",
    "FEDERATED_LEARNING_ROUNDS = 1  # The round in with Federated Learning will be executed\n",
    "# set to 20 for ResNet50\n",
    "SENSITIVITY_PERCENTAGE = 10\n",
    "DISTANCE_METRIC = \"coordinate\"\n",
    "# cosine similarity options\n",
    "JUST_COMPARE_SIGNIFICANCE = False\n",
    "ZERO_INSIGNIFICANT_IN_BOTH = False\n",
    "COMPARE_MOST_SIGNIFICANCE_ONE = False\n",
    "COMPARE_LESS_SIGNIFICANCE_ZERO = (\n",
    "    False  # Set to True for Selective Parameter Cosine when DISTANCE_METRIC is cosine\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-12T01:55:33.850966Z",
     "iopub.status.busy": "2025-01-12T01:55:33.850676Z",
     "iopub.status.idle": "2025-01-12T01:55:33.868843Z",
     "shell.execute_reply": "2025-01-12T01:55:33.868055Z",
     "shell.execute_reply.started": "2025-01-12T01:55:33.850937Z"
    },
    "id": "uu3Fu3A3qnPR",
    "outputId": "45cda9ce-b5d6-4d22-8419-861beac9014a",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTENTION: The Model=cnn-Dataset=fmnist-N=noniid-#label2-P=10_on=coordinate_at=2025-01-14_19.log will be truncated at each run\n"
     ]
    }
   ],
   "source": [
    "log_path = None\n",
    "\n",
    "log_path = datetime.now().strftime(f\"Model={MODEL_TYPE}-Dataset={DATASET_TYPE}-N={PARTITION}-P={SENSITIVITY_PERCENTAGE}_on={DISTANCE_METRIC}_at=%Y-%m-%d_%H\")\n",
    "\n",
    "log_file = log_path + \".log\"\n",
    "print(f\"ATTENTION: The {log_file} will be truncated at each run\")\n",
    "open(log_file,  \"w\").close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSl3rZx23JuC"
   },
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Model Network</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T01:55:33.871421Z",
     "iopub.status.busy": "2025-01-12T01:55:33.871170Z",
     "iopub.status.idle": "2025-01-12T01:55:33.883394Z",
     "shell.execute_reply": "2025-01-12T01:55:33.882638Z",
     "shell.execute_reply.started": "2025-01-12T01:55:33.871402Z"
    },
    "id": "evEmrviBwIoH",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "    ):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        if MODEL_TYPE == \"resnet18\":\n",
    "            self.resnet = models.resnet18(pretrained=False)\n",
    "            if DATASET_TYPE == \"mnist\":\n",
    "                self.resnet.conv1 = nn.Conv2d(\n",
    "                    1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
    "                )\n",
    "            self.resnet.fc = nn.Linear(\n",
    "                self.resnet.fc.in_features, NUMBER_OF_CLASSES\n",
    "            )\n",
    "        elif MODEL_TYPE == \"resnet50\":\n",
    "            self.resnet = models.resnet50(pretrained=False)\n",
    "            self.resnet.fc = nn.Linear(self.resnet.fc.in_features, NUMBER_OF_CLASSES)\n",
    "        elif MODEL_TYPE == \"cnn\":\n",
    "            self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "            self.pool = nn.MaxPool2d(2, 2)\n",
    "            self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "            self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "            self.fc2 = nn.Linear(120, 84)\n",
    "            self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "        elif MODEL_TYPE == \"mobilenet\":\n",
    "            self.mobilenet_v3_large = models.mobilenet_v3_large(pretrained=False)\n",
    "            self.mobilenet_v3_large.classifier[3] = nn.Linear(self.mobilenet_v3_large.classifier[3].in_features, NUMBER_OF_CLASSES)\n",
    "\n",
    "        elif MODEL_TYPE == \"vgg16\":\n",
    "            self.vgg16 = models.vgg11(pretrained=False)\n",
    "            self.vgg16.classifier[6] = nn.Linear(4096, NUMBER_OF_CLASSES)\n",
    "\n",
    "        elif MODEL_TYPE == \"alexnet\":\n",
    "            # self.features = models.alexnet(pretrained=False)\n",
    "            # self.features.classifier[6] = nn.Linear(4096, NUMBER_OF_CLASSES)\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            )\n",
    "            \n",
    "            # Calculate the size of the output from the last convolutional layer\n",
    "            self._to_linear = 128 * (128 // 8) * (128 // 8)  # Adjust based on pooling layers\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(self._to_linear, 512),  # Adjusted input size\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(),\n",
    "                nn.Linear(512, NUMBER_OF_CLASSES),\n",
    "            )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = None\n",
    "\n",
    "        if MODEL_TYPE in [\"resnet18\", \"resnet50\"]:\n",
    "            out = self.resnet(x)\n",
    "        elif MODEL_TYPE == \"cnn\":\n",
    "            x = self.pool(F.relu(self.conv1(x)))\n",
    "            x = self.pool(F.relu(self.conv2(x)))\n",
    "            x = x.view(x.size(0), 16 * 4 * 4)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            out = x\n",
    "\n",
    "        elif MODEL_TYPE == \"mobilenet\":\n",
    "            out = self.mobilenet_v3_large(x)\n",
    "\n",
    "        elif MODEL_TYPE == \"vgg16\":\n",
    "            out = self.vgg16(x)\n",
    "\n",
    "        elif MODEL_TYPE == \"alexnet\":\n",
    "            # x = self.features(x)\n",
    "            x = self.features(x)\n",
    "            x = x.view(x.size(0), -1)  # Flatten\n",
    "            x = self.classifier(x)\n",
    "            out = x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jiowr9l83JuC"
   },
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Learning</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T01:55:33.885447Z",
     "iopub.status.busy": "2025-01-12T01:55:33.885201Z",
     "iopub.status.idle": "2025-01-12T01:55:33.900138Z",
     "shell.execute_reply": "2025-01-12T01:55:33.899426Z",
     "shell.execute_reply.started": "2025-01-12T01:55:33.885429Z"
    },
    "id": "cCM4LUpzwTXE",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(loader, model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total\n",
    "\n",
    "\n",
    "def train(net, node_id, train_loader, epochs: int):\n",
    "    \"\"\"Train the network on the training set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # if MODEL_TYPE != \"alexnet\": \n",
    "    if True: \n",
    "        optimizer = torch.optim.Adam(\n",
    "            net.parameters(),\n",
    "            lr=LEARNING_RATE,\n",
    "            betas=(0.9, 0.999),\n",
    "            eps=1e-7,\n",
    "            weight_decay=WEIGHT_DECAY,\n",
    "        )\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(net.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    net.train()\n",
    "    for epoch in range(epochs):\n",
    "        correct, total, epoch_loss = 0, 0, 0.0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            total += labels.size(0)\n",
    "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_loss /= len(train_loader.dataset)\n",
    "        epoch_acc = correct / total\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")\n",
    "\n",
    "    # Return the final accuracy and loss\n",
    "    return epoch_acc, epoch_loss\n",
    "\n",
    "\n",
    "def test(net, test_loader):\n",
    "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    loss /= len(test_loader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVgowNUE3JuC"
   },
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Client</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T01:55:33.901059Z",
     "iopub.status.busy": "2025-01-12T01:55:33.900862Z",
     "iopub.status.idle": "2025-01-12T01:55:33.918571Z",
     "shell.execute_reply": "2025-01-12T01:55:33.917833Z",
     "shell.execute_reply.started": "2025-01-12T01:55:33.901041Z"
    },
    "id": "pZKNLgYs3JuC",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Client:\n",
    "    def __init__(self, net, node_id, train_loader, test_loader):\n",
    "        self.net = net.to(DEVICE)\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.node_id = node_id\n",
    "        self.train_acc, self.test_acc = 0.0, 0.0\n",
    "        self.global_net = Net().to(DEVICE)\n",
    "\n",
    "    def set_bias(self, pref, bias):\n",
    "        self.bias = bias\n",
    "        self.pref = pref\n",
    "\n",
    "    def set_shard(self, shard):\n",
    "        self.shard = shard\n",
    "\n",
    "    def get_global_net(self):\n",
    "        return self.global_net\n",
    "\n",
    "    def setting_parameters(self, parameters: List[np.ndarray]):\n",
    "        params_dict = zip(self.net.state_dict().items(), parameters)\n",
    "        state_dict = OrderedDict(\n",
    "            {k: torch.Tensor(v).to(DEVICE) for k, v in params_dict}\n",
    "        )\n",
    "        self.net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "    def getting_parameters(self) -> List[np.ndarray]:\n",
    "        return [val.cpu().numpy() for _, val in self.net.state_dict().items()]\n",
    "\n",
    "    def fit(self, parameters):\n",
    "        self.setting_parameters(parameters)\n",
    "        train(self.net, self.node_id, self.train_loader, epochs=ROUND_EPOCHS)\n",
    "        return self.getting_parameters(), len(self.train_loader), {}\n",
    "\n",
    "    def evaluate(self, parameters):\n",
    "        self.setting_parameters(parameters)\n",
    "        loss, accuracy = test(self.net, self.test_loader)\n",
    "        return float(loss), len(self.test_loader), {\"accuracy\": float(accuracy)}\n",
    "\n",
    "    def Train_test_and_return_acc(self):\n",
    "        self.train_acc, _ = train(self.net, self.node_id, self.train_loader, ROUND_EPOCHS)\n",
    "        self.test_acc, _ = test(self.net, self.test_loader)\n",
    "        return self.train_acc, self.test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0snFUi-3JuC"
   },
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Server</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T01:55:33.919972Z",
     "iopub.status.busy": "2025-01-12T01:55:33.919641Z",
     "iopub.status.idle": "2025-01-12T01:55:33.937013Z",
     "shell.execute_reply": "2025-01-12T01:55:33.936433Z",
     "shell.execute_reply.started": "2025-01-12T01:55:33.919944Z"
    },
    "id": "0SW7jKZNwbhJ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def divide_nested_list(nested_list, divisor):\n",
    "    for i in range(len(nested_list)):\n",
    "        if isinstance(nested_list[i], list):\n",
    "            divide_nested_list(nested_list[i], divisor)\n",
    "        else:\n",
    "            nested_list[i] /= divisor\n",
    "    return nested_list\n",
    "\n",
    "\n",
    "def zero_nested_list(nested_list):\n",
    "    for i in range(len(nested_list)):\n",
    "        if isinstance(nested_list[i], list):\n",
    "            zero_nested_list(nested_list[i])\n",
    "        else:\n",
    "            nested_list[i] = 0\n",
    "    return nested_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T01:55:33.938087Z",
     "iopub.status.busy": "2025-01-12T01:55:33.937852Z",
     "iopub.status.idle": "2025-01-12T01:55:33.955413Z",
     "shell.execute_reply": "2025-01-12T01:55:33.954674Z",
     "shell.execute_reply.started": "2025-01-12T01:55:33.938059Z"
    },
    "id": "mw8QL1Qn3JuC",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Server:\n",
    "    def __init__(self, save_dir):\n",
    "        self.models = []\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(self.save_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "    def append_model(self, model: nn.Module):\n",
    "        if not isinstance(model, nn.Module):\n",
    "            raise TypeError(\"Only instances of nn.Module can be appended\")\n",
    "        self.models.append(model)\n",
    "\n",
    "    def aggregate(self):\n",
    "        if not self.models:\n",
    "            raise ValueError(\"No models added to the server.\")\n",
    "            \n",
    "        print(\"Model numbers:\", len(self.models))\n",
    "        device = next(self.models[0].parameters()).device\n",
    "        \n",
    "        if SAVE_BEFORE_AGGREGATION_MODELS:\n",
    "            for i, model in enumerate(self.models):\n",
    "                model_state_dict = model.state_dict()\n",
    "                model_save_path = os.path.join(self.save_dir, f'node_{i}.pth')\n",
    "                torch.save(model_state_dict, model_save_path)\n",
    "                print(f\"Saved model {i} state dict to {model_save_path}\")\n",
    "        else:\n",
    "            print(\"Skip saving before aggregation models because the value of `SAVE_BEFORE_AGGREGATION_MODELS` is False\")\n",
    "\n",
    "\n",
    "        for model in self.models:\n",
    "            model.to(device)\n",
    "        \n",
    "        avg_model = Net().to(device)\n",
    "        with torch.no_grad():\n",
    "            for param_name, avg_param in avg_model.named_parameters():\n",
    "                temp = torch.zeros_like(avg_param)\n",
    "                for model in self.models:\n",
    "                    model_param = dict(model.named_parameters())[param_name]\n",
    "                    temp += model_param.data\n",
    "                avg_param.copy_(temp / len(self.models))\n",
    "        \n",
    "        return avg_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A5a5ikHf3JuC"
   },
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Clustering</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T01:55:33.956378Z",
     "iopub.status.busy": "2025-01-12T01:55:33.956140Z",
     "iopub.status.idle": "2025-01-12T01:55:33.983015Z",
     "shell.execute_reply": "2025-01-12T01:55:33.982332Z",
     "shell.execute_reply.started": "2025-01-12T01:55:33.956360Z"
    },
    "id": "asSZtfDAwmrd",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def find_num_cluster(clusters):\n",
    "    num_cluster = []\n",
    "    for item in clusters:\n",
    "        if item not in num_cluster:\n",
    "            num_cluster.append(item)\n",
    "    return len(num_cluster)\n",
    "\n",
    "\n",
    "class Clustering:\n",
    "    def __init__(self, clients, trainLoaders, percentage, step):\n",
    "        self.clients = clients\n",
    "        self.step = step\n",
    "        self.num_nodes = len(clients)\n",
    "        self.percentage = percentage\n",
    "        self.Mask_Number = 0\n",
    "        self.maskIds = []\n",
    "        self.grads = []\n",
    "        self.load_and_calculate_sensitivity(trainLoaders)\n",
    "        self.distances = self.calculate_distance()\n",
    "        self.Clusters = self.make_clusters()\n",
    "\n",
    "    def assign_save_ids_to_weights(self, model):\n",
    "        weight_id_map = {}\n",
    "        weight_id = 0\n",
    "        for name, parameter in model.named_parameters():\n",
    "            weight_id_map[name] = {}\n",
    "            num_weights = parameter.numel()\n",
    "            for i in range(num_weights):\n",
    "                weight_id_map[name][i] = weight_id\n",
    "                weight_id += 1\n",
    "        filename = \"weight_to_id.csv\"\n",
    "        if not os.path.exists(filename):\n",
    "            with open(filename, \"w\", newline=\"\") as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                writer.writerow([\"Layer\", \"Weight Index\", \"Weight ID\"])\n",
    "                for layer_name, indices in weight_id_map.items():\n",
    "                    for index, weight_id in indices.items():\n",
    "                        writer.writerow([layer_name, index, weight_id])\n",
    "        return weight_id_map\n",
    "\n",
    "    def load_and_calculate_sensitivity(self, trainLoaders):\n",
    "        \"\"\"\n",
    "        Calculate sensitivity for each client and store the results in the object.\n",
    "        \"\"\"\n",
    "        for cid in self.clients:\n",
    "            model = load_torch_model(cid).to(DEVICE)\n",
    "            # testing\n",
    "            model.eval()\n",
    "            sensitivity_value = self.calculate_sensitivity(\n",
    "                model, trainLoaders[int(cid)]\n",
    "            )\n",
    "            weight_id_map = self.assign_save_ids_to_weights(\n",
    "                load_torch_model(0).to(DEVICE)\n",
    "            )\n",
    "\n",
    "            mask_ID, weights = self.get_maskIds(\n",
    "                sensitivity_value, weight_id_map, self.percentage, cid\n",
    "            )  # top sensitive weights will filter here\n",
    "            print(f\"Model weights and sensitivity data for client #{cid} processed.\")\n",
    "\n",
    "            self.maskIds.append(mask_ID)\n",
    "            self.grads.append(weights)\n",
    "\n",
    "    def calculate_sensitivity(self, model, dataloader):\n",
    "        # model.train()\n",
    "        model.eval()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        gradient_sums = {}\n",
    "        for name, param in model.named_parameters():\n",
    "            gradient_sums[name] = 0.0\n",
    "            param.requires_grad_(True)\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Backward pass\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            sensitivities = {}\n",
    "            for name, parameter in model.named_parameters():\n",
    "                grads = parameter.grad.abs().view(-1).cpu().numpy()\n",
    "                for i, grad in enumerate(grads):\n",
    "                    sensitivities[(name, i)] = grad\n",
    "            return sensitivities\n",
    "\n",
    "    # def get_maskIds(self, sensitivity_values_node, weight_id_map, sensitive_percentage):\n",
    "    #     num_weights = len(sensitivity_values_node)\n",
    "    #     top_k = int(np.ceil(sensitive_percentage * num_weights / 100))\n",
    "    #     self.Mask_Number = top_k\n",
    "    #     sorted_weights = sorted(\n",
    "    #         sensitivity_values_node.items(), key=lambda item: item[1], reverse=True\n",
    "    #     )[:top_k]\n",
    "    #     weights = [weight for (layer, index), weight in sensitivity_values_node.items()]\n",
    "    #     top_weight_ids = [\n",
    "    #         weight_id_map[layer][index] for (layer, index), _ in sorted_weights\n",
    "    #     ]\n",
    "    #     return top_weight_ids, weights\n",
    "    def get_maskIds(self, sensitivity_values_node, weight_id_map, sensitive_percentage, cid):\n",
    "        \"\"\"\n",
    "        Get the top sensitive weights and their IDs, and save all model parameters in order of importance before applying top_k.\n",
    "        \"\"\"\n",
    "        num_weights = len(sensitivity_values_node)\n",
    "        top_k = int(np.ceil(sensitive_percentage * num_weights / 100))\n",
    "        self.Mask_Number = top_k\n",
    "\n",
    "        # Sort all weights by sensitivity in descending order\n",
    "        sorted_weights = sorted(\n",
    "            sensitivity_values_node.items(), key=lambda item: item[1], reverse=True\n",
    "        )\n",
    "\n",
    "        self.save_model_parameters(sorted_weights, weight_id_map, cid)\n",
    "\n",
    "        top_sensitive_weights = sorted_weights[:top_k]\n",
    "\n",
    "        weights = [weight for (layer, index), weight in top_sensitive_weights]\n",
    "        top_weight_ids = [weight_id_map[layer][index] for (layer, index), _ in top_sensitive_weights]\n",
    "\n",
    "        return top_weight_ids, weights\n",
    "\n",
    "    def save_model_parameters(self, sorted_weights, weight_id_map, cid):\n",
    "        \"\"\"\n",
    "        Save all model parameters in order of importance to a file.\n",
    "        \"\"\"\n",
    "        results_dir = f\"model_parameters/{self.step}/\"\n",
    "        if not os.path.exists(results_dir):\n",
    "            os.makedirs(results_dir)\n",
    "\n",
    "        parameters_data = []\n",
    "        for (layer, index), weight in sorted_weights:\n",
    "            weight_id = weight_id_map[layer][index]\n",
    "            parameters_data.append({\n",
    "                \"weight_id\": weight_id,\n",
    "                \"weight_value\": weight\n",
    "            })\n",
    "\n",
    "        filename = os.path.join(results_dir, f\"all_model_parameters_ordered_by_importance_for_client_{cid}.csv\")\n",
    "        with open(filename, mode=\"w\", newline=\"\") as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=[\"weight_id\", \"weight_value\"])\n",
    "            writer.writeheader()\n",
    "            writer.writerows(parameters_data)\n",
    "\n",
    "        print(f\"All model parameters saved to {filename}\")\n",
    "\n",
    "    def normalize(self, distances, sensitive):\n",
    "        normal_distances = np.zeros((self.num_nodes, self.num_nodes))\n",
    "        for i in range(self.num_nodes):\n",
    "            normal_distances[i][i] = 0\n",
    "            for j in range(i + 1, self.num_nodes):\n",
    "                normal_distances[i][j] = normal_distances[j][i] = distances[i][j] / len(\n",
    "                    sensitive\n",
    "                )\n",
    "        return normal_distances\n",
    "\n",
    "    def calculate_common_ids(self, index1, index2):\n",
    "        arr1 = self.maskIds[index1]\n",
    "        arr2 = self.maskIds[index2]\n",
    "        sarr1 = set(arr1)\n",
    "        sarr2 = set(arr2)\n",
    "        inter = sarr1.intersection(sarr2)\n",
    "        similarity1 = len(inter)\n",
    "        return similarity1\n",
    "\n",
    "    def calculate_distance(\n",
    "        self,\n",
    "    ):\n",
    "        similarity_matrix = np.zeros((self.num_nodes, self.num_nodes))\n",
    "\n",
    "        for i in range(self.num_nodes):\n",
    "            for j in range(i + 1, self.num_nodes):\n",
    "\n",
    "                if DISTANCE_METRIC == \"coordinate\":\n",
    "                    similarity = self.calculate_common_ids(i, j)\n",
    "                elif DISTANCE_METRIC == \"cosine\":\n",
    "                    if JUST_COMPARE_SIGNIFICANCE:\n",
    "                        np_grad_i = np.array(self.grads[i])\n",
    "                        np_grad_j = np.array(self.grads[j])\n",
    "                        grad_i_significant_indices = (\n",
    "                            self.get_significant_weights_indices(np_grad_i)\n",
    "                        )\n",
    "                        grad_j_significant_indices = (\n",
    "                            self.get_significant_weights_indices(np_grad_j)\n",
    "                        )\n",
    "                        grad_i_significant_weights = np_grad_i[\n",
    "                            grad_i_significant_indices\n",
    "                        ]\n",
    "                        grad_j_significant_weights = np_grad_j[\n",
    "                            grad_j_significant_indices\n",
    "                        ]\n",
    "                        similarity = 1 - cosine(\n",
    "                            grad_i_significant_weights, grad_j_significant_weights\n",
    "                        )\n",
    "                    elif ZERO_INSIGNIFICANT_IN_BOTH:\n",
    "                        modified_grads_i, modified_grads_j = (\n",
    "                            self.zero_insignificant_in_both(\n",
    "                                np.array(self.grads[i]), np.array(self.grads[j])\n",
    "                            )\n",
    "                        )\n",
    "                        similarity = 1 - cosine(modified_grads_i, modified_grads_j)\n",
    "                    elif COMPARE_MOST_SIGNIFICANCE_ONE:\n",
    "                        grad_i = np.array(self.grads[i])\n",
    "                        grad_j = self.set_top_percent_to_one(np.array(self.grads[j]))\n",
    "                        similarity = 1 - cosine(grad_i, grad_j)\n",
    "                    elif COMPARE_LESS_SIGNIFICANCE_ZERO:\n",
    "                        grad_i = np.array(self.grads[i])\n",
    "                        grad_j = self.set_least_significant_to_zero(\n",
    "                            np.array(self.grads[j])\n",
    "                        )\n",
    "                        similarity = 1 - cosine(grad_i, grad_j)\n",
    "                    else:\n",
    "                        similarity = 1 - cosine(self.grads[i], self.grads[j])\n",
    "                elif DISTANCE_METRIC == \"euclidean\":\n",
    "                    # Euclidean distance\n",
    "                    similarity = -euclidean(\n",
    "                        self.grads[i], self.grads[j]\n",
    "                    )  # Negative for clustering\n",
    "                elif DISTANCE_METRIC == \"jensen-shannon\":\n",
    "                    # Jensen-Shannon divergence\n",
    "                    similarity = -jensenshannon(\n",
    "                        self.grads[i], self.grads[j]\n",
    "                    )  # Negative for clustering\n",
    "                elif DISTANCE_METRIC == \"wasserstein\":\n",
    "                    # Wasserstein distance\n",
    "                    similarity = -wasserstein_distance(\n",
    "                        self.grads[i], self.grads[j]\n",
    "                    )  # Negative for clustering\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported distance metric: {DISTANCE_METRIC}\")\n",
    "                similarity_matrix[i, j] = similarity\n",
    "                similarity_matrix[j, i] = similarity\n",
    "            similarity_matrix[i, i] = self.Mask_Number\n",
    "        distances = self.Mask_Number - similarity_matrix\n",
    "\n",
    "        self.save_distances_to_csv(distances)\n",
    "        with open(log_file, \"a\") as f:\n",
    "            f.write(\"\\n Start of the Similarity matrix\")\n",
    "            f.write(f\"\\n{distances}\")\n",
    "            f.write(\"\\n End of Similarity matrix\")\n",
    "\n",
    "        return distances\n",
    "\n",
    "    def index_to_value(self, groups):\n",
    "        value_groups = []\n",
    "        for group in groups:\n",
    "            list1 = []\n",
    "            for index in group:\n",
    "                list1.append(self.clients[index])\n",
    "            value_groups.append(list1)\n",
    "        return value_groups\n",
    "\n",
    "    def make_clusters(self):\n",
    "        normal_distances = (self.distances + self.distances.T) / 2\n",
    "        np.fill_diagonal(normal_distances, 0)\n",
    "        affinity_propagation = AffinityPropagation(affinity=\"precomputed\", random_state=0)\n",
    "        normal_distances = -normal_distances\n",
    "        clusters = affinity_propagation.fit_predict(normal_distances)\n",
    "        print(f\"cluster results:{clusters}\")\n",
    "        # Find the maximum cluster label from the assigned labels\n",
    "        max_label = max(clusters)\n",
    "        # Assign unique positive labels to noise points (initially labeled as -1)\n",
    "        noise_indices = clusters == -1\n",
    "        unique_noise_labels = np.arange(\n",
    "            max_label + 1, max_label + 1 + np.sum(noise_indices)\n",
    "        )\n",
    "        clusters[noise_indices] = unique_noise_labels\n",
    "        cluster_list = [\n",
    "            np.where(clusters == cluster_id)[0].tolist()\n",
    "            for cluster_id in range(find_num_cluster(clusters))\n",
    "        ]\n",
    "        cluster_list = self.index_to_value(cluster_list)\n",
    "        return cluster_list\n",
    "\n",
    "    def save_distances_to_csv(self, distances):\n",
    "        \"\"\"\n",
    "        Save the distance matrix to a CSV file.\n",
    "        \"\"\"\n",
    "        filename = f\"distances_{DISTANCE_METRIC}.csv\"\n",
    "        with open(filename, mode=\"w\", newline=\"\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Node\"] + [f\"Node_{i}\" for i in range(self.num_nodes)])\n",
    "            for i, row in enumerate(distances):\n",
    "                writer.writerow([f\"Node_{i}\"] + row.tolist())\n",
    "\n",
    "        print(f\"Distance matrix saved to {filename}\")\n",
    "\n",
    "    def set_top_percent_to_one(self, arr):\n",
    "        modified_array = np.copy(arr)\n",
    "        num_elements_to_set = int(len(arr) * self.percentage / 100)\n",
    "        if num_elements_to_set == 0:\n",
    "            return modified_array\n",
    "        indices_to_set = np.argpartition(modified_array, -num_elements_to_set)[\n",
    "            -num_elements_to_set:\n",
    "        ]\n",
    "        modified_array[indices_to_set] = 1\n",
    "        return modified_array\n",
    "\n",
    "    def set_least_significant_to_zero(self, arr):\n",
    "        modified_array = np.copy(arr)\n",
    "        num_elements_to_zero = int(len(arr) * (100 - self.percentage) / 100)\n",
    "        if num_elements_to_zero == 0:\n",
    "            return modified_array\n",
    "        indices_to_zero = np.argpartition(modified_array, num_elements_to_zero)[\n",
    "            :num_elements_to_zero\n",
    "        ]\n",
    "        modified_array[indices_to_zero] = 0\n",
    "        return modified_array\n",
    "\n",
    "    def get_significant_weights_indices(self, arr):\n",
    "        num_elements = len(arr)\n",
    "        num_significant = int(np.ceil(num_elements * self.percentage / 100))\n",
    "        if num_significant == 0:\n",
    "            return np.array([], dtype=int)\n",
    "        significant_indices = np.argpartition(-arr, num_significant - 1)[\n",
    "            :num_significant\n",
    "        ]\n",
    "        significant_indices = significant_indices[np.argsort(-arr[significant_indices])]\n",
    "        return significant_indices\n",
    "\n",
    "    def zero_insignificant_in_both(self, arr_i, arr_j):\n",
    "        num_params = len(arr_i)\n",
    "        significant_indices_i = self.get_significant_weights_indices(arr_i)\n",
    "        significant_indices_j = self.get_significant_weights_indices(arr_j)\n",
    "        all_indices = set(range(num_params))\n",
    "        insignificant_in_i = all_indices - set(significant_indices_i)\n",
    "        insignificant_in_j = all_indices - set(significant_indices_j)\n",
    "        insignificant_in_both = insignificant_in_i.intersection(insignificant_in_j)\n",
    "        modified_arr_i = np.copy(arr_i)\n",
    "        modified_arr_j = np.copy(arr_j)\n",
    "        insignificant_in_both = np.array(list(insignificant_in_both), dtype=int)\n",
    "        modified_arr_i[insignificant_in_both] = 0\n",
    "        modified_arr_j[insignificant_in_both] = 0\n",
    "        return modified_arr_i, modified_arr_j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAturGl13JuD"
   },
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Federated Learning</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T01:55:33.984211Z",
     "iopub.status.busy": "2025-01-12T01:55:33.983948Z",
     "iopub.status.idle": "2025-01-12T01:55:34.002765Z",
     "shell.execute_reply": "2025-01-12T01:55:34.002154Z",
     "shell.execute_reply.started": "2025-01-12T01:55:33.984185Z"
    },
    "id": "DyY6UPxMw0EA",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FL:\n",
    "    def __init__(\n",
    "        self,\n",
    "        clients,\n",
    "        client_initial_models,\n",
    "        round_number,\n",
    "        train_loaders,\n",
    "        test_loaders,\n",
    "        SENSITIVITY_PERCENTAGE,\n",
    "    ):\n",
    "        self.clients = clients\n",
    "        self.NUMBER_OF_CLIENTS = len(clients)\n",
    "        self.client_initial_models = client_initial_models\n",
    "        self.SENSITIVITY_PERCENTAGE = SENSITIVITY_PERCENTAGE\n",
    "        self.train_loaders = train_loaders\n",
    "        self.test_loaders = test_loaders\n",
    "        self.round_number = round_number\n",
    "        self.global_model = None\n",
    "        self.clustering_result = None\n",
    "        self.client_obj_list = []\n",
    "        self.accuracies = {}\n",
    "        self.training()\n",
    "\n",
    "    def training(self):\n",
    "        for cid in self.clients:\n",
    "            print(\"cid is:\", cid)\n",
    "            client = Client(\n",
    "                self.client_initial_models[self.clients.index(int(cid))],\n",
    "                cid,\n",
    "                self.train_loaders[int(cid)],\n",
    "                self.test_loaders[int(cid)],\n",
    "            )\n",
    "            self.client_obj_list.append(client)\n",
    "        global_model = Net()\n",
    "        os.makedirs(\"models\", exist_ok=True)\n",
    "        start_time = datetime.now()\n",
    "        for r in range(self.round_number):\n",
    "            print(f\"\\nRound {r+1}/{self.round_number}\")\n",
    "            server = Server(f\"./models/{MODEL_TYPE}/{DATASET_TYPE}/{ROUND_EPOCHS * FEDERATED_LEARNING_ROUNDS * CLUSTERING_PERIOD}_epochs_{FEDERATED_LEARNING_ROUNDS * CLUSTERING_PERIOD}_fl_rounds/{(r+1)* ROUND_EPOCHS}_epochs\")\n",
    "            global_accuracy = 0\n",
    "            for cid in self.clients:\n",
    "                train_acc, test_acc = self.client_obj_list[\n",
    "                    self.clients.index(cid)\n",
    "                ].Train_test_and_return_acc()\n",
    "                print(\n",
    "                    \"_____________________________________________________________________________________________________________\"\n",
    "                )\n",
    "                print(f\"node {cid}: train_acc: {train_acc}, test_acc:{test_acc}\")\n",
    "                with open(log_file, \"a\") as f:\n",
    "                    f.write(\n",
    "                        f\"\\nNode {cid} - Round {r+1}: Train Accuracy: {train_acc}%, Test Accuracy: {test_acc}%\"\n",
    "                    )\n",
    "                global_accuracy += test_acc\n",
    "                server.append_model(self.client_obj_list[self.clients.index(cid)].net)\n",
    "            global_model = server.aggregate()\n",
    "            # global_model = server.aggregate_prox(global_model)\n",
    "            end_time = datetime.now()\n",
    "            execution_time = end_time - start_time\n",
    "            print(\"time\", execution_time)\n",
    "            with open(log_file, \"a\") as f:\n",
    "                f.write(f\"\\n Exe FL Round Time: {execution_time}\")\n",
    "            # global_model, c = server.aggregate_scaffold(global_model, client_controls, c)\n",
    "            print(\"global acc:\", global_accuracy / self.NUMBER_OF_CLIENTS)\n",
    "            with open(log_file, \"a\") as f:\n",
    "                f.write(\n",
    "                    f\"\\nGlobal Model of {self.NUMBER_OF_CLIENTS}- Round {r+1}: Test Accuracy is: {global_accuracy/self.NUMBER_OF_CLIENTS}%\"\n",
    "                )\n",
    "            for cid in self.clients:\n",
    "                model_path = f\"models/before_aggregation/node_{cid}.pth\"\n",
    "                torch.save(\n",
    "                    self.client_obj_list[self.clients.index(cid)].net.state_dict(),\n",
    "                    model_path,\n",
    "                )\n",
    "                self.client_obj_list[self.clients.index(cid)].net = copy.deepcopy(\n",
    "                    global_model\n",
    "                )\n",
    "        self.global_model = global_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSKd2tLw3JuD"
   },
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Loading & Saving</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T01:55:34.003709Z",
     "iopub.status.busy": "2025-01-12T01:55:34.003519Z",
     "iopub.status.idle": "2025-01-12T01:55:34.019859Z",
     "shell.execute_reply": "2025-01-12T01:55:34.019127Z",
     "shell.execute_reply.started": "2025-01-12T01:55:34.003684Z"
    },
    "id": "LazN3rY5xDiZ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_torch_model(node_id):\n",
    "    model_path = f\"models/node_{node_id}.pth\"\n",
    "    model = torch.load(model_path)\n",
    "    return model\n",
    "\n",
    "\n",
    "def save_torch_model(model, node_id):\n",
    "    model_path = f\"models/node_{node_id}.pth\"\n",
    "    torch.save(model, model_path)\n",
    "\n",
    "\n",
    "def save_model_param(model, node_id, round_number):\n",
    "    model_path = f\"models/node_{node_id}_round_{round_number}.pth\"\n",
    "    torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sq920RVv3JuD"
   },
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Non-IID Distribution</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T01:55:34.021016Z",
     "iopub.status.busy": "2025-01-12T01:55:34.020791Z",
     "iopub.status.idle": "2025-01-12T01:55:34.088120Z",
     "shell.execute_reply": "2025-01-12T01:55:34.087297Z",
     "shell.execute_reply.started": "2025-01-12T01:55:34.020987Z"
    },
    "id": "eGjDwC9x3JuD",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig()\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "IMG_EXTENSIONS = (\n",
    "    \".jpg\",\n",
    "    \".jpeg\",\n",
    "    \".png\",\n",
    "    \".ppm\",\n",
    "    \".bmp\",\n",
    "    \".pgm\",\n",
    "    \".tif\",\n",
    "    \".tiff\",\n",
    "    \".webp\",\n",
    ")\n",
    "\n",
    "\n",
    "def mkdirs(dirpath):\n",
    "    try:\n",
    "        os.makedirs(dirpath)\n",
    "    except Exception as _:\n",
    "        pass\n",
    "\n",
    "\n",
    "def accimage_loader(path):\n",
    "    import accimage\n",
    "\n",
    "    try:\n",
    "        return accimage.Image(path)\n",
    "    except IOError:\n",
    "        return pil_loader(path)\n",
    "\n",
    "\n",
    "def pil_loader(path):\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, \"rb\") as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert(\"RGB\")\n",
    "\n",
    "\n",
    "def default_loader(path):\n",
    "    from torchvision import get_image_backend\n",
    "\n",
    "    if get_image_backend() == \"accimage\":\n",
    "        return accimage_loader(path)\n",
    "    else:\n",
    "        return pil_loader(path)\n",
    "\n",
    "\n",
    "class CustomTensorDataset(data.TensorDataset):\n",
    "    def __getitem__(self, index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors) + (index,)\n",
    "\n",
    "\n",
    "class MNIST_truncated(data.Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        dataidxs=None,\n",
    "        train=True,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "        download=False,\n",
    "    ):\n",
    "\n",
    "        self.root = root\n",
    "        self.dataidxs = dataidxs\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.download = download\n",
    "\n",
    "        self.data, self.target = self.__build_truncated_dataset__()\n",
    "\n",
    "    def __build_truncated_dataset__(self):\n",
    "\n",
    "        mnist_dataobj = MNIST(\n",
    "            self.root, self.train, self.transform, self.target_transform, self.download\n",
    "        )\n",
    "\n",
    "        data = mnist_dataobj.data\n",
    "        target = mnist_dataobj.targets\n",
    "\n",
    "        if self.dataidxs is not None:\n",
    "            data = data[self.dataidxs]\n",
    "            target = target[self.dataidxs]\n",
    "\n",
    "        return data, target\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.target[index]\n",
    "\n",
    "        img = Image.fromarray(img.numpy(), mode=\"L\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "class FashionMNIST_truncated(data.Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        dataidxs=None,\n",
    "        train=True,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "        download=False,\n",
    "    ):\n",
    "\n",
    "        self.root = root\n",
    "        self.dataidxs = dataidxs\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.download = download\n",
    "\n",
    "        self.data, self.target = self.__build_truncated_dataset__()\n",
    "\n",
    "    def __build_truncated_dataset__(self):\n",
    "\n",
    "        mnist_dataobj = FashionMNIST(\n",
    "            self.root, self.train, self.transform, self.target_transform, self.download\n",
    "        )\n",
    "\n",
    "        data = mnist_dataobj.data\n",
    "        target = mnist_dataobj.targets\n",
    "\n",
    "        if self.dataidxs is not None:\n",
    "            data = data[self.dataidxs]\n",
    "            target = target[self.dataidxs]\n",
    "\n",
    "        return data, target\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.target[index]\n",
    "\n",
    "        img = Image.fromarray(img.numpy(), mode=\"L\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "class SVHN_custom(data.Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        dataidxs=None,\n",
    "        train=True,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "        download=False,\n",
    "    ):\n",
    "\n",
    "        self.root = root\n",
    "        self.dataidxs = dataidxs\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.download = download\n",
    "\n",
    "        self.data, self.target = self.__build_truncated_dataset__()\n",
    "\n",
    "    def __build_truncated_dataset__(self):\n",
    "        if self.train is True:\n",
    "\n",
    "            svhn_dataobj = SVHN(\n",
    "                self.root, \"train\", self.transform, self.target_transform, self.download\n",
    "            )\n",
    "            data = svhn_dataobj.data\n",
    "            target = svhn_dataobj.labels\n",
    "        else:\n",
    "            svhn_dataobj = SVHN(\n",
    "                self.root, \"test\", self.transform, self.target_transform, self.download\n",
    "            )\n",
    "            data = svhn_dataobj.data\n",
    "            target = svhn_dataobj.labels\n",
    "\n",
    "        if self.dataidxs is not None:\n",
    "            data = data[self.dataidxs]\n",
    "            target = target[self.dataidxs]\n",
    "        return data, target\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.target[index]\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(np.transpose(img, (1, 2, 0)))\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "# torchvision CelebA\n",
    "class CelebA_custom(VisionDataset):\n",
    "    \"\"\"`Large-scale CelebFaces Attributes (CelebA) Dataset <http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html>`_ Dataset.\n",
    "\n",
    "    Args:\n",
    "        root (string): Root directory where images are downloaded to.\n",
    "        split (string): One of {'train', 'valid', 'test', 'all'}.\n",
    "            Accordingly dataset is selected.\n",
    "        target_type (string or list, optional): Type of target to use, ``attr``, ``identity``, ``bbox``,\n",
    "            or ``landmarks``. Can also be a list to output a tuple with all specified target types.\n",
    "            The targets represent:\n",
    "                ``attr`` (np.array shape=(40,) dtype=int): binary (0, 1) labels for attributes\n",
    "                ``identity`` (int): label for each person (data points with the same identity are the same person)\n",
    "                ``bbox`` (np.array shape=(4,) dtype=int): bounding box (x, y, width, height)\n",
    "                ``landmarks`` (np.array shape=(10,) dtype=int): landmark points (lefteye_x, lefteye_y, righteye_x,\n",
    "                    righteye_y, nose_x, nose_y, leftmouth_x, leftmouth_y, rightmouth_x, rightmouth_y)\n",
    "            Defaults to ``attr``. If empty, ``None`` will be returned as target.\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.ToTensor``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        download (bool, optional): If true, downloads the dataset from the internet and\n",
    "            puts it in root directory. If dataset is already downloaded, it is not\n",
    "            downloaded again.\n",
    "    \"\"\"\n",
    "\n",
    "    base_folder = \"celeba\"\n",
    "    # There currently does not appear to be a easy way to extract 7z in python (without introducing additional\n",
    "    # dependencies). The \"in-the-wild\" (not aligned+cropped) images are only in 7z, so they are not available\n",
    "    # right now.\n",
    "    file_list = [\n",
    "        # File ID                         MD5 Hash                            Filename\n",
    "        (\n",
    "            \"0B7EVK8r0v71pZjFTYXZWM3FlRnM\",\n",
    "            \"00d2c5bc6d35e252742224ab0c1e8fcb\",\n",
    "            \"img_align_celeba.zip\",\n",
    "        ),\n",
    "        # (\"0B7EVK8r0v71pbWNEUjJKdDQ3dGc\", \"b6cd7e93bc7a96c2dc33f819aa3ac651\", \"img_align_celeba_png.7z\"),\n",
    "        # (\"0B7EVK8r0v71peklHb0pGdDl6R28\", \"b6cd7e93bc7a96c2dc33f819aa3ac651\", \"img_celeba.7z\"),\n",
    "        (\n",
    "            \"0B7EVK8r0v71pblRyaVFSWGxPY0U\",\n",
    "            \"75e246fa4810816ffd6ee81facbd244c\",\n",
    "            \"list_attr_celeba.txt\",\n",
    "        ),\n",
    "        (\n",
    "            \"1_ee_0u7vcNLOfNLegJRHmolfH5ICW-XS\",\n",
    "            \"32bd1bd63d3c78cd57e08160ec5ed1e2\",\n",
    "            \"identity_CelebA.txt\",\n",
    "        ),\n",
    "        (\n",
    "            \"0B7EVK8r0v71pbThiMVRxWXZ4dU0\",\n",
    "            \"00566efa6fedff7a56946cd1c10f1c16\",\n",
    "            \"list_bbox_celeba.txt\",\n",
    "        ),\n",
    "        (\n",
    "            \"0B7EVK8r0v71pd0FJY3Blby1HUTQ\",\n",
    "            \"cc24ecafdb5b50baae59b03474781f8c\",\n",
    "            \"list_landmarks_align_celeba.txt\",\n",
    "        ),\n",
    "        # (\"0B7EVK8r0v71pTzJIdlJWdHczRlU\", \"063ee6ddb681f96bc9ca28c6febb9d1a\", \"list_landmarks_celeba.txt\"),\n",
    "        (\n",
    "            \"0B7EVK8r0v71pY0NSMzRuSXJEVkk\",\n",
    "            \"d32c9cbf5e040fd4025c592c306e6668\",\n",
    "            \"list_eval_partition.txt\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        dataidxs=None,\n",
    "        split=\"train\",\n",
    "        target_type=\"attr\",\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "        download=False,\n",
    "    ):\n",
    "        import pandas\n",
    "\n",
    "        super(CelebA_custom, self).__init__(\n",
    "            root, transform=transform, target_transform=target_transform\n",
    "        )\n",
    "        self.split = split\n",
    "        if isinstance(target_type, list):\n",
    "            self.target_type = target_type\n",
    "        else:\n",
    "            self.target_type = [target_type]\n",
    "\n",
    "        if not self.target_type and self.target_transform is not None:\n",
    "            raise RuntimeError(\"target_transform is specified but target_type is empty\")\n",
    "\n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        if not self._check_integrity():\n",
    "            raise RuntimeError(\n",
    "                \"Dataset not found or corrupted.\"\n",
    "                + \" You can use download=True to download it\"\n",
    "            )\n",
    "\n",
    "        split_map = {\n",
    "            \"train\": 0,\n",
    "            \"valid\": 1,\n",
    "            \"test\": 2,\n",
    "            \"all\": None,\n",
    "        }\n",
    "        split = split_map[split.lower()]\n",
    "\n",
    "        fn = partial(os.path.join, self.root, self.base_folder)\n",
    "        splits = pandas.read_csv(\n",
    "            fn(\"list_eval_partition.txt\"),\n",
    "            delim_whitespace=True,\n",
    "            header=None,\n",
    "            index_col=0,\n",
    "        )\n",
    "        identity = pandas.read_csv(\n",
    "            fn(\"identity_CelebA.txt\"), delim_whitespace=True, header=None, index_col=0\n",
    "        )\n",
    "        bbox = pandas.read_csv(\n",
    "            fn(\"list_bbox_celeba.txt\"), delim_whitespace=True, header=1, index_col=0\n",
    "        )\n",
    "        landmarks_align = pandas.read_csv(\n",
    "            fn(\"list_landmarks_align_celeba.txt\"), delim_whitespace=True, header=1\n",
    "        )\n",
    "        attr = pandas.read_csv(\n",
    "            fn(\"list_attr_celeba.txt\"), delim_whitespace=True, header=1\n",
    "        )\n",
    "\n",
    "        mask = slice(None) if split is None else (splits[1] == split)\n",
    "\n",
    "        self.filename = splits[mask].index.values\n",
    "        self.identity = torch.as_tensor(identity[mask].values)\n",
    "        self.bbox = torch.as_tensor(bbox[mask].values)\n",
    "        self.landmarks_align = torch.as_tensor(landmarks_align[mask].values)\n",
    "        self.attr = torch.as_tensor(attr[mask].values)\n",
    "        self.attr = (self.attr + 1) // 2  # map from {-1, 1} to {0, 1}\n",
    "        self.attr_names = list(attr.columns)\n",
    "        self.gender_index = self.attr_names.index(\"Male\")\n",
    "        self.dataidxs = dataidxs\n",
    "        if self.dataidxs is None:\n",
    "            self.target = self.attr[\n",
    "                :, self.gender_index : self.gender_index + 1\n",
    "            ].reshape(-1)\n",
    "        else:\n",
    "            self.target = self.attr[\n",
    "                self.dataidxs, self.gender_index : self.gender_index + 1\n",
    "            ].reshape(-1)\n",
    "\n",
    "    def _check_integrity(self):\n",
    "        for _, md5, filename in self.file_list:\n",
    "            fpath = os.path.join(self.root, self.base_folder, filename)\n",
    "            _, ext = os.path.splitext(filename)\n",
    "            # Allow original archive to be deleted (zip and 7z)\n",
    "            # Only need the extracted images\n",
    "            if ext not in [\".zip\", \".7z\"] and not check_integrity(fpath, md5):\n",
    "                return False\n",
    "\n",
    "        # Should check a hash of the images\n",
    "        return os.path.isdir(\n",
    "            os.path.join(self.root, self.base_folder, \"img_align_celeba\")\n",
    "        )\n",
    "\n",
    "    def download(self):\n",
    "        import zipfile\n",
    "\n",
    "        if self._check_integrity():\n",
    "            print(\"Files already downloaded and verified\")\n",
    "            return\n",
    "\n",
    "        for file_id, md5, filename in self.file_list:\n",
    "            download_file_from_google_drive(\n",
    "                file_id, os.path.join(self.root, self.base_folder), filename, md5\n",
    "            )\n",
    "\n",
    "        with zipfile.ZipFile(\n",
    "            os.path.join(self.root, self.base_folder, \"img_align_celeba.zip\"), \"r\"\n",
    "        ) as f:\n",
    "            f.extractall(os.path.join(self.root, self.base_folder))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.dataidxs is None:\n",
    "            X = PIL.Image.open(\n",
    "                os.path.join(\n",
    "                    self.root,\n",
    "                    self.base_folder,\n",
    "                    \"img_align_celeba\",\n",
    "                    self.filename[index],\n",
    "                )\n",
    "            )\n",
    "\n",
    "            target = []\n",
    "            for t in self.target_type:\n",
    "                if t == \"attr\":\n",
    "                    target.append(self.attr[index, self.gender_index])\n",
    "                elif t == \"identity\":\n",
    "                    target.append(self.identity[index, 0])\n",
    "                elif t == \"bbox\":\n",
    "                    target.append(self.bbox[index, :])\n",
    "                elif t == \"landmarks\":\n",
    "                    target.append(self.landmarks_align[index, :])\n",
    "                else:\n",
    "                    # TODO: refactor with utils.verify_str_arg\n",
    "                    raise ValueError('Target type \"{}\" is not recognized.'.format(t))\n",
    "        else:\n",
    "            X = PIL.Image.open(\n",
    "                os.path.join(\n",
    "                    self.root,\n",
    "                    self.base_folder,\n",
    "                    \"img_align_celeba\",\n",
    "                    self.filename[self.dataidxs[index]],\n",
    "                )\n",
    "            )\n",
    "\n",
    "            target = []\n",
    "            for t in self.target_type:\n",
    "                if t == \"attr\":\n",
    "                    target.append(self.attr[self.dataidxs[index], self.gender_index])\n",
    "                elif t == \"identity\":\n",
    "                    target.append(self.identity[self.dataidxs[index], 0])\n",
    "                elif t == \"bbox\":\n",
    "                    target.append(self.bbox[self.dataidxs[index], :])\n",
    "                elif t == \"landmarks\":\n",
    "                    target.append(self.landmarks_align[self.dataidxs[index], :])\n",
    "                else:\n",
    "                    # TODO: refactor with utils.verify_str_arg\n",
    "                    raise ValueError('Target type \"{}\" is not recognized.'.format(t))\n",
    "\n",
    "        if self.transform is not None:\n",
    "            X = self.transform(X)\n",
    "        # print(\"target[0]:\", target[0])\n",
    "        if target:\n",
    "            target = tuple(target) if len(target) > 1 else target[0]\n",
    "\n",
    "            if self.target_transform is not None:\n",
    "                target = self.target_transform(target)\n",
    "        else:\n",
    "            target = None\n",
    "        # print(\"celeba target:\", target)\n",
    "        return X, target\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.dataidxs is None:\n",
    "            return len(self.attr)\n",
    "        else:\n",
    "            return len(self.dataidxs)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        lines = [\"Target type: {target_type}\", \"Split: {split}\"]\n",
    "        return \"\\n\".join(lines).format(**self.__dict__)\n",
    "\n",
    "\n",
    "class STL10_truncated(data.Dataset):\n",
    "    def __init__(self, root, dataidxs=None, split=\"train\", transform=None, target_transform=None, download=False):\n",
    "        \"\"\"\n",
    "        Custom STL10 dataset with support for data indexing.\n",
    "        Args:\n",
    "            root (str): Dataset root directory.\n",
    "            dataidxs (list, optional): Indices for data partitioning. Defaults to None.\n",
    "            split (str, optional): Dataset split ('train', 'test', 'unlabeled'). Defaults to 'train'.\n",
    "            transform (callable, optional): Transformations for the input data. Defaults to None.\n",
    "            target_transform (callable, optional): Transformations for the target labels. Defaults to None.\n",
    "            download (bool, optional): Whether to download the dataset. Defaults to False.\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.dataidxs = dataidxs\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.download = download\n",
    "        self.data, self.target = self.__build_truncated_dataset__()\n",
    "\n",
    "    def __build_truncated_dataset__(self):\n",
    "        stl10_dataobj = STL10(\n",
    "            self.root, split=self.split, transform=self.transform, target_transform=self.target_transform, download=self.download\n",
    "        )\n",
    "        data = stl10_dataobj.data\n",
    "        target = np.array(stl10_dataobj.labels)\n",
    "\n",
    "        if self.dataidxs is not None:\n",
    "            data = data[self.dataidxs]\n",
    "            target = target[self.dataidxs]\n",
    "\n",
    "        return data, target\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is the class index.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.target[index]\n",
    "\n",
    "        # Ensure the image has the correct shape and dtype for PIL\n",
    "        img = np.transpose(img, (1, 2, 0))  # Convert from (C, H, W) to (H, W, C)\n",
    "        img = img.astype(np.uint8)          # Ensure dtype is uint8 for PIL compatibility\n",
    "        img = Image.fromarray(img)          # Convert to PIL Image\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "class CIFAR10_truncated(data.Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        dataidxs=None,\n",
    "        train=True,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "        download=False,\n",
    "    ):\n",
    "\n",
    "        self.root = root\n",
    "        self.dataidxs = dataidxs\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.download = download\n",
    "\n",
    "        self.data, self.target = self.__build_truncated_dataset__()\n",
    "\n",
    "    def __build_truncated_dataset__(self):\n",
    "\n",
    "        cifar_dataobj = CIFAR10(\n",
    "            self.root, self.train, self.transform, self.target_transform, self.download\n",
    "        )\n",
    "\n",
    "        data = cifar_dataobj.data\n",
    "        target = np.array(cifar_dataobj.targets)\n",
    "\n",
    "        if self.dataidxs is not None:\n",
    "            data = data[self.dataidxs]\n",
    "            target = target[self.dataidxs]\n",
    "\n",
    "        return data, target\n",
    "\n",
    "    def truncate_channel(self, index):\n",
    "        for i in range(index.shape[0]):\n",
    "            gs_index = index[i]\n",
    "            self.data[gs_index, :, :, 1] = 0.0\n",
    "            self.data[gs_index, :, :, 2] = 0.0\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.target[index]\n",
    "\n",
    "        # print(\"cifar10 img:\", img)\n",
    "        # print(\"cifar10 target:\", target)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "def gen_bar_updater() -> Callable[[int, int, int], None]:\n",
    "    pbar = tqdm(total=None)\n",
    "\n",
    "    def bar_update(count, block_size, total_size):\n",
    "        if pbar.total is None and total_size:\n",
    "            pbar.total = total_size\n",
    "        progress_bytes = count * block_size\n",
    "        pbar.update(progress_bytes - pbar.n)\n",
    "\n",
    "    return bar_update\n",
    "\n",
    "\n",
    "def download_url(\n",
    "    url: str, root: str, filename: Optional[str] = None, md5: Optional[str] = None\n",
    ") -> None:\n",
    "    \"\"\"Download a file from a url and place it in root.\n",
    "    Args:\n",
    "        url (str): URL to download file from\n",
    "        root (str): Directory to place downloaded file in\n",
    "        filename (str, optional): Name to save the file under. If None, use the basename of the URL\n",
    "        md5 (str, optional): MD5 checksum of the download. If None, do not check\n",
    "    \"\"\"\n",
    "    import urllib\n",
    "\n",
    "    root = os.path.expanduser(root)\n",
    "    if not filename:\n",
    "        filename = os.path.basename(url)\n",
    "    fpath = os.path.join(root, filename)\n",
    "\n",
    "    os.makedirs(root, exist_ok=True)\n",
    "\n",
    "    # check if file is already present locally\n",
    "    if check_integrity(fpath, md5):\n",
    "        print(\"Using downloaded and verified file: \" + fpath)\n",
    "    else:  # download the file\n",
    "        try:\n",
    "            print(\"Downloading \" + url + \" to \" + fpath)\n",
    "            urllib.request.urlretrieve(url, fpath, reporthook=gen_bar_updater())\n",
    "        except (urllib.error.URLError, IOError) as e:  # type: ignore[attr-defined]\n",
    "            if url[:5] == \"https\":\n",
    "                url = url.replace(\"https:\", \"http:\")\n",
    "                print(\n",
    "                    \"Failed download. Trying https -> http instead.\"\n",
    "                    \" Downloading \" + url + \" to \" + fpath\n",
    "                )\n",
    "                urllib.request.urlretrieve(url, fpath, reporthook=gen_bar_updater())\n",
    "            else:\n",
    "                raise e\n",
    "        # check integrity of downloaded file\n",
    "        if not check_integrity(fpath, md5):\n",
    "            raise RuntimeError(\"File not found or corrupted.\")\n",
    "\n",
    "\n",
    "def _is_tarxz(filename: str) -> bool:\n",
    "    return filename.endswith(\".tar.xz\")\n",
    "\n",
    "\n",
    "def _is_tar(filename: str) -> bool:\n",
    "    return filename.endswith(\".tar\")\n",
    "\n",
    "\n",
    "def _is_targz(filename: str) -> bool:\n",
    "    return filename.endswith(\".tar.gz\")\n",
    "\n",
    "\n",
    "def _is_tgz(filename: str) -> bool:\n",
    "    return filename.endswith(\".tgz\")\n",
    "\n",
    "\n",
    "def _is_gzip(filename: str) -> bool:\n",
    "    return filename.endswith(\".gz\") and not filename.endswith(\".tar.gz\")\n",
    "\n",
    "\n",
    "def _is_zip(filename: str) -> bool:\n",
    "    return filename.endswith(\".zip\")\n",
    "\n",
    "\n",
    "def extract_archive(\n",
    "    from_path: str, to_path: Optional[str] = None, remove_finished: bool = False\n",
    ") -> None:\n",
    "    if to_path is None:\n",
    "        to_path = os.path.dirname(from_path)\n",
    "\n",
    "    if _is_tar(from_path):\n",
    "        with tarfile.open(from_path, \"r\") as tar:\n",
    "\n",
    "            def is_within_directory(directory, target):\n",
    "\n",
    "                abs_directory = os.path.abspath(directory)\n",
    "                abs_target = os.path.abspath(target)\n",
    "\n",
    "                prefix = os.path.commonprefix([abs_directory, abs_target])\n",
    "\n",
    "                return prefix == abs_directory\n",
    "\n",
    "            def safe_extract(tar, path=\".\", members=None, *, numeric_owner=False):\n",
    "\n",
    "                for member in tar.getmembers():\n",
    "                    member_path = os.path.join(path, member.name)\n",
    "                    if not is_within_directory(path, member_path):\n",
    "                        raise Exception(\"Attempted Path Traversal in Tar File\")\n",
    "\n",
    "                tar.extractall(path, members, numeric_owner=numeric_owner)\n",
    "\n",
    "            safe_extract(tar, path=to_path)\n",
    "    elif _is_targz(from_path) or _is_tgz(from_path):\n",
    "        with tarfile.open(from_path, \"r:gz\") as tar:\n",
    "\n",
    "            def is_within_directory(directory, target):\n",
    "\n",
    "                abs_directory = os.path.abspath(directory)\n",
    "                abs_target = os.path.abspath(target)\n",
    "\n",
    "                prefix = os.path.commonprefix([abs_directory, abs_target])\n",
    "\n",
    "                return prefix == abs_directory\n",
    "\n",
    "            def safe_extract(tar, path=\".\", members=None, *, numeric_owner=False):\n",
    "\n",
    "                for member in tar.getmembers():\n",
    "                    member_path = os.path.join(path, member.name)\n",
    "                    if not is_within_directory(path, member_path):\n",
    "                        raise Exception(\"Attempted Path Traversal in Tar File\")\n",
    "\n",
    "                tar.extractall(path, members, numeric_owner=numeric_owner)\n",
    "\n",
    "            safe_extract(tar, path=to_path)\n",
    "    elif _is_tarxz(from_path):\n",
    "        with tarfile.open(from_path, \"r:xz\") as tar:\n",
    "\n",
    "            def is_within_directory(directory, target):\n",
    "\n",
    "                abs_directory = os.path.abspath(directory)\n",
    "                abs_target = os.path.abspath(target)\n",
    "\n",
    "                prefix = os.path.commonprefix([abs_directory, abs_target])\n",
    "\n",
    "                return prefix == abs_directory\n",
    "\n",
    "            def safe_extract(tar, path=\".\", members=None, *, numeric_owner=False):\n",
    "\n",
    "                for member in tar.getmembers():\n",
    "                    member_path = os.path.join(path, member.name)\n",
    "                    if not is_within_directory(path, member_path):\n",
    "                        raise Exception(\"Attempted Path Traversal in Tar File\")\n",
    "\n",
    "                tar.extractall(path, members, numeric_owner=numeric_owner)\n",
    "\n",
    "            safe_extract(tar, path=to_path)\n",
    "    elif _is_gzip(from_path):\n",
    "        to_path = os.path.join(\n",
    "            to_path, os.path.splitext(os.path.basename(from_path))[0]\n",
    "        )\n",
    "        with open(to_path, \"wb\") as out_f, gzip.GzipFile(from_path) as zip_f:\n",
    "            out_f.write(zip_f.read())\n",
    "    elif _is_zip(from_path):\n",
    "        with zipfile.ZipFile(from_path, \"r\") as z:\n",
    "            z.extractall(to_path)\n",
    "    else:\n",
    "        raise ValueError(\"Extraction of {} not supported\".format(from_path))\n",
    "\n",
    "    if remove_finished:\n",
    "        os.remove(from_path)\n",
    "\n",
    "\n",
    "def download_and_extract_archive(\n",
    "    url: str,\n",
    "    download_root: str,\n",
    "    extract_root: Optional[str] = None,\n",
    "    filename: Optional[str] = None,\n",
    "    md5: Optional[str] = None,\n",
    "    remove_finished: bool = False,\n",
    ") -> None:\n",
    "    download_root = os.path.expanduser(download_root)\n",
    "    if extract_root is None:\n",
    "        extract_root = download_root\n",
    "    if not filename:\n",
    "        filename = os.path.basename(url)\n",
    "\n",
    "    download_url(url, download_root, filename, md5)\n",
    "\n",
    "    archive = os.path.join(download_root, filename)\n",
    "    print(\"Extracting {} to {}\".format(archive, extract_root))\n",
    "    extract_archive(archive, extract_root, remove_finished)\n",
    "\n",
    "\n",
    "class FEMNIST(MNIST):\n",
    "    \"\"\"\n",
    "    This dataset is derived from the Leaf repository\n",
    "    (https://github.com/TalwalkarLab/leaf) pre-processing of the Extended MNIST\n",
    "    dataset, grouping examples by writer. Details about Leaf were published in\n",
    "    \"LEAF: A Benchmark for Federated Settings\" https://arxiv.org/abs/1812.01097.\n",
    "    \"\"\"\n",
    "\n",
    "    resources = [\n",
    "        (\n",
    "            \"https://raw.githubusercontent.com/tao-shen/FEMNIST_pytorch/master/femnist.tar.gz\",\n",
    "            \"59c65cec646fc57fe92d27d83afdf0ed\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        dataidxs=None,\n",
    "        train=True,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "        download=False,\n",
    "    ):\n",
    "        super(MNIST, self).__init__(\n",
    "            root, transform=transform, target_transform=target_transform\n",
    "        )\n",
    "        self.train = train\n",
    "        self.dataidxs = dataidxs\n",
    "\n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        if not self._check_exists():\n",
    "            raise RuntimeError(\n",
    "                \"Dataset not found.\" + \" You can use download=True to download it\"\n",
    "            )\n",
    "        if self.train:\n",
    "            data_file = self.training_file\n",
    "        else:\n",
    "            data_file = self.test_file\n",
    "\n",
    "        self.data, self.targets, self.users_index = torch.load(\n",
    "            os.path.join(self.processed_folder, data_file)\n",
    "        )\n",
    "\n",
    "        if self.dataidxs is not None:\n",
    "            self.data = self.data[self.dataidxs]\n",
    "            self.targets = self.targets[self.dataidxs]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.data[index], int(self.targets[index])\n",
    "        img = Image.fromarray(img.numpy(), mode=\"F\")\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        return img, target\n",
    "\n",
    "    def download(self):\n",
    "        \"\"\"Download the FEMNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
    "        import shutil\n",
    "\n",
    "        if self._check_exists():\n",
    "            return\n",
    "\n",
    "        mkdirs(self.raw_folder)\n",
    "        mkdirs(self.processed_folder)\n",
    "\n",
    "        # download files\n",
    "        for url, md5 in self.resources:\n",
    "            filename = url.rpartition(\"/\")[2]\n",
    "            download_and_extract_archive(\n",
    "                url, download_root=self.raw_folder, filename=filename, md5=md5\n",
    "            )\n",
    "\n",
    "        # process and save as torch files\n",
    "        print(\"Processing...\")\n",
    "        shutil.move(\n",
    "            os.path.join(self.raw_folder, self.training_file), self.processed_folder\n",
    "        )\n",
    "        shutil.move(\n",
    "            os.path.join(self.raw_folder, self.test_file), self.processed_folder\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _check_exists(self) -> bool:\n",
    "        return all(\n",
    "            check_integrity(\n",
    "                os.path.join(\n",
    "                    self.raw_folder,\n",
    "                    os.path.splitext(os.path.basename(url))[0]\n",
    "                    + os.path.splitext(os.path.basename(url))[1],\n",
    "                )\n",
    "            )\n",
    "            for url, _ in self.resources\n",
    "        )\n",
    "\n",
    "\n",
    "class Generated(MNIST):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        dataidxs=None,\n",
    "        train=True,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "        download=False,\n",
    "    ):\n",
    "        super(MNIST, self).__init__(\n",
    "            root, transform=transform, target_transform=target_transform\n",
    "        )\n",
    "        self.train = train\n",
    "        self.dataidxs = dataidxs\n",
    "\n",
    "        if self.train:\n",
    "            self.data = np.load(\"data/generated/X_train.npy\")\n",
    "            self.targets = np.load(\"data/generated/y_train.npy\")\n",
    "        else:\n",
    "            self.data = np.load(\"data/generated/X_test.npy\")\n",
    "            self.targets = np.load(\"data/generated/y_test.npy\")\n",
    "\n",
    "        if self.dataidxs is not None:\n",
    "            self.data = self.data[self.dataidxs]\n",
    "            self.targets = self.targets[self.dataidxs]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data, target = self.data[index], self.targets[index]\n",
    "        return data, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "class genData(MNIST):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data, target = self.data[index], self.targets[index]\n",
    "        return data, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "class CIFAR100_truncated(data.Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        dataidxs=None,\n",
    "        train=True,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "        download=False,\n",
    "    ):\n",
    "\n",
    "        self.root = root\n",
    "        self.dataidxs = dataidxs\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.download = download\n",
    "\n",
    "        self.data, self.target = self.__build_truncated_dataset__()\n",
    "\n",
    "    def __build_truncated_dataset__(self):\n",
    "\n",
    "        cifar_dataobj = CIFAR100(\n",
    "            self.root, self.train, self.transform, self.target_transform, self.download\n",
    "        )\n",
    "\n",
    "        if torchvision.__version__ == \"0.2.1\":\n",
    "            if self.train:\n",
    "                data, target = cifar_dataobj.train_data, np.array(\n",
    "                    cifar_dataobj.train_labels\n",
    "                )\n",
    "            else:\n",
    "                data, target = cifar_dataobj.test_data, np.array(\n",
    "                    cifar_dataobj.test_labels\n",
    "                )\n",
    "        else:\n",
    "            data = cifar_dataobj.data\n",
    "            target = np.array(cifar_dataobj.targets)\n",
    "\n",
    "        if self.dataidxs is not None:\n",
    "            data = data[self.dataidxs]\n",
    "            target = target[self.dataidxs]\n",
    "\n",
    "        return data, target\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.target[index]\n",
    "        img = Image.fromarray(img)\n",
    "        # print(\"cifar10 img:\", img)\n",
    "        # print(\"cifar10 target:\", target)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "class ImageFolder_custom(DatasetFolder):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        dataidxs=None,\n",
    "        train=True,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "        download=None,\n",
    "    ):\n",
    "        self.root = root\n",
    "        self.dataidxs = dataidxs\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        imagefolder_obj = ImageFolder(self.root, self.transform, self.target_transform)\n",
    "        self.loader = imagefolder_obj.loader\n",
    "        if self.dataidxs is not None:\n",
    "            self.samples = np.array(imagefolder_obj.samples)[self.dataidxs]\n",
    "        else:\n",
    "            self.samples = np.array(imagefolder_obj.samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.samples[index][0]\n",
    "        target = self.samples[index][1]\n",
    "        target = int(target)\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return sample, target\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.dataidxs is None:\n",
    "            return len(self.samples)\n",
    "        else:\n",
    "            return len(self.dataidxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T01:55:34.089328Z",
     "iopub.status.busy": "2025-01-12T01:55:34.089036Z",
     "iopub.status.idle": "2025-01-12T01:55:34.135474Z",
     "shell.execute_reply": "2025-01-12T01:55:34.134693Z",
     "shell.execute_reply.started": "2025-01-12T01:55:34.089307Z"
    },
    "id": "27nyJr8n3JuE",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def mkdirs(dirpath):\n",
    "    try:\n",
    "        os.makedirs(dirpath)\n",
    "    except Exception as _:\n",
    "        pass\n",
    "\n",
    "\n",
    "def load_mnist_data(datadir):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    mnist_train_ds = MNIST_truncated(\n",
    "        datadir, train=True, download=True, transform=transform\n",
    "    )\n",
    "    mnist_test_ds = MNIST_truncated(\n",
    "        datadir, train=False, download=True, transform=transform\n",
    "    )\n",
    "    X_train, y_train = mnist_train_ds.data, mnist_train_ds.target\n",
    "    X_test, y_test = mnist_test_ds.data, mnist_test_ds.target\n",
    "    X_train = X_train.data.numpy()\n",
    "    y_train = y_train.data.numpy()\n",
    "    X_test = X_test.data.numpy()\n",
    "    y_test = y_test.data.numpy()\n",
    "    return (X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "def load_fmnist_data(datadir):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    mnist_train_ds = FashionMNIST_truncated(\n",
    "        datadir, train=True, download=True, transform=transform\n",
    "    )\n",
    "    mnist_test_ds = FashionMNIST_truncated(\n",
    "        datadir, train=False, download=True, transform=transform\n",
    "    )\n",
    "    X_train, y_train = mnist_train_ds.data, mnist_train_ds.target\n",
    "    X_test, y_test = mnist_test_ds.data, mnist_test_ds.target\n",
    "    X_train = X_train.data.numpy()\n",
    "    y_train = y_train.data.numpy()\n",
    "    X_test = X_test.data.numpy()\n",
    "    y_test = y_test.data.numpy()\n",
    "    return (X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "def load_svhn_data(datadir):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((TRANSFORM_INPUT_SIZE, TRANSFORM_INPUT_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    ])\n",
    "    svhn_train_ds = SVHN_custom(datadir, train=True, download=True, transform=transform)\n",
    "    svhn_test_ds = SVHN_custom(datadir, train=False, download=True, transform=transform)\n",
    "    X_train, y_train = svhn_train_ds.data, svhn_train_ds.target\n",
    "    X_test, y_test = svhn_test_ds.data, svhn_test_ds.target\n",
    "    # X_train = X_train.data.numpy()\n",
    "    # y_train = y_train.data.numpy()\n",
    "    # X_test = X_test.data.numpy()\n",
    "    # y_test = y_test.data.numpy()\n",
    "    return (X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "def load_cifar10_data(datadir):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ]\n",
    "    )\n",
    "    cifar10_train_ds = CIFAR10_truncated(\n",
    "        datadir, train=True, download=True, transform=transform\n",
    "    )\n",
    "    cifar10_test_ds = CIFAR10_truncated(\n",
    "        datadir, train=False, download=True, transform=transform\n",
    "    )\n",
    "    X_train, y_train = cifar10_train_ds.data, cifar10_train_ds.target\n",
    "    X_test, y_test = cifar10_test_ds.data, cifar10_test_ds.target\n",
    "\n",
    "    return (X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "def load_celeba_data(datadir):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    celeba_train_ds = CelebA_custom(\n",
    "        datadir, split=\"train\", target_type=\"attr\", download=True, transform=transform\n",
    "    )\n",
    "    celeba_test_ds = CelebA_custom(\n",
    "        datadir, split=\"test\", target_type=\"attr\", download=True, transform=transform\n",
    "    )\n",
    "    gender_index = celeba_train_ds.attr_names.index(\"Male\")\n",
    "    y_train = celeba_train_ds.attr[:, gender_index : gender_index + 1].reshape(-1)\n",
    "    y_test = celeba_test_ds.attr[:, gender_index : gender_index + 1].reshape(-1)\n",
    "    # y_train = y_train.numpy()\n",
    "    # y_test = y_test.numpy()\n",
    "    return (None, y_train, None, y_test)\n",
    "\n",
    "\n",
    "def load_femnist_data(datadir):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    mnist_train_ds = FEMNIST(datadir, train=True, transform=transform, download=True)\n",
    "    mnist_test_ds = FEMNIST(datadir, train=False, transform=transform, download=True)\n",
    "    X_train, y_train, u_train = (\n",
    "        mnist_train_ds.data,\n",
    "        mnist_train_ds.targets,\n",
    "        mnist_train_ds.users_index,\n",
    "    )\n",
    "    X_test, y_test, u_test = (\n",
    "        mnist_test_ds.data,\n",
    "        mnist_test_ds.targets,\n",
    "        mnist_test_ds.users_index,\n",
    "    )\n",
    "    X_train = X_train.data.numpy()\n",
    "    y_train = y_train.data.numpy()\n",
    "    u_train = np.array(u_train)\n",
    "    X_test = X_test.data.numpy()\n",
    "    y_test = y_test.data.numpy()\n",
    "    u_test = np.array(u_test)\n",
    "    return (X_train, y_train, u_train, X_test, y_test, u_test)\n",
    "\n",
    "\n",
    "def load_cifar100_data(datadir):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    cifar100_train_ds = CIFAR100_truncated(\n",
    "        datadir, train=True, download=True, transform=transform\n",
    "    )\n",
    "    cifar100_test_ds = CIFAR100_truncated(\n",
    "        datadir, train=False, download=True, transform=transform\n",
    "    )\n",
    "    X_train, y_train = cifar100_train_ds.data, cifar100_train_ds.target\n",
    "    X_test, y_test = cifar100_test_ds.data, cifar100_test_ds.target\n",
    "    # y_train = y_train.numpy()\n",
    "    # y_test = y_test.numpy()\n",
    "    return (X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "def load_tinyimagenet_data(datadir):\n",
    "    split = \"val\"\n",
    "    TinyImageNet(datadir, split=split)\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(64, padding=4),  # Random cropping with padding\n",
    "        transforms.RandomHorizontalFlip(),  # Horizontal flip\n",
    "        transforms.RandomRotation(15),  # Random rotation\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Color jitter\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.4802, 0.4481, 0.3975], std=[0.2302, 0.2265, 0.2262]),  # Normalization\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.4802, 0.4481, 0.3975], std=[0.2302, 0.2265, 0.2262]),\n",
    "    ])\n",
    "    # transform = transforms.Compose([transforms.ToTensor()])\n",
    "    xray_train_ds = ImageFolder_custom(\n",
    "        datadir + \"tiny-imagenet-200/train/\", transform=transform_train\n",
    "    )\n",
    "    xray_test_ds = ImageFolder_custom(\n",
    "        datadir + \"tiny-imagenet-200/val/\", transform=transform_test\n",
    "    )\n",
    "    X_train, y_train = np.array([s[0] for s in xray_train_ds.samples]), np.array(\n",
    "        [int(s[1]) for s in xray_train_ds.samples]\n",
    "    )\n",
    "    X_test, y_test = np.array([s[0] for s in xray_test_ds.samples]), np.array(\n",
    "        [int(s[1]) for s in xray_test_ds.samples]\n",
    "    )\n",
    "    return (X_train, y_train, X_test, y_test)\n",
    "\n",
    "def load_stl10_data(datadir):\n",
    "    transform_train = transforms.Compose([\n",
    "    transforms.Resize((TRANSFORM_INPUT_SIZE, TRANSFORM_INPUT_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "    transforms.Resize((TRANSFORM_INPUT_SIZE, TRANSFORM_INPUT_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    stl10_train_ds = STL10_truncated(datadir, split=\"train\", transform=transform_train, download=True)\n",
    "    stl10_test_ds = STL10_truncated(datadir, split=\"test\", transform=transform_test, download=True)\n",
    "\n",
    "    X_train, y_train = stl10_train_ds.data, stl10_train_ds.target\n",
    "    X_test, y_test = stl10_test_ds.data, stl10_test_ds.target\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def record_net_data_stats(y_train, net_dataidx_map, logdir):\n",
    "    net_cls_counts = {}\n",
    "    for net_i, dataidx in net_dataidx_map.items():\n",
    "        unq, unq_cnt = np.unique(y_train[dataidx], return_counts=True)\n",
    "        tmp = {unq[i]: unq_cnt[i] for i in range(len(unq))}\n",
    "        net_cls_counts[net_i] = tmp\n",
    "    logger.info(\"Data statistics: %s\" % str(net_cls_counts))\n",
    "    return net_cls_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_data(dataset, datadir, logdir, partition, n_parties, beta=0.1):\n",
    "    test_dataidx_map = {}\n",
    "\n",
    "    # Load dataset\n",
    "    if dataset == \"mnist\":\n",
    "        X_train, y_train, X_test, y_test = load_mnist_data(datadir)\n",
    "    elif dataset == \"fmnist\":\n",
    "        X_train, y_train, X_test, y_test = load_fmnist_data(datadir)\n",
    "    elif dataset == \"cifar10\":\n",
    "        X_train, y_train, X_test, y_test = load_cifar10_data(datadir)\n",
    "    elif dataset == \"svhn\":\n",
    "        X_train, y_train, X_test, y_test = load_svhn_data(datadir)\n",
    "    elif dataset == \"celeba\":\n",
    "        X_train, y_train, X_test, y_test = load_celeba_data(datadir)\n",
    "    elif dataset == \"femnist\":\n",
    "        X_train, y_train, u_train, X_test, y_test, u_test = load_femnist_data(datadir)\n",
    "    elif dataset == \"cifar100\":\n",
    "        X_train, y_train, X_test, y_test = load_cifar100_data(datadir)\n",
    "    elif dataset == \"tinyimagenet\":\n",
    "        X_train, y_train, X_test, y_test = load_tinyimagenet_data(datadir)\n",
    "    elif dataset == \"stl10\":\n",
    "        X_train, y_train, X_test, y_test = load_stl10_data(datadir)\n",
    "    elif dataset == \"generated\":\n",
    "        # Code for generated dataset (omitted for brevity)\n",
    "        pass\n",
    "    # Add other datasets if needed\n",
    "\n",
    "    n_train = y_train.shape[0]\n",
    "\n",
    "    # Partition the data\n",
    "    if partition == \"homo\":\n",
    "        # Homogeneous data partition\n",
    "        idxs = np.random.permutation(n_train)\n",
    "        batch_idxs = np.array_split(idxs, n_parties)\n",
    "        net_dataidx_map = {i: batch_idxs[i] for i in range(n_parties)}\n",
    "\n",
    "    elif partition == \"noniid-labeldir\":\n",
    "        min_size = 0\n",
    "        min_require_size = 10  # Minimum number required for each party\n",
    "        if dataset == \"cifar100\":\n",
    "            K = 100  # Number of classes\n",
    "        else:\n",
    "            k = 10\n",
    "\n",
    "        N = y_train.shape[0]\n",
    "        net_dataidx_map = {}\n",
    "        test_dataidx_map = {}  # Make sure to initialize this\n",
    "\n",
    "        while min_size < min_require_size:\n",
    "            idx_batch = [[] for _ in range(n_parties)]\n",
    "            for k in range(K):\n",
    "                idx_k = np.where(y_train == k)[0]\n",
    "                np.random.shuffle(idx_k)\n",
    "                proportions = np.random.dirichlet(np.repeat(beta, n_parties))\n",
    "                proportions = np.array(\n",
    "                    [\n",
    "                        p * (len(idx_j) < N / n_parties)\n",
    "                        for p, idx_j in zip(proportions, idx_batch)\n",
    "                    ]\n",
    "                )\n",
    "                proportions = proportions / proportions.sum()  # Normalize\n",
    "                proportions = (np.cumsum(proportions) * len(idx_k)).astype(int)[:-1]\n",
    "                idx_batch = [\n",
    "                    idx_j + idx.tolist()\n",
    "                    for idx_j, idx in zip(idx_batch, np.split(idx_k, proportions))\n",
    "                ]\n",
    "\n",
    "            min_size = min([len(idx_j) for idx_j in idx_batch])\n",
    "\n",
    "        for j in range(n_parties):\n",
    "            np.random.shuffle(idx_batch[j])\n",
    "            net_dataidx_map[j] = idx_batch[j]\n",
    "\n",
    "            # Initialize test_dataidx_map for current party\n",
    "            test_dataidx_map[j] = []\n",
    "\n",
    "            # Gather test indices for current party based on labels in net_dataidx_map[j]\n",
    "            for k in range(K):\n",
    "                if k in y_train[net_dataidx_map[j]]:\n",
    "                    # Access test indices for class k\n",
    "                    idx_test_k = np.where(y_test == k)[0]\n",
    "                    np.random.shuffle(idx_test_k)\n",
    "\n",
    "                    # The number of sample for each party based on training set size\n",
    "                    n_samples = int(len(net_dataidx_map[j]) * len(idx_test_k) / N)\n",
    "                    test_dataidx_map[j].extend(idx_test_k[:n_samples])\n",
    "\n",
    "            test_dataidx_map[j] = np.array(test_dataidx_map[j])\n",
    "\n",
    "        # Cleanup to avoid empty concatenation error\n",
    "        for j in range(n_parties):\n",
    "            if len(test_dataidx_map[j]) == 0:\n",
    "                test_dataidx_map[j] = np.array(\n",
    "                    []\n",
    "                )  # Set to an empty array to avoid errors later\n",
    "\n",
    "    elif partition == \"noniid-fix\":\n",
    "        # Custom fixed distribution logic\n",
    "        desired_distribution = DESIRED_DISTRIBUTION\n",
    "\n",
    "        # Number of clients and classes\n",
    "        num_clients = len(desired_distribution)\n",
    "        num_classes = len(desired_distribution[0])\n",
    "\n",
    "        assert num_clients == NUMBER_OF_CLIENTS\n",
    "        assert num_classes == NUMBER_OF_CLASSES\n",
    "\n",
    "        # Initialize the data indices for each client\n",
    "        net_dataidx_map = {i: [] for i in range(num_clients)}\n",
    "\n",
    "        # Iterate over each class and assign samples to clients based on the desired distribution\n",
    "        for class_idx in range(num_classes):\n",
    "            # Get the indices of all samples belonging to the current class\n",
    "            class_indices = np.where(y_train == class_idx)[0]\n",
    "\n",
    "            # Shuffle the indices to ensure randomness\n",
    "            np.random.shuffle(class_indices)\n",
    "\n",
    "            # Assign samples to clients based on the desired distribution\n",
    "            start_idx = 0\n",
    "            for client_idx in range(num_clients):\n",
    "                num_samples = desired_distribution[client_idx][class_idx]\n",
    "                if num_samples > 0:\n",
    "                    end_idx = start_idx + num_samples\n",
    "                    net_dataidx_map[client_idx].extend(class_indices[start_idx:end_idx])\n",
    "                    start_idx = end_idx\n",
    "\n",
    "        # Initialize test_dataidx_map for each client\n",
    "        for j in range(num_clients):\n",
    "            test_dataidx_map[j] = []\n",
    "\n",
    "            # Gather test indices for current party based on labels in net_dataidx_map[j]\n",
    "            for k in range(num_classes):\n",
    "                if k in y_train[net_dataidx_map[j]]:\n",
    "                    # Access test indices for class k\n",
    "                    idx_test_k = np.where(y_test == k)[0]\n",
    "                    np.random.shuffle(idx_test_k)\n",
    "\n",
    "                    # The number of samples for each party based on training set size\n",
    "                    n_samples = int(len(net_dataidx_map[j]) * len(idx_test_k) / n_train)\n",
    "                    test_dataidx_map[j].extend(idx_test_k[:n_samples])\n",
    "\n",
    "            test_dataidx_map[j] = np.array(test_dataidx_map[j])\n",
    "\n",
    "        # Cleanup to avoid empty concatenation error\n",
    "        for j in range(num_clients):\n",
    "            if len(test_dataidx_map[j]) == 0:\n",
    "                test_dataidx_map[j] = np.array(\n",
    "                    []\n",
    "                )  # Set to an empty array to avoid errors later\n",
    "\n",
    "    elif partition.startswith(\"noniid-#label\") and partition[13:].isdigit():\n",
    "        # Existing logic for noniid-#label partitioning\n",
    "        num = int(partition[13:])\n",
    "        if dataset in (\"celeba\", \"covtype\", \"a9a\", \"rcv1\", \"SUSY\"):\n",
    "            num = 1\n",
    "            K = 2\n",
    "        else:\n",
    "            if dataset == \"cifar100\":\n",
    "                K = 100\n",
    "            elif dataset == \"tinyimagenet\":\n",
    "                K = 200\n",
    "            else:\n",
    "                K = 10\n",
    "        if num == K:\n",
    "            # IID partition\n",
    "            net_dataidx_map = {\n",
    "                i: np.ndarray(0, dtype=np.int64) for i in range(n_parties)\n",
    "            }\n",
    "            for i in range(K):\n",
    "                idx_k = np.where(y_train == i)[0]\n",
    "                np.random.shuffle(idx_k)\n",
    "                split = np.array_split(idx_k, n_parties)\n",
    "                for j in range(n_parties):\n",
    "                    net_dataidx_map[j] = np.append(net_dataidx_map[j], split[j])\n",
    "        else:\n",
    "            times = [0 for _ in range(K)]\n",
    "            contain = []\n",
    "            for i in range(n_parties):\n",
    "                current = [i % K]\n",
    "                times[i % K] += 1\n",
    "                j = 1\n",
    "                while j < num:\n",
    "                    ind = random.randint(0, K - 1)\n",
    "                    if ind not in current:\n",
    "                        j += 1\n",
    "                        current.append(ind)\n",
    "                        times[ind] += 1\n",
    "                contain.append(current)\n",
    "            net_dataidx_map = {\n",
    "                i: np.ndarray(0, dtype=np.int64) for i in range(n_parties)\n",
    "            }\n",
    "            test_dataidx_map = {\n",
    "                i: np.ndarray(0, dtype=np.int64) for i in range(n_parties)\n",
    "            }\n",
    "            for i in range(K):\n",
    "                if times[i] > 0:\n",
    "                    idx_k = np.where(y_train == i)[0]\n",
    "                    idx_t = np.where(y_test == i)[0]\n",
    "                    np.random.shuffle(idx_k)\n",
    "                    np.random.shuffle(idx_t)\n",
    "                    split = np.array_split(idx_k, times[i])\n",
    "                    splitt = np.array_split(idx_t, times[i])\n",
    "                    ids = 0\n",
    "                    for j in range(n_parties):\n",
    "                        if i in contain[j]:\n",
    "                            net_dataidx_map[j] = np.append(\n",
    "                                net_dataidx_map[j], split[ids]\n",
    "                            )\n",
    "                            test_dataidx_map[j] = np.append(\n",
    "                                test_dataidx_map[j], splitt[ids]\n",
    "                            )\n",
    "                            ids += 1\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown partition method: {partition}\")\n",
    "\n",
    "    # Record the data statistics\n",
    "    traindata_cls_counts = record_net_data_stats(y_train, net_dataidx_map, logdir)\n",
    "\n",
    "    return (\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        net_dataidx_map,\n",
    "        test_dataidx_map,\n",
    "        traindata_cls_counts,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0.0, std=1.0, net_id=None, total=0):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        self.net_id = net_id\n",
    "        self.num = int(sqrt(total))\n",
    "        if self.num * self.num < total:\n",
    "            self.num = self.num + 1\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        if self.net_id is None:\n",
    "            return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "        else:\n",
    "            tmp = torch.randn(tensor.size())\n",
    "            filt = torch.zeros(tensor.size())\n",
    "            size = int(28 / self.num)\n",
    "            row = int(self.net_id / size)\n",
    "            col = self.net_id % size\n",
    "            for i in range(size):\n",
    "                for j in range(size):\n",
    "                    filt[:, row * size + i, col * size + j] = 1\n",
    "            tmp = tmp * filt\n",
    "            return tensor + tmp * self.std + self.mean\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \"(mean={0}, std={1})\".format(\n",
    "            self.mean, self.std\n",
    "        )\n",
    "\n",
    "\n",
    "def get_dataloader(\n",
    "    dataset,\n",
    "    datadir,\n",
    "    train_bs,\n",
    "    test_bs,\n",
    "    dataidxs=None,\n",
    "    testidxs=None,\n",
    "    noise_level=0,\n",
    "    net_id=None,\n",
    "    total=0,\n",
    "):\n",
    "    if dataset in (\n",
    "        \"mnist\",\n",
    "        \"femnist\",\n",
    "        \"fmnist\",\n",
    "        \"cifar10\",\n",
    "        \"svhn\",\n",
    "        \"generated\",\n",
    "        \"covtype\",\n",
    "        \"a9a\",\n",
    "        \"rcv1\",\n",
    "        \"SUSY\",\n",
    "        \"cifar100\",\n",
    "        \"tinyimagenet\",\n",
    "        \"stl10\"\n",
    "    ):\n",
    "        if dataset == \"mnist\":\n",
    "            dl_obj = MNIST_truncated\n",
    "            transform_train = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    AddGaussianNoise(0.0, noise_level, net_id, total),\n",
    "                ]\n",
    "            )\n",
    "            transform_test = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    AddGaussianNoise(0.0, noise_level, net_id, total),\n",
    "                ]\n",
    "            )\n",
    "        elif dataset == \"femnist\":\n",
    "            dl_obj = FEMNIST\n",
    "            transform_train = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    AddGaussianNoise(0.0, noise_level, net_id, total),\n",
    "                ]\n",
    "            )\n",
    "            transform_test = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    AddGaussianNoise(0.0, noise_level, net_id, total),\n",
    "                ]\n",
    "            )\n",
    "        elif dataset == \"fmnist\":\n",
    "            dl_obj = FashionMNIST_truncated\n",
    "            transform_train = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    AddGaussianNoise(0.0, noise_level, net_id, total),\n",
    "                ]\n",
    "            )\n",
    "            transform_test = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    AddGaussianNoise(0.0, noise_level, net_id, total),\n",
    "                ]\n",
    "            )\n",
    "        elif dataset == \"svhn\":\n",
    "            dl_obj = SVHN_custom\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.Resize((TRANSFORM_INPUT_SIZE, TRANSFORM_INPUT_SIZE)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "            ])\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.Resize((TRANSFORM_INPUT_SIZE, TRANSFORM_INPUT_SIZE)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "            ])\n",
    "\n",
    "        elif dataset == \"cifar10\":\n",
    "            print(\"in cifar10\")\n",
    "            dl_obj = CIFAR10_truncated\n",
    "            transform_train = transforms.Compose(\n",
    "                [\n",
    "                    # transforms.Resize((224,224)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Lambda(\n",
    "                        lambda x: F.pad(\n",
    "                            Variable(x.unsqueeze(0), requires_grad=False),\n",
    "                            (4, 4, 4, 4),\n",
    "                            mode=\"reflect\",\n",
    "                        ).data.squeeze()\n",
    "                    ),\n",
    "                    transforms.ToPILImage(),\n",
    "                    transforms.RandomCrop(32),\n",
    "                    transforms.ToTensor(),\n",
    "                    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                    AddGaussianNoise(0.0, noise_level, net_id, total),\n",
    "                ]\n",
    "            )\n",
    "            # data prep for test set\n",
    "            transform_test = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                    AddGaussianNoise(0.0, noise_level, net_id, total),\n",
    "                ]\n",
    "            )\n",
    "        elif dataset == \"cifar100\":\n",
    "            print(\"in 100\")\n",
    "            dl_obj = CIFAR100_truncated\n",
    "            normalize = transforms.Normalize(\n",
    "                mean=[0.5070751592371323, 0.48654887331495095, 0.4409178433670343],\n",
    "                std=[0.2673342858792401, 0.2564384629170883, 0.27615047132568404],\n",
    "            )\n",
    "\n",
    "            transform_train = transforms.Compose(\n",
    "                [\n",
    "                    # transforms.ToPILImage(),\n",
    "                    transforms.RandomCrop(32, padding=4),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.RandomRotation(15),\n",
    "                    transforms.ToTensor(),\n",
    "                    normalize,\n",
    "                ]\n",
    "            )\n",
    "            # data prep for test set\n",
    "            transform_test = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "        elif dataset == \"tinyimagenet\":\n",
    "            dl_obj = ImageFolder_custom\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.RandomCrop(64, padding=4),  # Random cropping with padding\n",
    "                transforms.RandomHorizontalFlip(),  # Horizontal flip\n",
    "                transforms.RandomRotation(15),  # Random rotation\n",
    "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Color jitter\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.4802, 0.4481, 0.3975], std=[0.2302, 0.2265, 0.2262]),  # Normalization\n",
    "            ])\n",
    "\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.4802, 0.4481, 0.3975], std=[0.2302, 0.2265, 0.2262]),\n",
    "            ])\n",
    "        elif dataset == \"stl10\":\n",
    "            dl_obj = STL10_truncated\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.Resize((TRANSFORM_INPUT_SIZE, TRANSFORM_INPUT_SIZE)),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "            ])\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.Resize((TRANSFORM_INPUT_SIZE, TRANSFORM_INPUT_SIZE)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "            ])\n",
    "        else:\n",
    "            dl_obj = Generated\n",
    "            transform_train = None\n",
    "            transform_test = None\n",
    "        if dataset == \"tinyimagenet\":\n",
    "            train_ds = dl_obj(\n",
    "                datadir + \"tiny-imagenet-200/train/\",\n",
    "                dataidxs=dataidxs,\n",
    "                transform=transform_train,\n",
    "            )\n",
    "            test_ds = dl_obj(\n",
    "                datadir + \"tiny-imagenet-200/val/\",\n",
    "                dataidxs=testidxs,\n",
    "                transform=transform_test\n",
    "            )\n",
    "        elif dataset == \"stl10\":\n",
    "            train_ds = dl_obj(\n",
    "                datadir,\n",
    "                dataidxs=dataidxs,\n",
    "                split=\"train\",\n",
    "                transform=transform_train,\n",
    "                download=True\n",
    "            )\n",
    "            test_ds = dl_obj(\n",
    "                datadir,\n",
    "                dataidxs=testidxs,\n",
    "                split=\"test\",\n",
    "                transform=transform_test,\n",
    "                download=True\n",
    "            )\n",
    "        else:\n",
    "            print(\"dir\", datadir)\n",
    "            train_ds = dl_obj(\n",
    "                datadir,\n",
    "                dataidxs=dataidxs,\n",
    "                train=True,\n",
    "                transform=transform_train,\n",
    "                download=True,\n",
    "            )\n",
    "            test_ds = dl_obj(\n",
    "                datadir,\n",
    "                dataidxs=testidxs,\n",
    "                train=False,\n",
    "                transform=transform_test,\n",
    "                download=True,\n",
    "            )\n",
    "        train_dl = data.DataLoader(\n",
    "            dataset=train_ds, batch_size=train_bs, shuffle=True, drop_last=False\n",
    "        )\n",
    "        test_dl = data.DataLoader(\n",
    "            dataset=test_ds, batch_size=test_bs, shuffle=False, drop_last=False\n",
    "        )\n",
    "        print(train_ds, \"train ds\")\n",
    "    return train_dl, test_dl, train_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T01:55:34.136620Z",
     "iopub.status.busy": "2025-01-12T01:55:34.136333Z",
     "iopub.status.idle": "2025-01-12T01:55:34.153399Z",
     "shell.execute_reply": "2025-01-12T01:55:34.152636Z",
     "shell.execute_reply.started": "2025-01-12T01:55:34.136591Z"
    },
    "id": "eizGyXaA3JuE",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_loaders(NUMBER_OF_CLIENTS):\n",
    "\n",
    "    (\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        net_dataidx_map,\n",
    "        test_dataidx_map,\n",
    "        traindata_cls_counts,\n",
    "    ) = partition_data(\n",
    "        dataset=DATASET_TYPE,\n",
    "        datadir=\"./data/\",\n",
    "        logdir=\"./logs/\",\n",
    "        partition=PARTITION,\n",
    "        n_parties=10,\n",
    "    )\n",
    "    print(\"shapes\", X_train.shape, y_train.shape)\n",
    "    train_loaders = []\n",
    "    test_loaders = []\n",
    "    for client_id in range(NUMBER_OF_CLIENTS):\n",
    "\n",
    "        dataidxs = net_dataidx_map[client_id]\n",
    "        testidxs = test_dataidx_map[client_id]\n",
    "\n",
    "        train_dl_local, test_dl_local, train_ds_local, test_ds_local = get_dataloader(\n",
    "            dataset=DATASET_TYPE,\n",
    "            datadir=\"./data/\",\n",
    "            train_bs=TRAIN_BATCH_SIZE,\n",
    "            test_bs=TEST_BATCH_SIZE,\n",
    "            dataidxs=dataidxs,\n",
    "            testidxs=testidxs,\n",
    "        )\n",
    "        train_loaders.append(train_dl_local)\n",
    "        test_loaders.append(test_dl_local)\n",
    "\n",
    "    return train_loaders, test_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T01:55:34.154291Z",
     "iopub.status.busy": "2025-01-12T01:55:34.154045Z",
     "iopub.status.idle": "2025-01-12T01:55:34.166443Z",
     "shell.execute_reply": "2025-01-12T01:55:34.165775Z",
     "shell.execute_reply.started": "2025-01-12T01:55:34.154260Z"
    },
    "id": "-IvzdpYcxGZx",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_and_prepare_data():\n",
    "    train_loaders, test_loaders = get_loaders(10)\n",
    "    return train_loaders, test_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-12T01:55:34.167345Z",
     "iopub.status.busy": "2025-01-12T01:55:34.167107Z",
     "iopub.status.idle": "2025-01-12T01:57:34.302831Z",
     "shell.execute_reply": "2025-01-12T01:57:34.302034Z",
     "shell.execute_reply.started": "2025-01-12T01:55:34.167318Z"
    },
    "id": "ORJsNkg1xMY4",
    "outputId": "9769a96f-c6b6-46c1-ebf6-2eff0f483bf2",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Data statistics: {0: {np.int64(0): np.int64(6000), np.int64(2): np.int64(3000)}, 1: {np.int64(1): np.int64(1500), np.int64(9): np.int64(3000)}, 2: {np.int64(1): np.int64(1500), np.int64(2): np.int64(3000)}, 3: {np.int64(3): np.int64(3000), np.int64(4): np.int64(3000)}, 4: {np.int64(1): np.int64(1500), np.int64(4): np.int64(3000)}, 5: {np.int64(5): np.int64(6000), np.int64(7): np.int64(2000)}, 6: {np.int64(6): np.int64(3000), np.int64(7): np.int64(2000)}, 7: {np.int64(6): np.int64(3000), np.int64(7): np.int64(2000)}, 8: {np.int64(3): np.int64(3000), np.int64(8): np.int64(6000)}, 9: {np.int64(1): np.int64(1500), np.int64(9): np.int64(3000)}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes (60000, 28, 28) (60000,)\n",
      "dir ./data/\n",
      "<__main__.FashionMNIST_truncated object at 0x79e9de1f2ea0> train ds\n",
      "dir ./data/\n",
      "<__main__.FashionMNIST_truncated object at 0x79ea105b3110> train ds\n",
      "dir ./data/\n",
      "<__main__.FashionMNIST_truncated object at 0x79e9df597890> train ds\n",
      "dir ./data/\n",
      "<__main__.FashionMNIST_truncated object at 0x79e9df594bf0> train ds\n",
      "dir ./data/\n",
      "<__main__.FashionMNIST_truncated object at 0x79e9df595be0> train ds\n",
      "dir ./data/\n",
      "<__main__.FashionMNIST_truncated object at 0x79e9df597050> train ds\n",
      "dir ./data/\n",
      "<__main__.FashionMNIST_truncated object at 0x79e9df5979e0> train ds\n",
      "dir ./data/\n",
      "<__main__.FashionMNIST_truncated object at 0x79e9df595a90> train ds\n",
      "dir ./data/\n",
      "<__main__.FashionMNIST_truncated object at 0x79e9df596a20> train ds\n",
      "dir ./data/\n",
      "<__main__.FashionMNIST_truncated object at 0x79e9df596f30> train ds\n"
     ]
    }
   ],
   "source": [
    "train_loaders, test_loaders = load_and_prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CrFoxva3JuE"
   },
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Visualization</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T01:57:34.304017Z",
     "iopub.status.busy": "2025-01-12T01:57:34.303705Z",
     "iopub.status.idle": "2025-01-12T01:57:34.308180Z",
     "shell.execute_reply": "2025-01-12T01:57:34.307427Z",
     "shell.execute_reply.started": "2025-01-12T01:57:34.303984Z"
    },
    "id": "uySBR7Ez3JuE",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# class Visualizer:\n",
    "#     def __init__(self, train_loaders):\n",
    "#         self.train_loaders = train_loaders\n",
    "\n",
    "#     def count_classes(self):\n",
    "#         class_counts = []\n",
    "#         for loader in self.train_loaders:\n",
    "#             counts = np.zeros(10, dtype=int)\n",
    "#             for _, labels in loader:\n",
    "#                 for label in labels:\n",
    "#                     counts[label] += 1\n",
    "#             class_counts.append(counts)\n",
    "#         return class_counts\n",
    "\n",
    "#     def plot_class_distribution(\n",
    "#         self,\n",
    "#         DATASET_TYPE=\"Train\",\n",
    "#     ):\n",
    "#         class_counts = self.count_classes()\n",
    "#         num_classes = NUMBER_OF_CLASSES\n",
    "#         labels = [\n",
    "#             \"airplane\",\n",
    "#             \"automobile\",\n",
    "#             \"bird\",\n",
    "#             \"cat\",\n",
    "#             \"deer\",\n",
    "#             \"dog\",\n",
    "#             \"frog\",\n",
    "#             \"horse\",\n",
    "#             \"ship\",\n",
    "#             \"truck\",\n",
    "#         ]\n",
    "#         num_nodes = len(class_counts)\n",
    "#         fig, ax = plt.subplots(figsize=(10, 6))\n",
    "#         width = 0.6\n",
    "\n",
    "#         counts = np.array(class_counts)\n",
    "#         x = np.arange(num_nodes)\n",
    "\n",
    "#         colors = plt.cm.tab10.colors\n",
    "\n",
    "#         bottom = np.zeros(num_nodes)\n",
    "#         for i in range(num_classes):\n",
    "#             counts_per_class = counts[:, i]\n",
    "#             ax.bar(\n",
    "#                 x,\n",
    "#                 counts_per_class,\n",
    "#                 width,\n",
    "#                 bottom=bottom,\n",
    "#                 label=labels[i],\n",
    "#                 color=colors[i % len(colors)],\n",
    "#                 edgecolor=\"white\",\n",
    "#             )\n",
    "#             bottom += counts_per_class\n",
    "#         ax.set_xlabel(\"Nodes\")\n",
    "#         ax.set_ylabel(\"Number of Samples\")\n",
    "#         ax.set_title(f\"Distribution of {DATASET_TYPE} Classes Across Different Nodes\")\n",
    "#         ax.set_xticks(x)\n",
    "#         ax.set_xticklabels([f\"{i+1}\" for i in range(num_nodes)], rotation=0)\n",
    "#         ax.legend(\n",
    "#             title=\"Classes\",\n",
    "#             bbox_to_anchor=(1.05, 1),\n",
    "#             loc=\"upper left\",\n",
    "#             borderaxespad=0.0,\n",
    "#             frameon=False,\n",
    "#         )\n",
    "#         plt.tight_layout()\n",
    "#         plt.subplots_adjust(right=0.75)\n",
    "\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T01:57:34.309162Z",
     "iopub.status.busy": "2025-01-12T01:57:34.308924Z",
     "iopub.status.idle": "2025-01-12T01:57:34.326263Z",
     "shell.execute_reply": "2025-01-12T01:57:34.325451Z",
     "shell.execute_reply.started": "2025-01-12T01:57:34.309143Z"
    },
    "id": "KS4EpcfYxOK6",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Visualizer(train_loaders).plot_class_distribution()\n",
    "# Visualizer(test_loaders).plot_class_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-12T01:57:34.329557Z",
     "iopub.status.busy": "2025-01-12T01:57:34.329338Z",
     "iopub.status.idle": "2025-01-12T01:57:34.339954Z",
     "shell.execute_reply": "2025-01-12T01:57:34.339325Z",
     "shell.execute_reply.started": "2025-01-12T01:57:34.329538Z"
    },
    "id": "TigBlX7z3JuE",
    "outputId": "31693f42-3347-4e01-e61c-3ab36ebfbdfb",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# def calculate_label_distribution(dataloader):\n",
    "#     label_counts = np.zeros(NUMBER_OF_CLASSES)\n",
    "#     for _, labels in dataloader:\n",
    "#         for label in labels.numpy():\n",
    "#             label_counts[label] += 1\n",
    "            \n",
    "#     print(f\"Label distribution is: {label_counts}\")\n",
    "#     return label_counts\n",
    "\n",
    "# def plot_client_distributions(distributions):\n",
    "#     num_clients = len(distributions)\n",
    "#     cols = 3  # Number of columns for subplots\n",
    "#     rows = (num_clients + cols - 1) // cols  # Calculate number of rows needed\n",
    "\n",
    "#     fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))\n",
    "#     axes = axes.flatten()  # Flatten the axes array for easy indexing\n",
    "\n",
    "#     for i, label_counts in enumerate(distributions):\n",
    "#         axes[i].bar(range(NUMBER_OF_CLASSES), label_counts, color='skyblue')\n",
    "#         axes[i].set_xlabel('Class Labels')\n",
    "#         axes[i].set_ylabel('Number of Samples')\n",
    "#         axes[i].set_title(f'Client {i}')\n",
    "#         axes[i].set_xticks(range(NUMBER_OF_CLASSES))\n",
    "#         axes[i].set_xticklabels([f'Class {j}' for j in range(NUMBER_OF_CLASSES)])\n",
    "#         axes[i].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "#     # Hide any unused subplots\n",
    "#     for j in range(i + 1, len(axes)):\n",
    "#         fig.delaxes(axes[j])\n",
    "\n",
    "#     plt.suptitle('Label Distribution for Each Client')\n",
    "#     plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust layout to make room for the title\n",
    "#     plt.show()\n",
    "\n",
    "# def compute_similarity_matrix(distributions):\n",
    "#     similarity_matrix = cosine_similarity(distributions)\n",
    "#     return similarity_matrix\n",
    "\n",
    "# def cluster_clients(similarity_matrix):\n",
    "#     # clustering = AffinityPropagation(affinity='precomputed', random_state=42)\n",
    "#     clustering = AffinityPropagation(affinity='precomputed', random_state=0)\n",
    "#     clustering.fit(similarity_matrix)\n",
    "#     return clustering.labels_\n",
    "\n",
    "# def group_clients_by_cluster(labels):\n",
    "#     clusters = {}\n",
    "#     for client_id, cluster_id in enumerate(labels):\n",
    "#         if cluster_id not in clusters:\n",
    "#             clusters[cluster_id] = []\n",
    "#         clusters[cluster_id].append(client_id)\n",
    "#     return clusters\n",
    "\n",
    "# def save_similarity_matrix_to_csv(similarity_matrix, filename=\"similarity_matrix.csv\"):\n",
    "#     with open(filename, mode='w', newline='') as file:\n",
    "#         writer = csv.writer(file)\n",
    "#         writer.writerow([\"Client\"] + [f\"Client_{i}\" for i in range(len(similarity_matrix))])\n",
    "#         for i, row in enumerate(similarity_matrix):\n",
    "#             writer.writerow([f\"Client_{i}\"] + row.tolist())\n",
    "#     print(f\"Similarity matrix saved to {filename}\")\n",
    "\n",
    "# def compute_silhouette_score(similarity_matrix, cluster_labels):\n",
    "#     distance_matrix = 2 - (similarity_matrix + 1)\n",
    "#     score = silhouette_score(distance_matrix, cluster_labels, metric='precomputed')\n",
    "#     return score\n",
    "\n",
    "# # Main execution\n",
    "# label_distributions = [calculate_label_distribution(loader) for loader in train_loaders]\n",
    "\n",
    "# # New function call to plot the distributions\n",
    "# plot_client_distributions(label_distributions)\n",
    "\n",
    "# similarity_matrix = compute_similarity_matrix(label_distributions)\n",
    "# save_similarity_matrix_to_csv(similarity_matrix, filename=\"clients_datasets_similarity_matrix.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T01:57:34.341221Z",
     "iopub.status.busy": "2025-01-12T01:57:34.340933Z",
     "iopub.status.idle": "2025-01-12T01:57:34.357306Z",
     "shell.execute_reply": "2025-01-12T01:57:34.356553Z",
     "shell.execute_reply.started": "2025-01-12T01:57:34.341190Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clients clustering label based on their dataset:  [1 0 1 2 2 3 3 3 2 0]\n",
      "Clients clustering based on their dataset:  {np.int64(1): [0, 2], np.int64(0): [1, 9], np.int64(2): [3, 4, 8], np.int64(3): [5, 6, 7]}\n"
     ]
    }
   ],
   "source": [
    "cluster_labels = cluster_clients(similarity_matrix)\n",
    "print(\"Clients clustering label based on their dataset: \", cluster_labels)\n",
    "\n",
    "def group_clients_by_cluster(labels):\n",
    "    clusters = {}\n",
    "    for client_id, cluster_id in enumerate(labels):\n",
    "        if cluster_id not in clusters:\n",
    "            clusters[cluster_id] = []\n",
    "        clusters[cluster_id].append(client_id)\n",
    "    return clusters\n",
    "\n",
    "clusters = group_clients_by_cluster(cluster_labels)\n",
    "print(\"Clients clustering based on their dataset: \", clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T01:57:34.358286Z",
     "iopub.status.busy": "2025-01-12T01:57:34.358066Z",
     "iopub.status.idle": "2025-01-12T01:57:34.376974Z",
     "shell.execute_reply": "2025-01-12T01:57:34.376224Z",
     "shell.execute_reply.started": "2025-01-12T01:57:34.358257Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# silhouette_cosine = compute_silhouette_score(similarity_matrix, [0, 1, 0, 2, 2, 3, 3, 3, 2, 1])\n",
    "# print(f\"Silhouette score for data clustering is: {silhouette_cosine}\")\n",
    "\n",
    "# silhouette_cosine = compute_silhouette_score(similarity_matrix, [2, 0, 1, 1, 1, 1, 2, 2, 1, 0,])\n",
    "# print(f\"Silhouette score for cosine is: {silhouette_cosine}\")\n",
    "\n",
    "# silhouette_cosine_less_sig_pruned = compute_silhouette_score(similarity_matrix, [0, 3, 0, 1, 1, 3, 2, 2, 3, 3,])\n",
    "# print(f\"Silhouette score for cosine (optimal) common less sig pruned is: {silhouette_cosine_less_sig_pruned}\")\n",
    "\n",
    "# silhouette_coordinate = compute_silhouette_score(similarity_matrix, [0, 3, 0, 1, 1, 3, 2, 2, 0, 3,])\n",
    "# print(f\"Silhouette score for coordinate is: {silhouette_coordinate}\")\n",
    "\n",
    "# silhouette_euclidean = compute_silhouette_score(similarity_matrix, [3, 0, 3, 1, 0, 3, 3, 3, 2, 0,])\n",
    "# print(f\"Silhouette score for euclidean is: {silhouette_euclidean}\")\n",
    "\n",
    "# silhouette_wasserstein = compute_silhouette_score(similarity_matrix, [2, 0, 2, 2, 2, 0, 2, 2, 1, 0,])\n",
    "# print(f\"Silhouette score for wasserstein is: {silhouette_wasserstein}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFgZGqbk3JuE"
   },
   "source": [
    "# <div style=\"border-radius:10px; border:#f4af03 solid; padding: 15px; background-color: #506886; font-size:100%; text-align:center\">Executing</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-12T01:57:34.377837Z",
     "iopub.status.busy": "2025-01-12T01:57:34.377654Z"
    },
    "id": "KTKWmDCUxXfO",
    "outputId": "60e5a5cb-29f4-41b0-84a7-a203fa0c2503",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---------Clustering step 0\n",
      "-------------in initial genertaio\n",
      "cluster [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "clientIDs [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "len_client_models(should be 10): 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ---in making new FL----cluster models len: 10 cluster IDs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "cid is: 0\n",
      "cid is: 1\n",
      "cid is: 2\n",
      "cid is: 3\n",
      "cid is: 4\n",
      "cid is: 5\n",
      "cid is: 6\n",
      "cid is: 7\n",
      "cid is: 8\n",
      "cid is: 9\n",
      "\n",
      "Round 1/1\n",
      "Epoch 1/1, Loss: 0.0156, Accuracy: 0.4613\n",
      "_____________________________________________________________________________________________________________\n",
      "node 0: train_acc: 0.4613333333333333, test_acc:0.6666666666666666\n",
      "Epoch 1/1, Loss: 0.0182, Accuracy: 0.0069\n",
      "_____________________________________________________________________________________________________________\n",
      "node 1: train_acc: 0.006888888888888889, test_acc:0.49466666666666664\n",
      "Epoch 1/1, Loss: 0.0176, Accuracy: 0.6667\n",
      "_____________________________________________________________________________________________________________\n",
      "node 2: train_acc: 0.6666666666666666, test_acc:0.6666666666666666\n",
      "Epoch 1/1, Loss: 0.0166, Accuracy: 0.5970\n",
      "_____________________________________________________________________________________________________________\n",
      "node 3: train_acc: 0.597, test_acc:0.847\n",
      "Epoch 1/1, Loss: 0.0182, Accuracy: 0.0809\n",
      "_____________________________________________________________________________________________________________\n",
      "node 4: train_acc: 0.08088888888888889, test_acc:0.7666666666666667\n",
      "Epoch 1/1, Loss: 0.0167, Accuracy: 0.4060\n",
      "_____________________________________________________________________________________________________________\n",
      "node 5: train_acc: 0.406, test_acc:0.7496251874062968\n",
      "Epoch 1/1, Loss: 0.0182, Accuracy: 0.1928\n",
      "_____________________________________________________________________________________________________________\n",
      "node 6: train_acc: 0.1928, test_acc:0.7142857142857143\n",
      "Epoch 1/1, Loss: 0.0175, Accuracy: 0.5506\n",
      "_____________________________________________________________________________________________________________\n",
      "node 7: train_acc: 0.5506, test_acc:0.6002400960384153\n",
      "Epoch 1/1, Loss: 0.0164, Accuracy: 0.5047\n",
      "_____________________________________________________________________________________________________________\n",
      "node 8: train_acc: 0.5046666666666667, test_acc:0.6666666666666666\n",
      "Epoch 1/1, Loss: 0.0170, Accuracy: 0.6667\n",
      "_____________________________________________________________________________________________________________\n",
      "node 9: train_acc: 0.6666666666666666, test_acc:0.6666666666666666\n",
      "Model numbers: 10\n",
      "Saved model 0 state dict to ./models/cnn/fmnist/1_epochs_1_fl_rounds/1_epochs/node_0.pth\n",
      "Saved model 1 state dict to ./models/cnn/fmnist/1_epochs_1_fl_rounds/1_epochs/node_1.pth\n",
      "Saved model 2 state dict to ./models/cnn/fmnist/1_epochs_1_fl_rounds/1_epochs/node_2.pth\n",
      "Saved model 3 state dict to ./models/cnn/fmnist/1_epochs_1_fl_rounds/1_epochs/node_3.pth\n",
      "Saved model 4 state dict to ./models/cnn/fmnist/1_epochs_1_fl_rounds/1_epochs/node_4.pth\n",
      "Saved model 5 state dict to ./models/cnn/fmnist/1_epochs_1_fl_rounds/1_epochs/node_5.pth\n",
      "Saved model 6 state dict to ./models/cnn/fmnist/1_epochs_1_fl_rounds/1_epochs/node_6.pth\n",
      "Saved model 7 state dict to ./models/cnn/fmnist/1_epochs_1_fl_rounds/1_epochs/node_7.pth\n",
      "Saved model 8 state dict to ./models/cnn/fmnist/1_epochs_1_fl_rounds/1_epochs/node_8.pth\n",
      "Saved model 9 state dict to ./models/cnn/fmnist/1_epochs_1_fl_rounds/1_epochs/node_9.pth\n",
      "time 0:00:13.600614\n",
      "global acc: 0.6839150997730427\n",
      "----------------------Info before clustering----------------------\n",
      "model_len: 10\n",
      "Client IDS: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "All model parameters saved to model_parameters/0/all_model_parameters_ordered_by_importance_for_client_0.csv\n",
      "Model weights and sensitivity data for client #0 processed.\n",
      "All model parameters saved to model_parameters/0/all_model_parameters_ordered_by_importance_for_client_1.csv\n",
      "Model weights and sensitivity data for client #1 processed.\n",
      "All model parameters saved to model_parameters/0/all_model_parameters_ordered_by_importance_for_client_2.csv\n",
      "Model weights and sensitivity data for client #2 processed.\n",
      "All model parameters saved to model_parameters/0/all_model_parameters_ordered_by_importance_for_client_3.csv\n",
      "Model weights and sensitivity data for client #3 processed.\n",
      "All model parameters saved to model_parameters/0/all_model_parameters_ordered_by_importance_for_client_4.csv\n",
      "Model weights and sensitivity data for client #4 processed.\n",
      "All model parameters saved to model_parameters/0/all_model_parameters_ordered_by_importance_for_client_5.csv\n",
      "Model weights and sensitivity data for client #5 processed.\n",
      "All model parameters saved to model_parameters/0/all_model_parameters_ordered_by_importance_for_client_6.csv\n",
      "Model weights and sensitivity data for client #6 processed.\n",
      "All model parameters saved to model_parameters/0/all_model_parameters_ordered_by_importance_for_client_7.csv\n",
      "Model weights and sensitivity data for client #7 processed.\n",
      "All model parameters saved to model_parameters/0/all_model_parameters_ordered_by_importance_for_client_8.csv\n",
      "Model weights and sensitivity data for client #8 processed.\n",
      "All model parameters saved to model_parameters/0/all_model_parameters_ordered_by_importance_for_client_9.csv\n",
      "Model weights and sensitivity data for client #9 processed.\n",
      "Distance matrix saved to distances_coordinate.csv\n",
      "cluster results:[0 2 1 0 1 0 1 1 1 2]\n",
      "new clustering: [[0, 3, 5], [2, 4, 6, 7, 8], [1, 9]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clusters=[]\n",
    "initial = [i for i in range(NUMBER_OF_CLIENTS)]\n",
    "clusters.append(initial)\n",
    "\n",
    "\n",
    "def generate_initial_models(step,cluster,client_ids,client_Models):\n",
    "    print(\"-------------in initial genertaio\")\n",
    "    print(\"cluster\", cluster)\n",
    "    print(\"clientIDs\", client_ids)\n",
    "    print(\"len_client_models(should be 10):\",len(client_Models))\n",
    "    list1=[]\n",
    "\n",
    "    if step==0:\n",
    "        for member in range(len(cluster)):\n",
    "            list1.append(Net())\n",
    "    else:\n",
    "        for index in cluster:\n",
    "            list1.append(client_Models[client_ids.index(index)])\n",
    "    return list1\n",
    "\n",
    "\n",
    "client_Models=[]\n",
    "client_copy_models = []\n",
    "\n",
    "for step in range(CLUSTERING_PERIOD):\n",
    "    client_copy_models=copy.deepcopy(client_Models)\n",
    "    client_Models=[]\n",
    "    print(\"\\n\\n---------Clustering step\", step)\n",
    "    FL_list=[]\n",
    "    client_ids=[]\n",
    "    for cluster in clusters:\n",
    "        for Id in cluster:\n",
    "            client_ids.append(Id)\n",
    "        cluster_initial_models=generate_initial_models(step,cluster,client_ids,client_copy_models)\n",
    "        print(\" ---in making new FL----cluster models len:\", len(cluster_initial_models),\"cluster IDs:\", client_ids)\n",
    "        f = FL(cluster,cluster_initial_models,FEDERATED_LEARNING_ROUNDS, train_loaders, test_loaders, SENSITIVITY_PERCENTAGE)\n",
    "        FL_list.append(f)\n",
    "        for member in f.client_obj_list:\n",
    "            client_Models.append(member.net)\n",
    "        for cid in client_ids:\n",
    "            save_torch_model(client_Models[client_ids.index(cid)], cid)\n",
    "\n",
    "    if DO_CLUSTER:\n",
    "        print(\"----------------------Info before clustering----------------------\")\n",
    "        print(\"model_len:\", len(client_Models))\n",
    "        print(\"Client IDS:\",client_ids )\n",
    "        start_cluster_time = datetime.now()\n",
    "        clusters = Clustering(client_ids, train_loaders, SENSITIVITY_PERCENTAGE, step).Clusters\n",
    "        end_cluster_time = datetime.now()\n",
    "        exe_cluster_time = end_cluster_time - start_cluster_time\n",
    "        with open(log_file, 'a') as f:\n",
    "            f.write(f\"\\n Exe Cluster Time: {exe_cluster_time}\")\n",
    "            f.write(f\"\\n The Clusters are: {clusters}\")\n",
    "        print(\"new clustering:\",clusters)\n",
    "    else:\n",
    "        print(\"----------------------the `DO_CLUSTER` is False stopping Federated Learning----------------------\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6072062,
     "sourceId": 9887519,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
